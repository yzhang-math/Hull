{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58aa266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler  # <-- ADD THIS LINE\n",
    "\n",
    "# The training data path should be updated to your actual training file.\n",
    "TRAIN_DATA_PATH = \"./kaggle/train.csv\"\n",
    "SPY_DATA_PATH = \"./kaggle/spy-historical.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8212c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_1(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Generates 40 new features from the base data.\n",
    "      This function is the target of the evolutionary algorithm.\n",
    "    \n",
    "      Available Feature Categories:\n",
    "      - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "      - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "      - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "      - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "      - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "      - S* (Sentiment features): 12 columns (S1-S12)\n",
    "      - V* (Volatility features): 13 columns (V1-V13)\n",
    "    \"\"\"\n",
    "    new_features = pl.DataFrame({\n",
    "        # --- New Interaction Features ---\n",
    "        'feat_M1_x_V10_x_P1': df['M1'] * df['V10'] * df['P1'],\n",
    "        'feat_P1_add_E10_div_I5': (df['P1'] + df['E10']) / (df['I5'] + 1e-6),\n",
    "        'feat_S3_div_I5_x_M12': df['S3'] / (df['I5'] + 1e-6) * df['M12'],\n",
    "        'feat_M12_x_V2_x_P2': df['M12'] * df['V2'] * df['P2'],\n",
    "        'feat_P2_sqrt_E4_x_S8': np.sqrt(df['P2'] * df['E4']) * df['S8'],\n",
    "        'feat_M2_x_S8_x_V2': df['M2'] * df['S8'] * df['V2'],\n",
    "        'feat_V10_cbrt_P5_x_E12': np.cbrt(df['V10'] * df['P5']) * df['E12'],\n",
    "        'feat_E12_x_I9_x_M1': df['E12'] * df['I9'] * df['M1'],\n",
    "        'feat_M1_log_V1_x_S7': np.log(df['M1'] + 1) * np.log(df['V1'] + 1) * df['S7'],\n",
    "        'feat_S7_abs_P1_x_I5': np.abs(df['S7'] - df['P1']) * df['I5'],\n",
    "    })\n",
    "    # Fill any nulls created by rolling windows\n",
    "    return new_features.with_columns(pl.all().forward_fill().backward_fill())\n",
    "\n",
    "\n",
    "\n",
    "def generate_features_2(df: pl.DataFrame) -> pl.DataFrame:\n",
    "  \"\"\"Generates 10 new features from the base data.\n",
    "    This function is the target of the evolutionary algorithm.\n",
    "  \n",
    "    Available Feature Categories:\n",
    "    - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "    - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "    - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "    - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "    - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "    - S* (Sentiment features): 12 columns (S1-S12)\n",
    "    - V* (Volatility features): 13 columns (V1-V13)\n",
    "    - MOM* (Momentum features): 0 columns\n",
    "  \"\"\"\n",
    "  cols = ['M1', 'V1', 'P1', 'E1', 'S1', 'I1', 'M10', 'V10', 'P10', 'E10']\n",
    "  # Generate interaction features with exponential moving averages\n",
    "  new_features = pl.DataFrame({\n",
    "      #'feat_P1_add_E1': df[cols[2]] + df[cols[3]],\n",
    "      #'feat_S1_sub_I1': df[cols[4]] - df[cols[5]],\n",
    "      #'feat_M10_div_V10': (df[cols[6]] / (df[cols[7]] + 1e-6)),\n",
    "      #'feat_P10_x_E10': df[cols[8]] * df[cols[9]],\n",
    "      # Exponential moving averages instead of simple rolling means\n",
    "      'feat_V1_ema_5': df[cols[1]].rolling_mean(window_size=5, min_samples=1, center=True),\n",
    "      'feat_V1_ewm_std_5': df[cols[1]].rolling_std(window_size=5, min_samples=1, center=True),\n",
    "      'feat_M1_ema_20': df[cols[0]].rolling_mean(window_size=20, min_samples=1, center=True),\n",
    "      'feat_M1_ewm_std_20': df[cols[0]].rolling_std(window_size=20, min_samples=1, center=True),\n",
    "      # Relative strength index\n",
    "      'feat_RSI_14': (df[cols[2]].rolling_max(14) - df[cols[2]].rolling_min(14)) / (df[cols[2]].rolling_max(14) - df[cols[2]].rolling_min(14) + 1e-6),\n",
    "  })\n",
    "  return new_features.with_columns(pl.all().fill_null(0))\n",
    "\n",
    "def generate_features_3(df: pl.DataFrame) -> pl.DataFrame:\n",
    "  \"\"\"Generates 10 new features from the base data.\n",
    "    This function is the target of the evolutionary algorithm.\n",
    "  \n",
    "    Available Feature Categories:\n",
    "    - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "    - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "    - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "    - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "    - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "    - S* (Sentiment features): 12 columns (S1-S12)\n",
    "    - V* (Volatility features): 13 columns (V1-V13)\n",
    "    - MOM* (Momentum features): 0 columns\n",
    "  \"\"\"\n",
    "  cols = ['M1', 'V1', 'P1', 'E1', 'S1', 'I1', 'M10', 'V10', 'P10', 'E10']\n",
    "  # Generate interaction features\n",
    "  new_features = pl.DataFrame({\n",
    "      # Pairwise interactions between different categories\n",
    "      'feat_M1_x_V1': df[cols[0]] * df[cols[1]],\n",
    "      'feat_P1_add_E1': df[cols[2]] + df[cols[3]],\n",
    "      'feat_S1_sub_I1': df[cols[4]] - df[cols[5]],\n",
    "      'feat_M10_div_V10': (df[cols[6]] / (df[cols[7]] + 1e-6)),\n",
    "      'feat_P10_x_E10': df[cols[8]] * df[cols[9]],\n",
    "      # Rolling window features on volatile columns\n",
    "      'feat_V1_roll_mean_5': df[cols[1]].rolling_mean(window_size=5),\n",
    "      'feat_V1_roll_std_5': df[cols[1]].rolling_std(window_size=5),\n",
    "      'feat_M1_roll_mean_20': df[cols[0]].rolling_mean(window_size=20),\n",
    "      'feat_M1_roll_std_20': df[cols[0]].rolling_std(window_size=20),\n",
    "      # A simple ratio\n",
    "      'feat_P1_div_M1': (df[cols[2]] / (df[cols[0]] + 1e-6)),\n",
    "      # New feature: Exponential moving average\n",
    "      'feat_M1_exp_mean_5': df[cols[0]].rolling_mean(window_size=5, min_periods=1, center=True),\n",
    "  })\n",
    "  # Fill any nulls created by rolling windows\n",
    "  return new_features.with_columns(pl.all().forward_fill().backward_fill())\n",
    "\n",
    "def generate_features_4(df: pl.DataFrame) -> pl.DataFrame:\n",
    "  \"\"\"Generates 40 new features from the base data.\n",
    "    This function is the target of the evolutionary algorithm.\n",
    "  \n",
    "    Available Feature Categories:\n",
    "    - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "    - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "    - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "    - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "    - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "    - S* (Sentiment features): 12 columns (S1-S12)\n",
    "    - V* (Volatility features): 13 columns (V1-V13)\n",
    "  \"\"\"\n",
    "  \"\"\"Improved version of `generate_features_v0` with different features.\"\"\"\n",
    "  new_features = pl.DataFrame({\n",
    "      # --- 20 Pairwise Interactions ---\n",
    "      #'feat_M1_x_V2': df['M1'] * df['V2'],\n",
    "      #'feat_P1_add_E2': df['P1'] + df['E2'],\n",
    "      'feat_S1_sub_I2': df['S1'] - df['I2'],\n",
    "      'feat_M10_div_V11': df['M10'] / (df['V11'] + 1e-6),\n",
    "      'feat_P10_x_E11': df['P10'] * df['E11'],\n",
    "      'feat_M2_x_S4': df['M2'] * df['S4'],\n",
    "      'feat_V2_div_P3': df['V2'] / (df['P3'] + 1e-6),\n",
    "      'feat_E4_sub_I4': df['E4'] - df['I4'],\n",
    "      'feat_S7_add_M11': df['S7'] + df['M11'],\n",
    "      'feat_I5_x_V12': df['I5'] * df['V12'],\n",
    "      'feat_P5_div_S9': df['P5'] / (df['S9'] + 1e-6),\n",
    "      'feat_E12_x_I8': df['E12'] * df['I8'],\n",
    "      'feat_M1_div_S2': df['M1'] / (df['S2'] + 1e-6),\n",
    "      'feat_V1_add_P2': df['V1'] + df['P2'],\n",
    "      'feat_E1_sub_I2': df['E1'] - df['I2'],\n",
    "      'feat_M2_div_V3': df['M2'] / (df['V3'] + 1e-6),\n",
    "      'feat_P2_x_S4': df['P2'] * df['S4'],\n",
    "      'feat_E4_add_M11': df['E4'] + df['M11'],\n",
    "      'feat_I3_sub_V11': df['I3'] - df['V11'],\n",
    "      'feat_S7_x_P11': df['S7'] * df['P11'],\n",
    "      # --- 10 Rolling Window Features ---\n",
    "      'feat_V2_roll_mean_5': df['V2'].rolling_mean(window_size=5),\n",
    "      'feat_V2_roll_std_5': df['V2'].rolling_std(window_size=5),\n",
    "      'feat_M2_roll_mean_20': df['M2'].rolling_mean(window_size=20),\n",
    "      'feat_M2_roll_std_20': df['M2'].rolling_std(window_size=20),\n",
    "      'feat_P2_roll_max_10': df['P2'].rolling_max(window_size=10),\n",
    "      'feat_P2_roll_min_10': df['P2'].rolling_min(window_size=10),\n",
    "      'feat_E2_roll_mean_50': df['E2'].rolling_mean(window_size=50),\n",
    "      'feat_S2_roll_std_50': df['S2'].rolling_std(window_size=50),\n",
    "      'feat_I2_roll_mean_10': df['I2'].rolling_mean(window_size=10),\n",
    "      'feat_V11_roll_std_10': df['V11'].rolling_std(window_size=10),\n",
    "      # --- 10 Complex Interactions (3+ elements) ---\n",
    "      'feat_M1_V2_div_P2': (df['M1'] * df['V2']) / (df['P2'] + 1e-6),\n",
    "      'feat_E2_S2_add_I2': df['E2'] + df['S2'] - df['I2'],\n",
    "      'feat_M2_P3_sub_V3': df['M2'] + df['P3'] - df['V3'],\n",
    "      'feat_S8_div_E4_I4': df['S8'] / (df['E4'] + df['I4'] + 1e-6),\n",
    "      'feat_P6_x_M11_x_V11': df['P6'] * df['M11'] * df['V11'],\n",
    "      'feat_roll_diff_M2_5_20': df['M2'].rolling_mean(window_size=5) - df['M2'].rolling_mean(window_size=20),\n",
    "      'feat_roll_diff_V2_5_20': df['V2'].rolling_mean(window_size=5) - df['V2'].rolling_mean(window_size=20),\n",
    "      'feat_M_S_P_combo_v2': (df['M11'] - df['M2']) / (df['S2'] + df['P2'] + 1e-6),\n",
    "      'feat_V_E_I_combo_v2': (df['V12'] + df['V3']) * (df['E2'] - df['I2']),\n",
    "      'feat_ratio_of_ratios_v2': (df['M2']/(df['V2']+1e-6)) / (df['P2']/(df['S2']+1e-6)),\n",
    "  })\n",
    "  # Fill any nulls created by rolling windows\n",
    "  return new_features.with_columns(pl.all().forward_fill().backward_fill())\n",
    "\n",
    "def generate_features_5(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Generates 40 new features from the base data.\n",
    "      This function is the target of the evolutionary algorithm.\n",
    "    \n",
    "      Available Feature Categories:\n",
    "      - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "      - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "      - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "      - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "      - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "      - S* (Sentiment features): 12 columns (S1-S12)\n",
    "      - V* (Volatility features): 13 columns (V1-V13)\n",
    "    \"\"\"\n",
    "    \"\"\"Improved version of `generate_features_v2` with more complex interactions.\"\"\"\n",
    "    new_features = pl.DataFrame({\n",
    "        'feat_M1_add_V1': df['M1'] + df['V1'],\n",
    "        'feat_P1_sub_E1': df['P1'] - df['E1'],\n",
    "        'feat_S1_mul_I1': df['S1'] * df['I1'],\n",
    "        #'feat_M10_div_V10': df['M10'] / (df['V10'] + 1e-6),\n",
    "        'feat_P10_mul_E10': df['P10'] * df['E10'],\n",
    "        'feat_M2_mul_S3': df['M2'] * df['S3'],\n",
    "        'feat_V2_div_P2': df['V2'] / (df['P2'] + 1e-6),\n",
    "        'feat_E4_sub_I3': df['E4'] - df['I3'],\n",
    "        'feat_S7_add_M12': df['S7'] + df['M12'],\n",
    "        'feat_I5_mul_V11': df['I5'] * df['V11'],\n",
    "        # 'feat_V1_roll_mean_5': df['V1'].rolling_mean(window_size=5),\n",
    "        # 'feat_V1_roll_std_5': df['V1'].rolling_std(window_size=5),\n",
    "        # 'feat_M1_roll_mean_20': df['M1'].rolling_mean(window_size=20),\n",
    "        # 'feat_M1_roll_std_20': df['M1'].rolling_std(window_size=20),\n",
    "        'feat_P1_roll_max_10': df['P1'].rolling_max(window_size=10),\n",
    "        'feat_P1_roll_min_10': df['P1'].rolling_min(window_size=10),\n",
    "        'feat_E1_roll_mean_50': df['E1'].rolling_mean(window_size=50),\n",
    "        'feat_S1_roll_std_50': df['S1'].rolling_std(window_size=50),\n",
    "        'feat_I1_roll_mean_10': df['I1'].rolling_mean(window_size=10),\n",
    "        'feat_V10_roll_std_10': df['V10'].rolling_std(window_size=10),\n",
    "        'feat_M1_V1_div_P1': (df['M1'] * df['V1']) / (df['P1'] + 1e-6),\n",
    "        'feat_E1_S1_add_I1': df['E1'] + df['S1'] - df['I1'],\n",
    "        'feat_M2_P2_sub_V2': df['M2'] + df['P2'] - df['V2'],\n",
    "        'feat_S7_div_E4_I3': df['S7'] / (df['E4'] + df['I3'] + 1e-6),\n",
    "        'feat_P5_x_M10_x_V10': df['P5'] * df['M10'] * df['V10'],\n",
    "        'feat_roll_diff_M1_5_20': df['M1'].rolling_mean(window_size=5) - df['M1'].rolling_mean(window_size=20),\n",
    "        'feat_roll_diff_V1_5_20': df['V1'].rolling_mean(window_size=5) - df['V1'].rolling_mean(window_size=20),\n",
    "        'feat_M_S_P_combo': (df['M12'] - df['M1']) / (df['S1'] + df['P1'] + 1e-6),\n",
    "        'feat_V_E_I_combo': (df['V11'] + df['V2']) * (df['E1'] - df['I1']),\n",
    "        'feat_ratio_of_ratios': (df['M1']/(df['V1']+1e-6)) / (df['P1']/(df['S1']+1e-6)),\n",
    "        'feat_M1_roll_skew_5': df['M1'].rolling_skew(window_size=5),\n",
    "        'feat_V1_roll_kurt_5': df['V1'].rolling_kurtosis(window_size=5),\n",
    "        'feat_E1_roll_skew_20': df['E1'].rolling_skew(window_size=20),\n",
    "        'feat_S1_roll_kurt_20': df['S1'].rolling_kurtosis(window_size=20),\n",
    "        'feat_I1_roll_skew_10': df['I1'].rolling_skew(window_size=10),\n",
    "        'feat_P1_roll_kurt_10': df['P1'].rolling_kurtosis(window_size=10),\n",
    "        'feat_M1_V1_E1_S1': df['M1'] * df['V1'] * df['E1'] * df['S1'],\n",
    "        'feat_P1_S1_I1_V1': df['P1'] * df['S1'] * df['I1'] * df['V1'],\n",
    "        'feat_M10_V10_P10_E10': df['M10'] * df['V10'] * df['P10'] * df['E10'],\n",
    "        'feat_E1_S1_V1_I1': df['E1'] * df['S1'] * df['V1'] * df['I1'],\n",
    "        'feat_complex_interaction': df['M1'] * (df['V1'] - df['E1']) / (df['P1'] + df['I1'] + 1e-6),\n",
    "    })\n",
    "    # Fill any nulls created by rolling windows\n",
    "    return new_features.with_columns(pl.all().forward_fill().backward_fill())\n",
    "\n",
    "\n",
    "def generate_features_6 (df: pl.DataFrame) -> pl.DataFrame:\n",
    "  \"\"\"Generates 40 new features from the base data.\n",
    "    This function is the target of the evolutionary algorithm.\n",
    "  \n",
    "    Available Feature Categories:\n",
    "    - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "    - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "    - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "    - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "    - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "    - S* (Sentiment features): 12 columns (S1-S12)\n",
    "    - V* (Volatility features): 13 columns (V1-V13)\n",
    "  \"\"\"\n",
    "  new_features = pl.DataFrame({\n",
    "      # --- 10 New Pairwise Interactions ---\n",
    "      'feat_M1_x_V2': df['M1'] * df['V2'],\n",
    "      'feat_P1_add_E2': df['P1'] + df['E2'],\n",
    "      'feat_S2_sub_I2': df['S2'] - df['I2'],\n",
    "      'feat_M3_div_V3': df['M3'] / (df['V3'] + 1e-6),\n",
    "      'feat_P2_x_E3': df['P2'] * df['E3'],\n",
    "      'feat_M4_x_S4': df['M4'] * df['S4'],\n",
    "      'feat_V4_div_P5': df['V4'] / (df['P5'] + 1e-6),\n",
    "      'feat_E5_sub_I4': df['E5'] - df['I4'],\n",
    "      'feat_S8_add_M13': df['S8'] + df['M13'],\n",
    "      'feat_I7_x_V12': df['I7'] * df['V12'],\n",
    "  })\n",
    "  return new_features.with_columns(pl.all().fill_null(0))\n",
    "\n",
    "def generate_features_7 (df: pl.DataFrame) -> pl.DataFrame:\n",
    "  \"\"\"Generates new features from the base data.\n",
    "    This function is the target of the evolutionary algorithm.\n",
    "  \n",
    "    Available Feature Categories:\n",
    "    - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "    - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "    - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "    - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "    - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "    - S* (Sentiment features): 12 columns (S1-S12)\n",
    "    - V* (Volatility features): 13 columns (V1-V13)\n",
    "  \"\"\"\n",
    "  new_features = pl.DataFrame({\n",
    "      # --- 20 Pairwise Interactions ---\n",
    "      'feat_M1_x_V1': df['M1'] * df['V1'],\n",
    "      'feat_P1_add_E1': df['P1'] + df['E1'],\n",
    "      'feat_S1_sub_I1': df['S1'] - df['I1'],\n",
    "      'feat_M10_div_V10': df['M10'] / (df['V10'] + 1e-6),\n",
    "      'feat_P10_x_E10': df['P10'] * df['E10'],\n",
    "      'feat_M2_x_S3': df['M2'] * df['S3'],\n",
    "      'feat_V2_div_P2': df['V2'] / (df['P2'] + 1e-6),\n",
    "      'feat_E4_sub_I3': df['E4'] - df['I3'],\n",
    "      'feat_S7_add_M12': df['S7'] + df['M12'],\n",
    "      'feat_I5_x_V11': df['I5'] * df['V11'],\n",
    "      'feat_P5_div_S8': df['P5'] / (df['S8'] + 1e-6),\n",
    "      'feat_E12_x_I9': df['E12'] * df['I9'],\n",
    "      'feat_M1_div_S1': df['M1'] / (df['S1'] + 1e-6),\n",
    "      'feat_V1_add_P1': df['V1'] + df['P1'],\n",
    "      'feat_E1_sub_I1': df['E1'] - df['I1'],\n",
    "      'feat_M2_div_V2': df['M2'] / (df['V2'] + 1e-6),\n",
    "      'feat_P2_x_S3': df['P2'] * df['S3'],\n",
    "      'feat_E4_add_M10': df['E4'] + df['M10'],\n",
    "      'feat_I3_sub_V10': df['I3'] - df['V10'],\n",
    "      'feat_S7_x_P10': df['S7'] * df['P10'],\n",
    "      # --- 10 Rolling Window Features ---\n",
    "      'feat_V2_roll_mean_5': df['V2'].rolling_mean(window_size=5),\n",
    "      'feat_V1_roll_std_5': df['V1'].rolling_std(window_size=5),\n",
    "      'feat_M1_roll_mean_20': df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_M3_roll_std_20': df['M3'].rolling_std(window_size=20),\n",
    "      'feat_P1_roll_max_10': df['P1'].rolling_max(window_size=10),\n",
    "      'feat_P1_roll_min_10': df['P1'].rolling_min(window_size=10),\n",
    "      'feat_E5_roll_mean_50': df['E5'].rolling_mean(window_size=50),\n",
    "      'feat_S1_roll_std_50': df['S1'].rolling_std(window_size=50),\n",
    "      'feat_I1_roll_mean_10': df['I1'].rolling_mean(window_size=10),\n",
    "      'feat_V10_roll_std_10': df['V10'].rolling_std(window_size=10),\n",
    "      # --- 10 Complex Interactions (3+ elements) ---\n",
    "      'feat_M1_V1_div_P1': (df['M1'] * df['V1']) / (df['P1'] + 1e-6),\n",
    "      'feat_E1_S1_add_I1': df['E1'] + df['S1'] - df['I1'],\n",
    "      'feat_M2_P2_sub_V2': df['M2'] + df['P2'] - df['V2'],\n",
    "      'feat_S7_div_E4_I3': df['S7'] / (df['E4'] + df['I3'] + 1e-6),\n",
    "      'feat_P5_x_M10_x_V10': df['P5'] * df['M10'] * df['V10'],\n",
    "      'feat_roll_diff_M1_5_20': df['M1'].rolling_mean(window_size=5) - df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_roll_diff_V1_5_20': df['V1'].rolling_mean(window_size=5) - df['V1'].rolling_mean(window_size=20),\n",
    "      'feat_M_S_P_combo': (df['M12'] - df['M1']) / (df['S1'] + df['P1'] + 1e-6),\n",
    "      'feat_V_E_I_combo': (df['V11'] + df['V2']) * (df['E1'] - df['I1']),\n",
    "      'feat_ratio_of_ratios': (df['M1']/(df['V1']+1e-6)) / (df['P1']/(df['S1']+1e-6)),\n",
    "      # --- 10 New Features ---\n",
    "      'feat_M1_x_V1_x_P1': df['M1'] * df['V1'] * df['P1'],\n",
    "      'feat_E1_div_S1': df['E1'] / (df['S1'] + 1e-6),\n",
    "      'feat_I1_sub_V1': df['I1'] - df['V1'],\n",
    "      'feat_M10_add_V10': df['M10'] + df['V10'],\n",
    "      'feat_P10_div_E10': df['P10'] / (df['E10'] + 1e-6),\n",
    "      'feat_M2_add_S3': df['M2'] + df['S3'],\n",
    "      'feat_V2_x_P2': df['V2'] * df['P2'],\n",
    "      'feat_E4_add_I3': df['E4'] + df['I3'],\n",
    "      'feat_S7_div_M12': df['S7'] / (df['M12'] + 1e-6),\n",
    "      'feat_I5_div_V11': df['I5'] / (df['V11'] + 1e-6),\n",
    "      'feat_M1_log_P1': np.log(df['M1'] + 1e-6) / np.log(df['P1'] + 1e-6),\n",
    "  })\n",
    "  # Fill any nulls created by rolling windows\n",
    "  return new_features.with_columns(pl.all().forward_fill())\n",
    "  return new_features.with_columns(pl.all().forward_fill().backward_fill())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cac177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4990, 98)\n",
      "\n",
      "--- Joining weekday feature onto sliced data ---\n",
      "Initial DataFrame shape: (4990, 99)\n",
      "Base DataFrame shape before cleaning: (4990, 99)\n",
      "Base DataFrame shape during cleaning: (4990, 98)\n",
      "Base DataFrame shape after cleaning: (4990, 98)\n",
      "Processed DataFrame shape: (4990, 149)\n",
      "Strategy Volatility: 11.38%, Market Volatility: 13.21%, Sharpe: 0.7761, Adjusted Sharpe: 0.7761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11666/185523524.py:17: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.selectors.numeric().rolling_mean(window_size=5, min_periods=1)\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/polars/series/series.py:1571: RuntimeWarning: invalid value encountered in log\n",
      "  lambda out: ufunc(*args, out=out, dtype=dtype_char, **kwargs),\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 22.79%, Market Volatility: 21.92%, Sharpe: 0.0960, Adjusted Sharpe: 0.0960\n",
      "Strategy Volatility: 49.34%, Market Volatility: 33.66%, Sharpe: -0.5628, Adjusted Sharpe: -0.0885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 19.77%, Market Volatility: 18.61%, Sharpe: 0.5394, Adjusted Sharpe: 0.5394\n",
      "Strategy Volatility: 10.91%, Market Volatility: 13.14%, Sharpe: 1.2802, Adjusted Sharpe: 0.6026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 29.72%, Market Volatility: 22.43%, Sharpe: 0.2100, Adjusted Sharpe: 0.1818\n",
      "Strategy Volatility: 7.82%, Market Volatility: 11.85%, Sharpe: 1.7270, Adjusted Sharpe: 1.0671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 9.68%, Market Volatility: 10.99%, Sharpe: 2.0634, Adjusted Sharpe: 2.0634\n",
      "Strategy Volatility: 15.90%, Market Volatility: 11.69%, Sharpe: 1.0296, Adjusted Sharpe: 0.8870\n",
      "Strategy Volatility: 20.15%, Market Volatility: 16.89%, Sharpe: 0.5209, Adjusted Sharpe: 0.5209\n",
      "Strategy Volatility: 11.07%, Market Volatility: 9.95%, Sharpe: 1.3846, Adjusted Sharpe: 1.3142\n",
      "Strategy Volatility: 8.24%, Market Volatility: 10.09%, Sharpe: 1.6707, Adjusted Sharpe: 1.5496\n",
      "Strategy Volatility: 22.49%, Market Volatility: 16.21%, Sharpe: 0.1740, Adjusted Sharpe: 0.1466\n",
      "Strategy Volatility: 12.57%, Market Volatility: 11.79%, Sharpe: 1.5189, Adjusted Sharpe: 1.5134\n",
      "Strategy Volatility: 44.33%, Market Volatility: 26.01%, Sharpe: 0.8171, Adjusted Sharpe: 0.5431\n",
      "Strategy Volatility: 12.97%, Market Volatility: 12.50%, Sharpe: 1.9830, Adjusted Sharpe: 1.9830\n",
      "Strategy Volatility: 37.14%, Market Volatility: 23.74%, Sharpe: -0.4369, Adjusted Sharpe: -0.3201\n",
      "Strategy Volatility: 14.31%, Market Volatility: 14.79%, Sharpe: 0.7714, Adjusted Sharpe: 0.7714\n",
      "Strategy Volatility: 9.05%, Market Volatility: 12.39%, Sharpe: 2.2862, Adjusted Sharpe: 1.4621\n",
      "Strategy Volatility: 20.07%, Market Volatility: 16.28%, Sharpe: 0.3150, Adjusted Sharpe: 0.2772\n",
      "\n",
      "Mean CV Score: 0.7943, std: 0.6604\n",
      "Strategy Volatility: 22.71%, Market Volatility: 17.49%, Sharpe: 0.4629, Adjusted Sharpe: 0.4214\n",
      " Overall Score Calculation: 0.4214\n",
      "Final Evaluation Score: 0.4214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate(excessarg: int) -> float:\n",
    "    \"\"\"\n",
    "    Main evaluation function for FunSearch. It loads the data\n",
    "    and runs the solver which performs cross-validation.\n",
    "    \"\"\"\n",
    "    full_train_df = pl.read_csv(TRAIN_DATA_PATH)\n",
    "    # Use a slice of data for faster evaluation runs during development\n",
    "    df_raw = full_train_df.slice(4000)\n",
    "    print(df_raw.shape)\n",
    "\n",
    "    #fill nulls in df with mean\n",
    "    df = df_raw.with_columns(\n",
    "        # Select all numeric columns for the operation\n",
    "        pl.selectors.numeric()\n",
    "          # Step 1: Attempt to fill with the rolling mean of each respective column\n",
    "          .fill_null(\n",
    "              pl.selectors.numeric().rolling_mean(window_size=5, min_samples=1)\n",
    "          )\n",
    "          # Step 2: Fall back to the global column mean for any remaining nulls\n",
    "          #.fill_null(strategy='mean')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "    pl.col(\"date_id\").cast(pl.Int64)\n",
    "    )\n",
    "    \n",
    "    weekday_df = add_weekday_column(SPY_DATA_PATH)\n",
    "    print(\"\\n--- Joining weekday feature onto sliced data ---\")\n",
    "    # Join the weekday information onto the sliced training data.\n",
    "    # A 'left' join ensures we keep all rows from the original `df`.\n",
    "    df_with_features = df.join(weekday_df, on=\"date_id\", how=\"left\")\n",
    "    # print(\"DataFrame after join:\")\n",
    "    # print(df_with_features.shape)\n",
    "    return solve(df_with_features)\n",
    "  \n",
    "def add_weekday_column(input_csv_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file, adds a 'weekday' column based on the 'Date' column,\n",
    "    and saves the result to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): The path to the source CSV file.\n",
    "        output_csv_path (str): The path where the output CSV will be saved.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a Polars DataFrame\n",
    "    df = pl.read_csv(input_csv_path)\n",
    "\n",
    "    # Add a new column named 'weekday'\n",
    "    # 1. Select the 'Date' column.\n",
    "    # 2. Convert the string representation to a proper date type.\n",
    "    # 3. Use the .dt.weekday() function to get the day of the week (Monday=1, Sunday=7).\n",
    "    # 4. Alias the new expression to 'weekday'.\n",
    "    df_with_weekday = df.with_columns(\n",
    "        pl.col(\"Date\").str.to_date().dt.weekday().alias(\"weekday\")\n",
    "    )\n",
    "\n",
    "    # Print the transformed DataFrame to the console to show the result\n",
    "    returned_df = df_with_weekday.select([\"date_id\", \"weekday\"])\n",
    "    return returned_df\n",
    "\n",
    "# def solve(df: pl.DataFrame) -> float:\n",
    "#     \"\"\"\n",
    "#     A placeholder for the user's actual solving/modeling function.\n",
    "#     This dummy function just prints the DataFrame it receives to show\n",
    "#     that the join operation was successful.\n",
    "#     \"\"\"\n",
    "#     print(\"--- DataFrame passed to solve() ---\")\n",
    "#     print(df.shape)\n",
    "#     # In a real scenario, this would return a score based on a model's performance\n",
    "#     # If the dataframe passed here is empty, a CV function would fail.\n",
    "#     if df.height == 0:\n",
    "#         raise ValueError(\"DataFrame passed to solve() is empty.\")\n",
    "#     return 1.0\n",
    "\n",
    "def solve(df: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Runs a full time-series cross-validation process to evaluate\n",
    "    the features generated by the `generate_features` function.\n",
    "    This version pre-generates features to avoid a CPU bottleneck.\n",
    "    \"\"\"\n",
    "    # --- Helper functions (unchanged) ---\n",
    "    def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "        ''' Calculates the competition score based on true values and predicted signals. '''\n",
    "        solution = y_true_df.to_pandas()\n",
    "        solution['position'] = y_pred_signals\n",
    "        solution['strategy_returns'] = (\n",
    "            solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "            solution['position'] * solution['forward_returns']\n",
    "        )\n",
    "        strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "        strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        strategy_std = solution['strategy_returns'].std()\n",
    "        if strategy_std == 0: return 0.0\n",
    "        trading_days_per_yr = 252\n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "        market_std = solution['forward_returns'].std()\n",
    "        market_volatility = market_std * np.sqrt(trading_days_per_yr) * 100\n",
    "        strategy_volatility = strategy_std * np.sqrt(trading_days_per_yr) * 100\n",
    "        excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "        vol_penalty = 1 + excess_vol\n",
    "        market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "        market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * trading_days_per_yr)\n",
    "        return_penalty = 1 + (return_gap**2) / 100\n",
    "        adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "        print(f\"Strategy Volatility: {strategy_volatility:.2f}%, Market Volatility: {market_volatility:.2f}%, Sharpe: {sharpe:.4f}, Adjusted Sharpe: {adjusted_sharpe:.4f}\")\n",
    "        return adjusted_sharpe\n",
    "\n",
    "    def convert_to_signal(predictions: np.ndarray, multiplier: float = 300.0) -> np.ndarray:\n",
    "        ''' Converts raw model predictions into trading signals in the range [0, 2]. '''\n",
    "        signals = predictions * multiplier + 0.8\n",
    "        return np.clip(signals, 0.0, 2.0)\n",
    "\n",
    "    # --- Feature Generation and Data Prep (Moved Outside the Loop) ---\n",
    "    #print(\"Preparing data and generating features once...\")\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    # 1. Prepare base data\n",
    "    base_df = df.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    print(f\"Base DataFrame shape before cleaning: {base_df.shape}\")\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    #base_df.write_csv(\"debug_base_df_before_cleaning.csv\")\n",
    "    base_df = base_df.drop('E7')\n",
    "    print(f\"Base DataFrame shape during cleaning: {base_df.shape}\")\n",
    "    \n",
    "    df = df.with_columns(\n",
    "    pl.col(\"date_id\").cast(pl.Int64)\n",
    "    )\n",
    "    base_df = base_df.with_columns(pl.all().forward_fill())\n",
    "    #base_df = base_df.with_columns(pl.all().forward_fill()).drop_nulls()\n",
    "    print(f\"Base DataFrame shape after cleaning: {base_df.shape}\")\n",
    "\n",
    "\n",
    "    # 2. Generate new features using the evolved function\n",
    "    new_features_df_1 = generate_features_1(base_df)\n",
    "    new_features_df_2 = generate_features_2(base_df)\n",
    "    new_features_df_3 = generate_features_3(base_df)\n",
    "    new_features_df_4 = generate_features_4(base_df)\n",
    "    new_features_df_5 = generate_features_5(base_df)\n",
    "    new_features_df_6 = generate_features_6(base_df)\n",
    "    new_features_df_7 = generate_features_7(base_df)\n",
    "    newf = [new_features_df_1, new_features_df_2, new_features_df_3, new_features_df_4,new_features_df_5, new_features_df_6]\n",
    "    newf = [new_features_df_1, new_features_df_2, new_features_df_6]\n",
    "    newf = [new_features_df_1, new_features_df_5]\n",
    "    new_features_df = pl.concat(newf, how=\"horizontal\")\n",
    "    new_features_df = generate_features_7(base_df)\n",
    "\n",
    "    # 3. Combine base data with new features\n",
    "    processed_df = pl.concat([base_df, new_features_df], how=\"horizontal\")\n",
    "    #processed_df = base_df\n",
    "\n",
    "    print(f\"Processed DataFrame shape: {processed_df.shape}\")\n",
    "\n",
    "    # 4. Set up data for modeling\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_feature_names = new_features_df.columns\n",
    "    ALL_FEATURES = base_features + new_feature_names\n",
    "    #ALL_FEATURES = ['']\n",
    "    TARGET_COL = \"target\"\n",
    "\n",
    "    X = processed_df.select(ALL_FEATURES)\n",
    "    y = processed_df.select(TARGET_COL)\n",
    "    scorer_info_df = processed_df.select([\"forward_returns\", \"risk_free_rate\"])\n",
    "\n",
    "    # --- Time-Series Cross-Validation (Loop is now much lighter) ---\n",
    "    # print(\"Starting cross-validation loop...\")\n",
    "    nsplits = 20\n",
    "    tscv = TimeSeriesSplit(n_splits=nsplits)\n",
    "    cv_scores = []\n",
    "    entire_signal = []\n",
    "    entire_ytest = scorer_info_df.clear()\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        #print(f\"  Starting Fold {i+1}/{nsplits}...\")\n",
    "        # X_train, X_test = X[train_index], X[test_index]\n",
    "        # y_train = y[train_index]\n",
    "        # y_test_info = scorer_info_df[test_index]\n",
    "\n",
    "        # 1. Get the raw Polars dataframes for this split\n",
    "        X_train_raw = X[train_index]\n",
    "        X_test_raw = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test_info = scorer_info_df[test_index]\n",
    "\n",
    "        # --- START: Normalization Code ---\n",
    "        # 2. Initialize the scaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # 3. Fit the scaler ONLY on the training data (converted to NumPy)\n",
    "        #    and transform the training data\n",
    "        X_train = scaler.fit_transform(X_train_raw.to_numpy())\n",
    "\n",
    "        # 4. Transform the test data using the SAME scaler\n",
    "        X_test = scaler.transform(X_test_raw.to_numpy())\n",
    "        # --- END: Normalization Code ---\n",
    "\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective='reg:absoluteerror', n_estimators=50, device='cuda',\n",
    "            learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8,\n",
    "            n_jobs=-1, random_state=42\n",
    "        )\n",
    "        \n",
    "        # GPU is now the primary worker here\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "        predictions = model.predict(X_test)\n",
    "        signals = convert_to_signal(predictions)\n",
    "        score = calculate_competition_score(y_test_info, signals)\n",
    "        cv_scores.append(score)\n",
    "        entire_signal.extend(signals)\n",
    "        entire_ytest.extend(y_test_info)\n",
    "        #print(f\"  Fold {i+1}/{nsplits} Score: {score:.4f}\")\n",
    "\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    print(f\"\\nMean CV Score: {mean_score:.4f}, std: {np.std(cv_scores):.4f}\")\n",
    "    overall_score = calculate_competition_score(entire_ytest, entire_signal)\n",
    "    print(f\" Overall Score Calculation: {overall_score:.4f}\")\n",
    "    return overall_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example call to evaluate function\n",
    "    final_score = evaluate(0)\n",
    "    print(f\"Final Evaluation Score: {final_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2b12211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4990, 98)\n",
      "\n",
      "--- Joining weekday feature onto sliced data ---\n",
      "Initial DataFrame shape: (4990, 99)\n",
      "Base DataFrame shape before cleaning: (4990, 99)\n",
      "Base DataFrame shape during cleaning: (4990, 98)\n",
      "Base DataFrame shape after cleaning: (4990, 98)\n",
      "Processed DataFrame shape: (4990, 149)\n",
      "Strategy Volatility: 22.72%, Market Volatility: 13.21%, Sharpe: 0.5171, Adjusted Sharpe: 0.3402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/polars/series/series.py:1571: RuntimeWarning: invalid value encountered in log\n",
      "  lambda out: ufunc(*args, out=out, dtype=dtype_char, **kwargs),\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 27.34%, Market Volatility: 21.92%, Sharpe: -0.6796, Adjusted Sharpe: -0.6488\n",
      "Strategy Volatility: 39.06%, Market Volatility: 33.66%, Sharpe: 0.0511, Adjusted Sharpe: 0.0511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 23.66%, Market Volatility: 18.61%, Sharpe: 0.4758, Adjusted Sharpe: 0.4441\n",
      "Strategy Volatility: 15.15%, Market Volatility: 13.14%, Sharpe: 1.6948, Adjusted Sharpe: 1.6948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 30.63%, Market Volatility: 22.43%, Sharpe: 0.3833, Adjusted Sharpe: 0.3288\n",
      "Strategy Volatility: 10.85%, Market Volatility: 11.85%, Sharpe: 1.8170, Adjusted Sharpe: 1.7690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 11.26%, Market Volatility: 10.99%, Sharpe: 1.4227, Adjusted Sharpe: 1.3972\n",
      "Strategy Volatility: 18.31%, Market Volatility: 11.69%, Sharpe: 1.2347, Adjusted Sharpe: 0.9037\n",
      "Strategy Volatility: 24.12%, Market Volatility: 16.89%, Sharpe: -0.0653, Adjusted Sharpe: -0.0530\n",
      "Strategy Volatility: 10.79%, Market Volatility: 9.95%, Sharpe: 2.3794, Adjusted Sharpe: 2.3794\n",
      "Strategy Volatility: 10.70%, Market Volatility: 10.09%, Sharpe: 1.6165, Adjusted Sharpe: 1.6165\n",
      "Strategy Volatility: 22.26%, Market Volatility: 16.21%, Sharpe: 0.0217, Adjusted Sharpe: 0.0185\n",
      "Strategy Volatility: 16.14%, Market Volatility: 11.79%, Sharpe: 1.6787, Adjusted Sharpe: 1.4357\n",
      "Strategy Volatility: 36.66%, Market Volatility: 26.01%, Sharpe: 0.6549, Adjusted Sharpe: 0.5415\n",
      "Strategy Volatility: 10.36%, Market Volatility: 12.50%, Sharpe: 1.8407, Adjusted Sharpe: 1.6233\n",
      "Strategy Volatility: 24.92%, Market Volatility: 23.74%, Sharpe: -1.2588, Adjusted Sharpe: -0.5034\n",
      "Strategy Volatility: 14.94%, Market Volatility: 14.79%, Sharpe: 0.3333, Adjusted Sharpe: 0.3065\n",
      "Strategy Volatility: 11.04%, Market Volatility: 12.39%, Sharpe: 2.6545, Adjusted Sharpe: 2.6545\n",
      "Strategy Volatility: 21.32%, Market Volatility: 16.28%, Sharpe: 0.4566, Adjusted Sharpe: 0.4115\n",
      "\n",
      "Mean CV Score: 0.8356, std: 0.9078\n",
      "Strategy Volatility: 21.81%, Market Volatility: 17.49%, Sharpe: 0.5199, Adjusted Sharpe: 0.4964\n",
      " Overall Score Calculation: 0.4964\n",
      "Direct Learning Position: Final Evaluation Score: 0.4964\n"
     ]
    }
   ],
   "source": [
    "def evaluate(excessarg: int) -> float:\n",
    "    \"\"\"\n",
    "    Main evaluation function for FunSearch. It loads the data\n",
    "    and runs the solver which performs cross-validation.\n",
    "    \"\"\"\n",
    "    full_train_df = pl.read_csv(TRAIN_DATA_PATH)\n",
    "    # Use a slice of data for faster evaluation runs during development\n",
    "    df_raw = full_train_df.slice(4000)\n",
    "    print(df_raw.shape)\n",
    "\n",
    "    #fill nulls in df with mean\n",
    "    df = df_raw.with_columns(\n",
    "        # Select all numeric columns for the operation\n",
    "        pl.selectors.numeric()\n",
    "          # Step 1: Attempt to fill with the rolling mean of each respective column\n",
    "          .fill_null(\n",
    "              pl.selectors.numeric().rolling_mean(window_size=5, min_samples=1)\n",
    "          )\n",
    "          # Step 2: Fall back to the global column mean for any remaining nulls\n",
    "          #.fill_null(strategy='mean')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "    pl.col(\"date_id\").cast(pl.Int64)\n",
    "    )\n",
    "    \n",
    "    weekday_df = add_weekday_column(SPY_DATA_PATH)\n",
    "    print(\"\\n--- Joining weekday feature onto sliced data ---\")\n",
    "    # Join the weekday information onto the sliced training data.\n",
    "    # A 'left' join ensures we keep all rows from the original `df`.\n",
    "    df_with_features = df.join(weekday_df, on=\"date_id\", how=\"left\")\n",
    "    # print(\"DataFrame after join:\")\n",
    "    # print(df_with_features.shape)\n",
    "    return solve(df_with_features)\n",
    "  \n",
    "def add_weekday_column(input_csv_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file, adds a 'weekday' column based on the 'Date' column,\n",
    "    and saves the result to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): The path to the source CSV file.\n",
    "        output_csv_path (str): The path where the output CSV will be saved.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a Polars DataFrame\n",
    "    df = pl.read_csv(input_csv_path)\n",
    "\n",
    "    # Add a new column named 'weekday'\n",
    "    # 1. Select the 'Date' column.\n",
    "    # 2. Convert the string representation to a proper date type.\n",
    "    # 3. Use the .dt.weekday() function to get the day of the week (Monday=1, Sunday=7).\n",
    "    # 4. Alias the new expression to 'weekday'.\n",
    "    df_with_weekday = df.with_columns(\n",
    "        pl.col(\"Date\").str.to_date().dt.weekday().alias(\"weekday\")\n",
    "    )\n",
    "\n",
    "    # Print the transformed DataFrame to the console to show the result\n",
    "    returned_df = df_with_weekday.select([\"date_id\", \"weekday\"])\n",
    "    return returned_df\n",
    "\n",
    "\n",
    "def solve(df: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Predicts pre computed optimal postions, rather than forward return\n",
    "    \"\"\"\n",
    "    # --- Helper functions (unchanged) ---\n",
    "    def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "        ''' Calculates the competition score based on true values and predicted signals. '''\n",
    "        solution = y_true_df.to_pandas()\n",
    "        solution['position'] = y_pred_signals\n",
    "        solution['position'] = solution['position'].clip(0, 2)\n",
    "        solution['strategy_returns'] = (\n",
    "            solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "            solution['position'] * solution['forward_returns']\n",
    "        )\n",
    "        strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "        strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        strategy_std = solution['strategy_returns'].std()\n",
    "        if strategy_std == 0: return 0.0\n",
    "        trading_days_per_yr = 252\n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "        market_std = solution['forward_returns'].std()\n",
    "        market_volatility = market_std * np.sqrt(trading_days_per_yr) * 100\n",
    "        strategy_volatility = strategy_std * np.sqrt(trading_days_per_yr) * 100\n",
    "        excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "        vol_penalty = 1 + excess_vol\n",
    "        market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "        market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * trading_days_per_yr)\n",
    "        return_penalty = 1 + (return_gap**2) / 100\n",
    "        adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "        print(f\"Strategy Volatility: {strategy_volatility:.2f}%, Market Volatility: {market_volatility:.2f}%, Sharpe: {sharpe:.4f}, Adjusted Sharpe: {adjusted_sharpe:.4f}\")\n",
    "        return adjusted_sharpe\n",
    "\n",
    "    def convert_to_signal(predictions: np.ndarray, multiplier: float = 300.0) -> np.ndarray:\n",
    "        ''' Converts raw model predictions into trading signals in the range [0, 2]. '''\n",
    "        return np.clip(predictions, 0.0, 2.0)\n",
    "\n",
    "    # --- Feature Generation and Data Prep (Moved Outside the Loop) ---\n",
    "    #print(\"Preparing data and generating features once...\")\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    # 1. Prepare base data\n",
    "    base_df = df.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    print(f\"Base DataFrame shape before cleaning: {base_df.shape}\")\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    #base_df.write_csv(\"debug_base_df_before_cleaning.csv\")\n",
    "    base_df = base_df.drop('E7')\n",
    "    print(f\"Base DataFrame shape during cleaning: {base_df.shape}\")\n",
    "    \n",
    "    df = df.with_columns(\n",
    "    pl.col(\"date_id\").cast(pl.Int64)\n",
    "    )\n",
    "    base_df = base_df.with_columns(pl.all().forward_fill())\n",
    "    #base_df = base_df.with_columns(pl.all().forward_fill()).drop_nulls()\n",
    "    print(f\"Base DataFrame shape after cleaning: {base_df.shape}\")\n",
    "\n",
    "\n",
    "    # 2. Generate new features using the evolved function\n",
    "    # new_features_df_1 = generate_features_1(base_df)\n",
    "    # new_features_df_2 = generate_features_2(base_df)\n",
    "    # new_features_df_3 = generate_features_3(base_df)\n",
    "    # new_features_df_4 = generate_features_4(base_df)\n",
    "    # new_features_df_5 = generate_features_5(base_df)\n",
    "    # new_features_df_6 = generate_features_6(base_df)\n",
    "    # new_features_df_7 = generate_features_7(base_df)\n",
    "    # newf = [new_features_df_1, new_features_df_2, new_features_df_3, new_features_df_4,new_features_df_5, new_features_df_6]\n",
    "    # newf = [new_features_df_1, new_features_df_2, new_features_df_6]\n",
    "    # newf = [new_features_df_1, new_features_df_5]\n",
    "    # new_features_df = pl.concat(newf, how=\"horizontal\")\n",
    "    new_features_df = generate_features_7(base_df)\n",
    "\n",
    "    # 3. Combine base data with new features\n",
    "    processed_df = pl.concat([base_df, new_features_df], how=\"horizontal\")\n",
    "    #processed_df = base_df\n",
    "\n",
    "    print(f\"Processed DataFrame shape: {processed_df.shape}\")\n",
    "\n",
    "    # 4. Set up data for modeling\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_feature_names = new_features_df.columns\n",
    "    ALL_FEATURES = base_features + new_feature_names\n",
    "    #ALL_FEATURES = ['']\n",
    "    TARGET_COL = \"target\"\n",
    "\n",
    "    X = processed_df.select(ALL_FEATURES)\n",
    "    y = processed_df.select(TARGET_COL)\n",
    "    scorer_info_df = processed_df.select([\"forward_returns\", \"risk_free_rate\"])\n",
    "\n",
    "    # --- Time-Series Cross-Validation (Loop is now much lighter) ---\n",
    "    # print(\"Starting cross-validation loop...\")\n",
    "    nsplits = 20\n",
    "    tscv = TimeSeriesSplit(n_splits=nsplits)\n",
    "    cv_scores = []\n",
    "    entire_signal = []\n",
    "    entire_ytest = scorer_info_df.clear()\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        #print(f\"  Starting Fold {i+1}/{nsplits}...\")\n",
    "        # X_train, X_test = X[train_index], X[test_index]\n",
    "        # y_train = y[train_index]\n",
    "        # y_test_info = scorer_info_df[test_index]\n",
    "\n",
    "        # 1. Get the raw Polars dataframes for this split\n",
    "        X_train_raw = X[train_index]\n",
    "        X_test_raw = X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test_info = scorer_info_df[test_index]\n",
    "\n",
    "        # Get the original 'Y' (forward return) for this training fold\n",
    "        y_train_original = y[train_index] \n",
    "        \n",
    "        # This has the original 'Y' for scoring the test set\n",
    "        y_test_info = scorer_info_df[test_index]\n",
    "\n",
    "        # --- START: New Target Transformation ---\n",
    "        # 1. Convert the original Y target to a NumPy array\n",
    "        y_train_np = y_train_original.to_numpy()\n",
    "        \n",
    "        # 2. Create a new empty array to hold the transformed target\n",
    "        y_train_pos = np.zeros_like(y_train_np)\n",
    "        \n",
    "        # 3. Loop through and apply your custom logic\n",
    "        for j in range(len(y_train_np)):\n",
    "            y_val = y_train_np[j]\n",
    "            \n",
    "            if y_val > 0:\n",
    "                # Logic for positive returns\n",
    "                y_train_pos[j] = max(3.0 - 100*y_val, 0.3)\n",
    "            elif y_val == 0:\n",
    "                # Logic for zero returns\n",
    "                y_train_pos[j] = 0.5\n",
    "            else:\n",
    "                # Logic for non-positive (<= 0) returns\n",
    "                y_train_pos[j] = 0.2\n",
    "\n",
    "        # --- END: New Target Transformation ---\n",
    "\n",
    "        # --- START: Normalization Code ---\n",
    "        # 2. Initialize the scaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # 3. Fit the scaler ONLY on the training data (converted to NumPy)\n",
    "        #    and transform the training data\n",
    "        X_train = scaler.fit_transform(X_train_raw.to_numpy())\n",
    "\n",
    "        # 4. Transform the test data using the SAME scaler\n",
    "        X_test = scaler.transform(X_test_raw.to_numpy())\n",
    "        # --- END: Normalization Code ---\n",
    "\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective='reg:absoluteerror', n_estimators=50, device='cuda',\n",
    "            learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8,\n",
    "            n_jobs=-1, random_state=42\n",
    "        )\n",
    "        \n",
    "        # GPU is now the primary worker here\n",
    "        model.fit(X_train, y_train_pos, verbose=False)\n",
    "\n",
    "        predictions = model.predict(X_test)\n",
    "        signals = convert_to_signal(predictions)\n",
    "        score = calculate_competition_score(y_test_info, signals)\n",
    "        cv_scores.append(score)\n",
    "        entire_signal.extend(signals)\n",
    "        entire_ytest.extend(y_test_info)\n",
    "        #print(f\"  Fold {i+1}/{nsplits} Score: {score:.4f}\")\n",
    "\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    print(f\"\\nMean CV Score: {mean_score:.4f}, std: {np.std(cv_scores):.4f}\")\n",
    "\n",
    "    # true_returns = entire_ytest.get_column(\"forward_returns\").to_numpy()\n",
    "    # for jj in range(len(true_returns)):\n",
    "    #     if true_returns[jj] > 0:\n",
    "    #         entire_signal[jj] = max(2.0 - 100*true_returns[jj], 0.3)\n",
    "    #         #entire_signal[jj] = 0.8 + 400*true_returns[jj]\n",
    "    #     elif true_returns[jj] == 0:\n",
    "    #         entire_signal[jj] = 0.5\n",
    "    #         #entire_signal[jj] = 0.8 + 400*true_returns[jj]\n",
    "\n",
    "    #     else:\n",
    "    #         entire_signal[jj] = 0.1\n",
    "    #         #entire_signal[jj] = 0.8 + 400*true_returns[jj]\n",
    "    overall_score = calculate_competition_score(entire_ytest, entire_signal)\n",
    "    print(f\" Overall Score Calculation: {overall_score:.4f}\")\n",
    "    return overall_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example call to evaluate function\n",
    "    final_score = evaluate(0)\n",
    "    print(f\"Direct Learning Position: Final Evaluation Score: {final_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66af5650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4990, 98)\n",
      "\n",
      "--- Joining weekday feature onto sliced data ---\n",
      "Strategy Volatility: 29.02%, Market Volatility: 18.04%, Sharpe: -0.1376, Adjusted Sharpe: -0.0977\n",
      "Strategy Volatility: 19.91%, Market Volatility: 12.06%, Sharpe: -0.6682, Adjusted Sharpe: -0.3534\n",
      "Strategy Volatility: 28.14%, Market Volatility: 18.73%, Sharpe: 0.2388, Adjusted Sharpe: 0.1727\n",
      "Strategy Volatility: 15.72%, Market Volatility: 11.29%, Sharpe: 0.3979, Adjusted Sharpe: 0.3187\n",
      "Strategy Volatility: 16.21%, Market Volatility: 13.57%, Sharpe: 2.0374, Adjusted Sharpe: 2.0374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/polars/series/series.py:1571: RuntimeWarning: invalid value encountered in log\n",
      "  lambda out: ufunc(*args, out=out, dtype=dtype_char, **kwargs),\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 61.86%, Market Volatility: 33.47%, Sharpe: -0.3417, Adjusted Sharpe: -0.1078\n",
      "Strategy Volatility: 20.21%, Market Volatility: 19.49%, Sharpe: 2.2324, Adjusted Sharpe: 2.2324\n",
      "Strategy Volatility: 18.39%, Market Volatility: 15.95%, Sharpe: 2.5822, Adjusted Sharpe: 2.3098\n",
      "Strategy Volatility: 19.95%, Market Volatility: 12.90%, Sharpe: 1.7831, Adjusted Sharpe: 1.3246\n",
      "Strategy Volatility: 17.41%, Market Volatility: 10.75%, Sharpe: 2.3257, Adjusted Sharpe: 1.6387\n",
      "Strategy Volatility: 30.53%, Market Volatility: 19.82%, Sharpe: -0.7726, Adjusted Sharpe: -0.1490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 47.65%, Market Volatility: 25.52%, Sharpe: -0.4651, Adjusted Sharpe: -0.2476\n",
      "Strategy Volatility: 32.72%, Market Volatility: 23.90%, Sharpe: -1.1266, Adjusted Sharpe: -0.4426\n",
      "Strategy Volatility: 18.46%, Market Volatility: 14.92%, Sharpe: 1.1653, Adjusted Sharpe: 1.1234\n",
      "Strategy Volatility: 8.91%, Market Volatility: 11.06%, Sharpe: 2.6834, Adjusted Sharpe: 2.6834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 12.34%, Market Volatility: 11.87%, Sharpe: 2.2307, Adjusted Sharpe: 0.8824\n",
      "Strategy Volatility: 12.58%, Market Volatility: 10.33%, Sharpe: 1.6605, Adjusted Sharpe: 1.6304\n",
      "Strategy Volatility: 18.00%, Market Volatility: 14.38%, Sharpe: 1.2630, Adjusted Sharpe: 1.2008\n",
      "Strategy Volatility: 37.35%, Market Volatility: 21.45%, Sharpe: -1.2254, Adjusted Sharpe: -0.3728\n",
      "Strategy Volatility: 17.23%, Market Volatility: 11.31%, Sharpe: 3.4064, Adjusted Sharpe: 2.5749\n",
      "\n",
      "Mean CV Score: 0.9179, std: 1.0660\n",
      "Final Evaluation Score: 0.9179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def solve(df: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Runs a full time-series cross-validation process to evaluate\n",
    "    the features generated by the `generate_features` function.\n",
    "    This version uses a LightGBM model.\n",
    "    \"\"\"\n",
    "    # --- Helper functions (unchanged) ---\n",
    "    def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "        ''' Calculates the competition score based on true values and predicted signals. '''\n",
    "        solution = y_true_df.to_pandas()\n",
    "        solution['position'] = y_pred_signals\n",
    "        solution['strategy_returns'] = (\n",
    "            solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "            solution['position'] * solution['forward_returns']\n",
    "        )\n",
    "        strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "        strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        strategy_std = solution['strategy_returns'].std()\n",
    "        if strategy_std == 0: return 0.0\n",
    "        trading_days_per_yr = 252\n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "        market_std = solution['forward_returns'].std()\n",
    "        market_volatility = market_std * np.sqrt(trading_days_per_yr) * 100\n",
    "        strategy_volatility = strategy_std * np.sqrt(trading_days_per_yr) * 100\n",
    "        excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "        vol_penalty = 1 + excess_vol\n",
    "        market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "        market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * trading_days_per_yr)\n",
    "        return_penalty = 1 + (return_gap**2) / 100\n",
    "        adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "        print(f\"Strategy Volatility: {strategy_volatility:.2f}%, Market Volatility: {market_volatility:.2f}%, Sharpe: {sharpe:.4f}, Adjusted Sharpe: {adjusted_sharpe:.4f}\")\n",
    "        return adjusted_sharpe\n",
    "\n",
    "    def convert_to_signal(predictions: np.ndarray, multiplier: float = 400.0) -> np.ndarray:\n",
    "        ''' Converts raw model predictions into trading signals in the range [0, 2]. '''\n",
    "        signals = predictions * multiplier + 1\n",
    "        return np.clip(signals, 0.0, 2.0)\n",
    "\n",
    "    # --- Feature Generation and Data Prep ---\n",
    "    base_df = df.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    base_df = base_df.with_columns(pl.all().forward_fill()).drop_nulls()\n",
    "\n",
    "    # Generate and combine features\n",
    "    # new_features_df_1 = generate_features_1(base_df)\n",
    "    # new_features_df_2 = generate_features_2(base_df)\n",
    "    # new_features_df_3 = generate_features_3(base_df)\n",
    "    # new_features_df_4 = generate_features_4(base_df)\n",
    "    # new_features_df_5 = generate_features_5(base_df)\n",
    "    # new_features_df_6 = generate_features_6(base_df)\n",
    "    # #newf = [new_features_df_1, new_features_df_2, new_features_df_3, new_features_df_4, new_features_df_5, new_features_df_6]\n",
    "    # newf = [new_features_df_1, new_features_df_2, new_features_df_3, new_features_df_5, new_features_df_6]\n",
    "    # new_features_df = pl.concat(newf, how=\"horizontal\")\n",
    "    new_features_df = generate_features_7(base_df)\n",
    "    \n",
    "    processed_df = pl.concat([base_df, new_features_df], how=\"horizontal\")\n",
    "\n",
    "    # Setup data for modeling\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_feature_names = new_features_df.columns\n",
    "    ALL_FEATURES = base_features + new_feature_names\n",
    "    TARGET_COL = \"target\"\n",
    "\n",
    "    X = processed_df.select(ALL_FEATURES)\n",
    "    y = processed_df.select(TARGET_COL)\n",
    "    scorer_info_df = processed_df.select([\"forward_returns\", \"risk_free_rate\"])\n",
    "\n",
    "    # --- Time-Series Cross-Validation ---\n",
    "    nsplits = 20\n",
    "    tscv = TimeSeriesSplit(n_splits=nsplits)\n",
    "    cv_scores = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        y_test_info = scorer_info_df[test_index]\n",
    "\n",
    "        # Define the LightGBM model\n",
    "        model = lgb.LGBMRegressor(\n",
    "            objective='regression_l1',  # MAE, an alternative to squared error\n",
    "            n_estimators=50,\n",
    "            device='cpu',              # Use 'cpu' if you don't have a compatible GPU\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,             # Equivalent to 'bagging_fraction'\n",
    "            colsample_bytree=0.8,      # Equivalent to 'feature_fraction'\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=-1                 # Suppress verbose output\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        # Convert to NumPy arrays before fitting and predicting\n",
    "        model.fit(X_train.to_numpy(), y_train.to_numpy().ravel())\n",
    "\n",
    "        predictions = model.predict(X_test.to_numpy())\n",
    "        signals = convert_to_signal(predictions)\n",
    "        score = calculate_competition_score(y_test_info, signals)\n",
    "        cv_scores.append(score)\n",
    "\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    print(f\"\\nMean CV Score: {mean_score:.4f}, std: {np.std(cv_scores):.4f}\")\n",
    "    return mean_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example call to evaluate function\n",
    "    final_score = evaluate(0)\n",
    "    print(f\"Final Evaluation Score: {final_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "891e48e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4990, 98)\n",
      "Starting LSTM cross-validation with LOOKBACK=10...\n",
      "  Starting Fold 1/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/polars/series/series.py:1571: RuntimeWarning: invalid value encountered in log\n",
      "  lambda out: ufunc(*args, out=out, dtype=dtype_char, **kwargs),\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 21.82%, Market Volatility: 28.77%, Sharpe: 0.2020, Adjusted Sharpe: 0.2020\n",
      "  Fold 1/10 Score: 0.2020\n",
      "  Starting Fold 2/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 15.06%, Market Volatility: 16.55%, Sharpe: 1.6012, Adjusted Sharpe: 1.6012\n",
      "  Fold 2/10 Score: 1.6012\n",
      "  Starting Fold 3/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy Volatility: 30.77%, Market Volatility: 18.48%, Sharpe: 0.8547, Adjusted Sharpe: 0.5833\n",
      "  Fold 3/10 Score: 0.5833\n",
      "  Starting Fold 4/10...\n",
      "Strategy Volatility: 20.22%, Market Volatility: 11.09%, Sharpe: 1.7110, Adjusted Sharpe: 1.0541\n",
      "  Fold 4/10 Score: 1.0541\n",
      "  Starting Fold 5/10...\n",
      "Strategy Volatility: 16.73%, Market Volatility: 14.90%, Sharpe: -0.6085, Adjusted Sharpe: -0.1837\n",
      "  Fold 5/10 Score: -0.1837\n",
      "  Starting Fold 6/10...\n",
      "Strategy Volatility: 1.08%, Market Volatility: 10.61%, Sharpe: 0.5491, Adjusted Sharpe: 0.2097\n",
      "  Fold 6/10 Score: 0.2097\n",
      "  Starting Fold 7/10...\n",
      "Strategy Volatility: 28.08%, Market Volatility: 19.60%, Sharpe: 0.4607, Adjusted Sharpe: 0.3739\n",
      "  Fold 7/10 Score: 0.3739\n",
      "  Starting Fold 8/10...\n",
      "Strategy Volatility: 29.39%, Market Volatility: 15.51%, Sharpe: 1.9621, Adjusted Sharpe: 1.1574\n",
      "  Fold 8/10 Score: 1.1574\n",
      "  Starting Fold 9/10...\n",
      "Strategy Volatility: 38.23%, Market Volatility: 19.72%, Sharpe: -0.2548, Adjusted Sharpe: -0.0878\n",
      "  Fold 9/10 Score: -0.0878\n",
      "  Starting Fold 10/10...\n",
      "Strategy Volatility: 18.03%, Market Volatility: 14.59%, Sharpe: 0.5023, Adjusted Sharpe: 0.3294\n",
      "  Fold 10/10 Score: 0.3294\n",
      "\n",
      "Mean CV Score: 0.5239, std: 0.5463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5239453937354256"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import xgboost as xgb  # You can keep or remove this\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler  # <-- ADD THIS\n",
    "\n",
    "# --- ADD THESE FOR LSTM ---\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "# ---------------------------\n",
    "\n",
    "# The training data path should be updated to your actual training file.\n",
    "TRAIN_DATA_PATH = \"./kaggle/train.csv\"\n",
    "SPY_DATA_PATH = \"./kaggle/spy-historical.csv\"\n",
    "\n",
    "# Optional: Set a random seed for reproducibility in Keras\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "def create_sequences(X_data, y_data, lookback):\n",
    "    \"\"\"\n",
    "    Converts 2D feature data and 1D/2D target data into 3D sequences\n",
    "    for LSTM.\n",
    "    \n",
    "    X_data: NumPy array of features (samples, n_features)\n",
    "    y_data: NumPy array (y_train) or Polars DataFrame (y_test_info)\n",
    "    lookback: Number of timesteps to look back\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(lookback, len(X_data)):\n",
    "        # Get the sequence of features [t-lookback, ..., t-1]\n",
    "        X_seq.append(X_data[i-lookback:i, :])\n",
    "        \n",
    "        # Get the target at time [t]\n",
    "        y_seq.append(y_data[i])\n",
    "\n",
    "    # Convert lists to a NumPy array for features\n",
    "    X_seq_arr = np.array(X_seq)\n",
    "    \n",
    "    # Handle y_data (which could be a NumPy array or Polars DataFrame)\n",
    "    if isinstance(y_data, pl.DataFrame):\n",
    "        # This is the y_test_info DataFrame. Re-concat it.\n",
    "        y_seq_arr = pl.concat(y_seq)\n",
    "    else:\n",
    "        # This is the y_train NumPy array.\n",
    "        y_seq_arr = np.array(y_seq)\n",
    "        \n",
    "    return X_seq_arr, y_seq_arr\n",
    "\n",
    "def build_lstm_model(lookback, n_features):\n",
    "    \"\"\"\n",
    "    Builds a simple Keras LSTM model.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(lookback, n_features))\n",
    "    \n",
    "    # A simple LSTM layer\n",
    "    x = LSTM(units=50, return_sequences=False)(inputs)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer: 1 neuron for regression\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    # Using 'mae' (Mean Absolute Error) as the loss,\n",
    "    # since it's the same as the competition metric.\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate(individual_index: int) -> float:\n",
    "    \"\"\"\n",
    "    Trains and evaluates a model using time-series cross-validation.\n",
    "    This version is modified for an LSTM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Define new hyperparameters for LSTM ---\n",
    "    LOOKBACK = 10       # How many days to look back\n",
    "    EPOCHS = 20         # How many times to train on the data\n",
    "    BATCH_SIZE = 64     # How many samples per training update\n",
    "    # -------------------------------------------\n",
    "\n",
    "    # Load data (this logic remains the same)\n",
    "        # --- Helper functions (unchanged) ---\n",
    "    def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "        ''' Calculates the competition score based on true values and predicted signals. '''\n",
    "        solution = y_true_df.to_pandas()\n",
    "        solution['position'] = y_pred_signals\n",
    "        solution['strategy_returns'] = (\n",
    "            solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "            solution['position'] * solution['forward_returns']\n",
    "        )\n",
    "        strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "        strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        strategy_std = solution['strategy_returns'].std()\n",
    "        if strategy_std == 0: return 0.0\n",
    "        trading_days_per_yr = 252\n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "        market_std = solution['forward_returns'].std()\n",
    "        market_volatility = market_std * np.sqrt(trading_days_per_yr) * 100\n",
    "        strategy_volatility = strategy_std * np.sqrt(trading_days_per_yr) * 100\n",
    "        excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "        vol_penalty = 1 + excess_vol\n",
    "        market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "        market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * trading_days_per_yr)\n",
    "        return_penalty = 1 + (return_gap**2) / 100\n",
    "        adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "        print(f\"Strategy Volatility: {strategy_volatility:.2f}%, Market Volatility: {market_volatility:.2f}%, Sharpe: {sharpe:.4f}, Adjusted Sharpe: {adjusted_sharpe:.4f}\")\n",
    "        return adjusted_sharpe\n",
    "\n",
    "    def convert_to_signal(predictions: np.ndarray, multiplier: float = 400.0) -> np.ndarray:\n",
    "        ''' Converts raw model predictions into trading signals in the range [0, 2]. '''\n",
    "        signals = predictions * multiplier + 1\n",
    "        return np.clip(signals, 0.0, 2.0)\n",
    "\n",
    "    # --- Feature Generation and Data Prep ---\n",
    "    full_train_df = pl.read_csv(TRAIN_DATA_PATH)\n",
    "    # Use a slice of data for faster evaluation runs during development\n",
    "    df_raw = full_train_df.slice(4000)\n",
    "    print(df_raw.shape)\n",
    "\n",
    "    #fill nulls in df with mean\n",
    "    df = df_raw.with_columns(\n",
    "        # Select all numeric columns for the operation\n",
    "        pl.selectors.numeric()\n",
    "          # Step 1: Attempt to fill with the rolling mean of each respective column\n",
    "          .fill_null(\n",
    "              pl.selectors.numeric().rolling_mean(window_size=5, min_samples=1)\n",
    "          )\n",
    "          # Step 2: Fall back to the global column mean for any remaining nulls\n",
    "          #.fill_null(strategy='mean')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "    pl.col(\"date_id\").cast(pl.Int64)\n",
    "    )\n",
    "\n",
    "    base_df = df.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    base_df = base_df.with_columns(pl.all().forward_fill())#.drop_nulls()\n",
    "\n",
    "    # Generate and combine features\n",
    "    # new_features_df_1 = generate_features_1(base_df)\n",
    "    # new_features_df_2 = generate_features_2(base_df)\n",
    "    # new_features_df_3 = generate_features_3(base_df)\n",
    "    # new_features_df_4 = generate_features_4(base_df)\n",
    "    # new_features_df_5 = generate_features_5(base_df)\n",
    "    # new_features_df_6 = generate_features_6(base_df)\n",
    "    # #newf = [new_features_df_1, new_features_df_2, new_features_df_3, new_features_df_4, new_features_df_5, new_features_df_6]\n",
    "    # newf = [new_features_df_1, new_features_df_2, new_features_df_3, new_features_df_5, new_features_df_6]\n",
    "    # new_features_df = pl.concat(newf, how=\"horizontal\")\n",
    "    new_features_df = generate_features_7(base_df)\n",
    "    \n",
    "    processed_df = pl.concat([base_df, new_features_df], how=\"horizontal\").with_columns(\n",
    "        # Select all numeric columns for the operation\n",
    "        pl.selectors.numeric()\n",
    "          # Step 1: Attempt to fill with the rolling mean of each respective column\n",
    "          .fill_null(\n",
    "              pl.selectors.numeric().rolling_mean(window_size=5, min_samples=1)\n",
    "          )\n",
    "          # Step 2: Fall back to the global column mean for any remaining nulls\n",
    "          .fill_null(strategy='mean')\n",
    "    )\n",
    "\n",
    "    # Setup data for modeling\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_feature_names = new_features_df.columns\n",
    "    ALL_FEATURES = base_features + new_feature_names\n",
    "    TARGET_COL = \"target\"\n",
    "\n",
    "    X = processed_df.select(ALL_FEATURES)\n",
    "    y = processed_df.select(TARGET_COL)\n",
    "    scorer_info_df = processed_df.select([\"forward_returns\", \"risk_free_rate\"])\n",
    "    #X, y, scorer_info_df = load_and_preprocess_data(individual_index)\n",
    "    if X is None:\n",
    "        return 0.0\n",
    "\n",
    "    # Time-Series Cross-Validation\n",
    "    nsplits = 10  # Reduced splits, as NN training is slow\n",
    "    tscv = TimeSeriesSplit(n_splits=nsplits)\n",
    "    cv_scores = []\n",
    "    \n",
    "    print(f\"Starting LSTM cross-validation with LOOKBACK={LOOKBACK}...\")\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        print(f\"  Starting Fold {i+1}/{nsplits}...\")\n",
    "        \n",
    "        # 1. Get raw data for this split\n",
    "        X_train_raw = X[train_index]\n",
    "        X_test_raw = X[test_index]\n",
    "        y_train_raw = y[train_index].to_numpy() # Convert y_train to NumPy\n",
    "        y_test_info_raw = scorer_info_df[test_index]\n",
    "\n",
    "        # 2. --- NORMALIZATION ---\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_raw.to_numpy())\n",
    "        X_test_scaled = scaler.transform(X_test_raw.to_numpy())\n",
    "\n",
    "        # --- ADD THIS FIX ---\n",
    "        # Replace any NaNs/Infs (from divide-by-zero) with 0\n",
    "        X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        # --------------------\n",
    "\n",
    "        # 3. --- DATA RESHAPING (SEQUENCING) ---\n",
    "        X_train_seq, y_train_seq = create_sequences(\n",
    "            X_train_scaled, y_train_raw, LOOKBACK\n",
    "        )\n",
    "        \n",
    "        # Create 3D sequences for testing\n",
    "        # CRITICAL: We also sequence y_test_info_raw to keep the scores aligned\n",
    "        X_test_seq, y_test_info_seq = create_sequences(\n",
    "            X_test_scaled, y_test_info_raw, LOOKBACK\n",
    "        )\n",
    "        \n",
    "        # Stop if we don't have enough data to test (e.g., test set < LOOKBACK)\n",
    "        if len(X_test_seq) == 0:\n",
    "            print(f\"  Skipping Fold {i+1}: Not enough test data to create sequences.\")\n",
    "            continue\n",
    "\n",
    "        # 4. --- BUILD AND TRAIN MODEL ---\n",
    "        n_features = X_train_seq.shape[2]\n",
    "        model = build_lstm_model(LOOKBACK, n_features)\n",
    "        \n",
    "        # Use EarlyStopping to prevent overfitting\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss', patience=3, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train_seq, y_train_seq,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_split=0.1,  # Use 10% of train data for validation\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0  # Set to 1 to see training progress\n",
    "        )\n",
    "\n",
    "        # 5. --- PREDICT AND SCORE ---\n",
    "        # Predict on the 3D test sequences\n",
    "        predictions_scaled = model.predict(X_test_seq, verbose=0)\n",
    "        \n",
    "        # predictions will be (n_samples, 1), flatten to (n_samples,)\n",
    "        predictions = predictions_scaled.flatten()\n",
    "\n",
    "        signals = convert_to_signal(predictions)\n",
    "        \n",
    "        # Score against the SEQUENCED y_test_info DataFrame\n",
    "        score = calculate_competition_score(y_test_info_seq, signals)\n",
    "        cv_scores.append(score)\n",
    "        print(f\"  Fold {i+1}/{nsplits} Score: {score:.4f}\")\n",
    "\n",
    "    if not cv_scores:\n",
    "        print(\"No scores were recorded. Check data and LOOKBACK period.\")\n",
    "        return 0.0\n",
    "\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    print(f\"\\nMean CV Score: {mean_score:.4f}, std: {np.std(cv_scores):.4f}\")\n",
    "    return mean_score\n",
    "\n",
    "evaluate(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
