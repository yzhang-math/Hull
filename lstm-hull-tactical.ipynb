{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T11:58:06.805333Z",
     "iopub.status.busy": "2025-10-06T11:58:06.804806Z",
     "iopub.status.idle": "2025-10-06T11:58:14.938165Z",
     "shell.execute_reply": "2025-10-06T11:58:14.937552Z",
     "shell.execute_reply.started": "2025-10-06T11:58:06.805315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "import os\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "FINAL_MODEL = None\n",
    "SCALER = None\n",
    "FEATURE_COLUMNS = []\n",
    "HISTORY_BUFFER = None\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEQUENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "   \"\"\"LSTM-based model for market return prediction\"\"\"\n",
    "   def __init__(self, input_dim, hidden_dim=128, num_layers=2, dropout=0.2):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "   def forward(self, x):\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)        \n",
    "        output = self.fc(context)\n",
    "        return output\n",
    "\n",
    "def convert_to_signal(predictions: np.ndarray, multiplier: float = 400.0) -> np.ndarray:\n",
    "    signals = predictions * multiplier + 1\n",
    "    return np.clip(signals, 0.0, 2.0)\n",
    "\n",
    "def simple_impute(df):\n",
    "    df_imp = df.copy()\n",
    "    \n",
    "    numeric_cols = df_imp.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df_imp[col] = df_imp[col].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    return df_imp\n",
    "\n",
    "def create_sequences(X, y, sequence_length):\n",
    "    \"\"\"Create sequences for LSTM training\"\"\"\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    \n",
    "    for i in range(len(X) - sequence_length):\n",
    "        X_seq.append(X[i:i + sequence_length])\n",
    "        y_seq.append(y[i + sequence_length])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def train_model():\n",
    "    global FINAL_MODEL, SCALER, FEATURE_COLUMNS, HISTORY_BUFFER, SEQUENCE_LENGTH\n",
    "    \n",
    "    print(\"Training LSTM model...\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    start_time = pd.Timestamp.now()\n",
    "    \n",
    "    path = \"./kaggle\"\n",
    "    print(f\"Loading data from {path}\")\n",
    "    \n",
    "    training_df = pd.read_csv(f\"{path}/train.csv\")\n",
    "    print(f\"Loaded {len(training_df)} rows\")\n",
    "    \n",
    "    print(\"Imputing missing values...\")\n",
    "    training_df = simple_impute(training_df)\n",
    "    print(f\"Imputation done. Remaining NaN: {training_df.isnull().sum().sum()}\")\n",
    "    \n",
    "    if isinstance(training_df, pd.DataFrame):\n",
    "        training_df = pl.from_pandas(training_df)\n",
    "    \n",
    "    training_df = training_df.rename({'market_forward_excess_returns': 'target'})\n",
    "    \n",
    "    feature_cols = [col for col in training_df.columns if col != 'date_id']\n",
    "    training_df = training_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    \n",
    "    training_df = training_df.drop_nulls()\n",
    "    print(f\"After dropping nulls: {len(training_df)} rows\")\n",
    "    \n",
    "    FEATURE_COLUMNS = [col for col in training_df.columns \n",
    "                       if col not in ['date_id', 'target', 'forward_returns', 'risk_free_rate']]\n",
    "    \n",
    "    print(f\"Training with {len(FEATURE_COLUMNS)} features\")\n",
    "    \n",
    "    X_train = training_df.select(FEATURE_COLUMNS).to_numpy()\n",
    "    y_train = training_df.select('target').to_numpy().ravel()\n",
    "    \n",
    "    SCALER = StandardScaler()\n",
    "    X_train_scaled = SCALER.fit_transform(X_train)\n",
    "    \n",
    "    print(f\"Creating sequences with length {SEQUENCE_LENGTH}...\")\n",
    "    X_seq, y_seq = create_sequences(X_train_scaled, y_train, SEQUENCE_LENGTH)\n",
    "    print(f\"Created {len(X_seq)} sequences\")\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X_seq).to(DEVICE)\n",
    "    y_tensor = torch.FloatTensor(y_seq).reshape(-1, 1).to(DEVICE)\n",
    "    \n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    input_dim = len(FEATURE_COLUMNS)\n",
    "    FINAL_MODEL = LSTMPredictor(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2,\n",
    "        dropout=0.3\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(FINAL_MODEL.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    num_epochs = 100\n",
    "    best_loss = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    FINAL_MODEL.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predictions = FINAL_MODEL(batch_X)\n",
    "            \n",
    "            loss = criterion(predictions, batch_y)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(FINAL_MODEL.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    FINAL_MODEL.eval()\n",
    "    print(f\"Model training complete! Best Loss: {best_loss:.6f}\")\n",
    "    \n",
    "    buffer_size = SEQUENCE_LENGTH + 50\n",
    "    HISTORY_BUFFER = training_df.select(\n",
    "        ['date_id'] + FEATURE_COLUMNS + ['target', 'forward_returns', 'risk_free_rate']\n",
    "    ).tail(buffer_size)\n",
    "    \n",
    "    elapsed = (pd.Timestamp.now() - start_time).total_seconds()\n",
    "    print(f\"Model training complete in {elapsed:.1f} seconds. Ready for predictions.\")\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Make a prediction for a single test row.\n",
    "    \n",
    "    Args:\n",
    "        test: A polars DataFrame with one row containing the test features\n",
    "        \n",
    "    Returns:\n",
    "        A float signal value between 0 and 2\n",
    "    \"\"\"\n",
    "    global HISTORY_BUFFER, FINAL_MODEL, SCALER, FEATURE_COLUMNS, SEQUENCE_LENGTH\n",
    "    \n",
    "    try:\n",
    "        feature_cols = [col for col in test.columns if col != 'date_id']\n",
    "        test = test.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "        \n",
    "        rename_mapping = {}\n",
    "        if 'lagged_forward_returns' in test.columns:\n",
    "            rename_mapping['lagged_forward_returns'] = 'forward_returns'\n",
    "        if 'lagged_risk_free_rate' in test.columns:\n",
    "            rename_mapping['lagged_risk_free_rate'] = 'risk_free_rate'\n",
    "        if 'lagged_market_forward_excess_returns' in test.columns:\n",
    "            rename_mapping['lagged_market_forward_excess_returns'] = 'target'\n",
    "        \n",
    "        if rename_mapping:\n",
    "            test = test.rename(rename_mapping)\n",
    "        \n",
    "        if 'is_scored' in test.columns:\n",
    "            test = test.drop('is_scored')\n",
    "        \n",
    "        for col in HISTORY_BUFFER.columns:\n",
    "            if col not in test.columns:\n",
    "                test = test.with_columns(pl.lit(0.0).cast(pl.Float64).alias(col))\n",
    "        \n",
    "        test = test.select(HISTORY_BUFFER.columns)\n",
    "        \n",
    "        HISTORY_BUFFER = pl.concat([HISTORY_BUFFER, test], how=\"vertical\")\n",
    "        \n",
    "        max_buffer = SEQUENCE_LENGTH + 100\n",
    "        if len(HISTORY_BUFFER) > max_buffer:\n",
    "            HISTORY_BUFFER = HISTORY_BUFFER.tail(max_buffer)\n",
    "        \n",
    "        if len(HISTORY_BUFFER) < SEQUENCE_LENGTH:\n",
    "            needed = SEQUENCE_LENGTH - len(HISTORY_BUFFER)\n",
    "            first_row = HISTORY_BUFFER.head(1)\n",
    "            padding = pl.concat([first_row] * needed, how=\"vertical\")\n",
    "            sequence_data = pl.concat([padding, HISTORY_BUFFER], how=\"vertical\")\n",
    "        else:\n",
    "            sequence_data = HISTORY_BUFFER.tail(SEQUENCE_LENGTH)\n",
    "        \n",
    "        X_sequence = sequence_data.select(FEATURE_COLUMNS).to_numpy()\n",
    "        \n",
    "        X_sequence = np.nan_to_num(X_sequence, nan=0.0)\n",
    "        \n",
    "        X_sequence_scaled = SCALER.transform(X_sequence)\n",
    "        \n",
    "        X_sequence_scaled = X_sequence_scaled.reshape(1, SEQUENCE_LENGTH, -1)\n",
    "        \n",
    "        X_tensor = torch.FloatTensor(X_sequence_scaled).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            raw_prediction = FINAL_MODEL(X_tensor).cpu().numpy()[0, 0]\n",
    "        \n",
    "        signal = convert_to_signal(np.array([raw_prediction]))[0]\n",
    "        \n",
    "        return float(signal)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Starting LSTM model training...\")\n",
    "print(\"=\"*50)\n",
    "train_model()\n",
    "print(\"=\"*50)\n",
    "print(\"Model ready. Starting inference server...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import kaggle_evaluation.default_inference_server\n",
    "\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(\"Running in competition mode\")\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    print(\"Running in local gateway mode\")\n",
    "    inference_server.run_local_gateway(\n",
    "        ('./kaggle',)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Evaluating LSTM on the last 180 days (Validation Set)...\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m val_pl_df \u001b[38;5;241m=\u001b[39m full_pl_df\u001b[38;5;241m.\u001b[39mtail(VALIDATION_SIZE)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Get features for scaling\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m X_val \u001b[38;5;241m=\u001b[39m val_pl_df\u001b[38;5;241m.\u001b[39mselect(FEATURE_COLUMNS)\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     82\u001b[0m y_val_info \u001b[38;5;241m=\u001b[39m val_pl_df\u001b[38;5;241m.\u001b[39mselect([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_returns\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrisk_free_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Scale the features\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_311/lib/python3.11/site-packages/polars/dataframe/frame.py:2021\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[0;34m(self, order, writable, allow_copy, structured, use_pyarrow)\u001b[0m\n\u001b[1;32m   2018\u001b[0m         out[c] \u001b[38;5;241m=\u001b[39m arrays[idx]\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m-> 2021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\u001b[38;5;241m.\u001b[39mto_numpy(order, writable\u001b[38;5;241m=\u001b[39mwritable, allow_copy\u001b[38;5;241m=\u001b[39mallow_copy)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_311/lib/python3.11/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Model Performance Evaluation (Last 180 Days)\n",
    "# ==================================================\n",
    "\n",
    "# --- 1. Metric Implementation ---\n",
    "\n",
    "def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Adjusted Sharpe Ratio for the competition.\n",
    "    Requires 'forward_returns' and 'risk_free_rate' in y_true_df.\n",
    "    \"\"\"\n",
    "    solution = y_true_df.to_pandas()\n",
    "    solution['position'] = y_pred_signals\n",
    "\n",
    "    # --- Strategy Returns Calculation ---\n",
    "    solution['strategy_returns'] = (\n",
    "        solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "        solution['position'] * solution['forward_returns']\n",
    "    )\n",
    "    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "    strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "    strategy_std = solution['strategy_returns'].std()\n",
    "    \n",
    "    if strategy_std == 0: return 0.0\n",
    "    \n",
    "    trading_days_per_yr = 252\n",
    "    sharpe = strategy_geo_mean / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "    \n",
    "    # --- Market Comparison and Volatility Penalty ---\n",
    "    market_std = solution['forward_returns'].std()\n",
    "    market_volatility = market_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    strategy_volatility = strategy_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    \n",
    "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "    vol_penalty = 1 + excess_vol\n",
    "    \n",
    "    # --- Return Gap Penalty ---\n",
    "    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "    market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "    \n",
    "    return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * trading_days_per_yr)\n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "    \n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    return adjusted_sharpe\n",
    "\n",
    "\n",
    "# --- 2. Data Preparation for Validation ---\n",
    "\n",
    "# Get the full training data (which includes the buffer and more)\n",
    "# We assume the 'training_df' (before the tail() for HISTORY_BUFFER)\n",
    "# represents all pre-processed data. We will re-read it for clean slicing.\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Evaluating LSTM on the last 180 days (Validation Set)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Re-load data, since training_df was filtered/converted locally in train_model()\n",
    "path = \"./kaggle\"\n",
    "full_df = pd.read_csv(f\"{path}/train.csv\").rename({'market_forward_excess_returns': 'target'}, axis=1)\n",
    "\n",
    "# Apply the same imputation used in training\n",
    "def simple_impute_pd(df):\n",
    "    df_imp = df.copy()\n",
    "    numeric_cols = df_imp.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        # Use ffill/bfill replacement for the now deprecated fillna(method)\n",
    "        df_imp[col] = df_imp[col].ffill().bfill().fillna(0)\n",
    "    return df_imp\n",
    "\n",
    "full_df = simple_impute_pd(full_df)\n",
    "\n",
    "# Convert to Polars for slicing and metric\n",
    "full_pl_df = pl.from_pandas(full_df)\n",
    "\n",
    "# Prepare the validation set (last 180 rows)\n",
    "VALIDATION_SIZE = 180\n",
    "val_pl_df = full_pl_df.tail(VALIDATION_SIZE)\n",
    "\n",
    "# Get features for scaling\n",
    "X_val = val_pl_df.select(FEATURE_COLUMNS).to_numpy()\n",
    "y_val_info = val_pl_df.select([\"target\", \"forward_returns\", \"risk_free_rate\"])\n",
    "\n",
    "# Scale the features\n",
    "X_val_scaled = SCALER.transform(X_val)\n",
    "\n",
    "# --- 3. Sequence Creation for Validation ---\n",
    "\n",
    "# NOTE: For proper time-series validation, the validation sequence\n",
    "# must include SEQUENCE_LENGTH-1 days of lookback *from the training data*.\n",
    "# The create_sequences function handles this automatically if we use the right slice.\n",
    "# However, for *inference* on the full validation block, we create sequences\n",
    "# differently to get one prediction per day.\n",
    "\n",
    "def create_inference_sequences(X, sequence_length):\n",
    "    X_seq = []\n",
    "    # Loop from the first day where a full sequence can be formed\n",
    "    for i in range(len(X) - sequence_length + 1):\n",
    "        X_seq.append(X[i:i + sequence_length])\n",
    "    return np.array(X_seq)\n",
    "\n",
    "# We need data for the full validation period (180 days) plus the sequence length\n",
    "# from the end of the training data.\n",
    "needed_history = SEQUENCE_LENGTH - 1\n",
    "start_index = len(full_pl_df) - VALIDATION_SIZE - needed_history\n",
    "\n",
    "# Slice the data to include the lookback for the first validation day\n",
    "full_slice = full_pl_df.slice(start_index, VALIDATION_SIZE + needed_history)\n",
    "X_full_slice = full_slice.select(FEATURE_COLUMNS).to_numpy()\n",
    "\n",
    "# Scale the full slice\n",
    "X_full_slice_scaled = SCALER.transform(X_full_slice)\n",
    "\n",
    "# Create sequences for the 180 days of validation\n",
    "X_val_seq = create_inference_sequences(X_full_slice_scaled, SEQUENCE_LENGTH)\n",
    "\n",
    "# --- 4. Prediction Generation ---\n",
    "FINAL_MODEL.eval()\n",
    "X_val_tensor = torch.FloatTensor(X_val_seq).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    raw_predictions = FINAL_MODEL(X_val_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# Convert to signal\n",
    "val_signals = convert_to_signal(raw_predictions)\n",
    "\n",
    "# --- 5. Score Calculation ---\n",
    "val_score = calculate_competition_score(y_val_info, val_signals)\n",
    "\n",
    "print(f\"Validation Score (Adjusted Sharpe) on last {VALIDATION_SIZE} days: {val_score:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Note: The output of this block will appear after the training output\n",
    "# in the final execution of the complete notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading and preparing full training data for CV...\n",
      "\n",
      "==================================================\n",
      "Starting 10-Fold Time Series Cross-Validation (LSTM)...\n",
      "==================================================\n",
      "  Fold 1/10 Score: 0.6629 (Test size: 726 days)\n",
      "  Fold 2/10 Score: -0.7845 (Test size: 726 days)\n",
      "  Fold 3/10 Score: 0.6038 (Test size: 726 days)\n",
      "  Fold 4/10 Score: 0.2729 (Test size: 726 days)\n",
      "  Fold 5/10 Score: 0.1265 (Test size: 726 days)\n",
      "  Fold 6/10 Score: 0.8977 (Test size: 726 days)\n",
      "  Fold 7/10 Score: 0.6898 (Test size: 726 days)\n",
      "  Fold 8/10 Score: 0.8551 (Test size: 726 days)\n",
      "  Fold 9/10 Score: 0.4549 (Test size: 726 days)\n",
      "  Fold 10/10 Score: 1.0304 (Test size: 726 days)\n",
      "\n",
      "Mean CV Score: 0.4809 (+/- 0.4978)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# LSTM Time Series Cross-Validation (CV)\n",
    "# ==================================================\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# --- 1. Utility/Metric Functions (Re-defined for CV scope) ---\n",
    "\n",
    "def convert_to_signal(predictions: np.ndarray, multiplier: float = 400.0) -> np.ndarray:\n",
    "    \"\"\"Converts model predictions (excess returns) to the trading signal (0 to 2).\"\"\"\n",
    "    signals = predictions * multiplier + 1\n",
    "    return np.clip(signals, 0.0, 2.0)\n",
    "\n",
    "def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "    \"\"\"Computes the Adjusted Sharpe Ratio (ASR) metric.\"\"\"\n",
    "    solution = y_true_df.to_pandas()\n",
    "    solution['position'] = y_pred_signals\n",
    "\n",
    "    solution['strategy_returns'] = (\n",
    "        solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "        solution['position'] * solution['forward_returns']\n",
    "    )\n",
    "    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "    strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "    strategy_std = solution['strategy_returns'].std()\n",
    "    \n",
    "    if strategy_std == 0: return 0.0\n",
    "    \n",
    "    trading_days_per_yr = 252\n",
    "    sharpe = strategy_geo_mean / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "    \n",
    "    market_std = solution['forward_returns'].std()\n",
    "    market_volatility = market_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    strategy_volatility = strategy_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    \n",
    "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "    vol_penalty = 1 + excess_vol\n",
    "    \n",
    "    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "    market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "    \n",
    "    return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * trading_days_per_yr)\n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "    \n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    return float(min(adjusted_sharpe, 1_000_000))\n",
    "\n",
    "def create_inference_sequences(X, sequence_length):\n",
    "    \"\"\"Creates sequences for batch prediction (one output per sequence).\"\"\"\n",
    "    X_seq = []\n",
    "    # Loop from the first day where a full sequence can be formed\n",
    "    for i in range(len(X) - sequence_length + 1):\n",
    "        X_seq.append(X[i:i + sequence_length])\n",
    "    return np.array(X_seq)\n",
    "\n",
    "# --- 2. Training Function Wrapper ---\n",
    "\n",
    "def train_lstm_fold(X_train_fold, y_train_fold, input_dim, sequence_length, device):\n",
    "    \"\"\"Trains the LSTM model for one cross-validation fold.\"\"\"\n",
    "    \n",
    "    # Sequence creation\n",
    "    def create_sequences_fold(X, y, seq_len):\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(len(X) - seq_len):\n",
    "            X_seq.append(X[i:i + seq_len])\n",
    "            y_seq.append(y[i + seq_len])\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "        \n",
    "    X_seq, y_seq = create_sequences_fold(X_train_fold, y_train_fold, sequence_length)\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X_seq).to(device)\n",
    "    y_tensor = torch.FloatTensor(y_seq).reshape(-1, 1).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    train_loader = DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "    \n",
    "    # Model instantiation\n",
    "    # (Note: Assumes LSTMPredictor class is defined in the notebook's global scope)\n",
    "    model = LSTMPredictor(\n",
    "        input_dim=input_dim, hidden_dim=256, num_layers=3, dropout=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)    \n",
    "    num_epochs = 100\n",
    "    best_loss = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            # print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# --- 3. CV Execution ---\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "SEQUENCE_LENGTH = 30\n",
    "PATH = \"./kaggle\"\n",
    "\n",
    "# Re-load data and apply imputation/prep as done in train_model\n",
    "print(\"Loading and preparing full training data for CV...\")\n",
    "full_df = pd.read_csv(f\"{PATH}/train.csv\").rename({'market_forward_excess_returns': 'target'}, axis=1)\n",
    "\n",
    "\n",
    "def simple_impute_pd(df):\n",
    "    df_imp = df.copy()\n",
    "    numeric_cols = df_imp.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        df_imp[col] = df_imp[col].ffill().bfill().fillna(0)\n",
    "    return df_imp\n",
    "full_df = simple_impute_pd(full_df).drop(columns=['date_id'])\n",
    "full_pl_df = pl.from_pandas(full_df).drop_nulls()\n",
    "\n",
    "full_pl_df = full_pl_df.slice(1000) # Optional: Skip initial rows if needed\n",
    "\n",
    "FEATURE_COLUMNS = [col for col in full_pl_df.columns \n",
    "                   if col not in ['target', 'forward_returns', 'risk_free_rate']]\n",
    "\n",
    "# Prepare data for CV\n",
    "X_all = full_pl_df.select(FEATURE_COLUMNS).to_numpy()\n",
    "y_all = full_pl_df.select('target').to_numpy().ravel()\n",
    "scorer_info_df_all = full_pl_df.select([\"forward_returns\", \"risk_free_rate\", \"target\"])\n",
    "\n",
    "# Scaling: Fit on the entire data, as the original scaler was fit on the whole train.csv\n",
    "SCALER_CV = StandardScaler()\n",
    "X_all_scaled = SCALER_CV.fit_transform(X_all)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "nsplits = 10\n",
    "print(f\"Starting {nsplits}-Fold Time Series Cross-Validation (LSTM)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=nsplits)\n",
    "cv_scores = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(X_all_scaled)):\n",
    "    \n",
    "    # --- Split Data ---\n",
    "    # We need a lookback history for the test set, taken from the end of the train set\n",
    "    needed_history = SEQUENCE_LENGTH - 1\n",
    "    \n",
    "    # X_train is the standard split for direct model training\n",
    "    X_train_fold = X_all_scaled[train_index]\n",
    "    y_train_fold = y_all[train_index]\n",
    "    \n",
    "    # X_test_full includes the lookback period from the train set\n",
    "    X_test_start_index = test_index[0] - needed_history\n",
    "    X_test_end_index = test_index[-1] + 1\n",
    "    X_test_full = X_all_scaled[X_test_start_index:X_test_end_index]\n",
    "    \n",
    "    # y_test_info is just the evaluation window\n",
    "    y_test_info = scorer_info_df_all[test_index]\n",
    "    \n",
    "    # --- Train Model ---\n",
    "    model = train_lstm_fold(\n",
    "        X_train_fold, y_train_fold, len(FEATURE_COLUMNS), SEQUENCE_LENGTH, DEVICE\n",
    "    )\n",
    "    \n",
    "    # --- Predict and Score ---\n",
    "    # Create sequences for the test set\n",
    "    X_test_seq = create_inference_sequences(X_test_full, SEQUENCE_LENGTH)\n",
    "    \n",
    "    X_test_tensor = torch.FloatTensor(X_test_seq).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        raw_predictions = model(X_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    signals = convert_to_signal(raw_predictions)\n",
    "    score = calculate_competition_score(y_test_info, signals)\n",
    "    \n",
    "    cv_scores.append(score)\n",
    "    print(f\"  Fold {i+1}/{nsplits} Score: {score:.4f} (Test size: {len(test_index)} days)\")\n",
    "\n",
    "print(f\"\\nMean CV Score: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13750964,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ml_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
