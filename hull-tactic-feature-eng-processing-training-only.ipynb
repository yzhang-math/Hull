{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&P 500 Returns Prediction — Comprehensive Analysis & Pipeline\n",
    "\n",
    "\n",
    "**Reference:** Insights and explorations are derived from the EDA notebook [Hull Tactical: Complete EDA Deep Dive](https://www.kaggle.com/code/ahsuna123/hull-tactical-complete-eda-deep-dive).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 1. Dataset Overview\n",
    "- **Rows:** 8,990  \n",
    "- **Features:** 98 (94 numeric, 4 special)  \n",
    "- **Target:** `forward_returns`  \n",
    "- **Feature Categories:**\n",
    "  - **Market Dynamics:** M1–M13  \n",
    "  - **Macro Economic:** E1–E20  \n",
    "  - **Interest Rate:** I1–I9  \n",
    "  - **Price Valuation:** P1–P13  \n",
    "  - **Volatility:** V1–V13  \n",
    "  - **Sentiment:** S1–S12  \n",
    "  - **Dummy Binary:** D1–D9  \n",
    "  - **Special:** 4  \n",
    "\n",
    "\n",
    "✅ Rich in macroeconomic and market features; some categories show **high internal correlations**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 2. Target Variable Insights\n",
    "- `forward_returns`:\n",
    "  - Not normally distributed (Jarque-Bera p-value = 0)  \n",
    "  - Stationary (ADF test p-value = 0)  \n",
    "  - Weak autocorrelation: lag 1 ≈ -0.045, lag 5 ≈ -0.024  \n",
    "  - Skewness ≈ -0.176, Kurtosis ≈ 2.19 → slightly platykurtic  \n",
    "  - Maximum drawdown ≈ -0.492  \n",
    "\n",
    "\n",
    "**Implications:**  \n",
    "- Use robust methods (tree-based, quantile regression).  \n",
    "- Temporal features can be leveraged due to stationarity.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 3. Feature Correlation & Stability\n",
    "- Most features weakly correlated with target (<0.07).  \n",
    "- High multicollinearity within:\n",
    "  - Macro_Economic, Interest_Rate, Price_Valuation, Volatility, Sentiment.  \n",
    "- Feature stability over time:\n",
    "  - **Stable:** Dummy_Binary, M1  \n",
    "  - **Unstable:** Macro_Economic (E11, E12), Price_Valuation (P12), Volatility & Sentiment (V10–V12, S12)  \n",
    "\n",
    "\n",
    "**Implications:**  \n",
    "- Non-linear models preferred.  \n",
    "- Unstable features should be transformed (lag/rolling means, smoothing).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 4. Outlier Analysis\n",
    "- Significant outliers in Macro_Economic and Price_Valuation features.  \n",
    "- Recommendation: Winsorization, clipping, or robust scaling.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 5. Feature Engineering Strategy\n",
    "- **Lag Features:** M1–M10, D1–D5, V1–V5 → lag1, lag3  \n",
    "- **Rolling Features:** Volatility features → rolling mean/std (window=5)  \n",
    "- **Interaction Features:** Selected pairs between Macro_Economic and Market_Dynamics features  \n",
    "- **Outlier Handling:** Clipping based on category-specific bounds  \n",
    "- **Dimensionality Reduction:** PCA (n_components=50) to reduce high correlation noise  \n",
    "- **Feature Selection:** SelectKBest (mutual information) when PCA is not used\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 6. Modeling Approach\n",
    "- **Ensemble:** LGBM + XGBoost + Random Forest  \n",
    "- **Hyperparameters:** Reduced for fast inference (n_estimators ≤ 300, max_depth limited)  \n",
    "- **Weighting:** LGBM 0.4, XGB 0.35, RF 0.25 (renormalized if a model fails)  \n",
    "- **Scaling:** RobustScaler applied to features  \n",
    "- **Training/Inference Flow:**\n",
    "  - Fit scalers and PCA during training  \n",
    "  - Handle outliers for each batch  \n",
    "  - Create lag/rolling/interaction features consistently for training & prediction  \n",
    "  - Maintain recent feature history for single-row lag computations  \n",
    "\n",
    "\n",
    "**Advantages:**\n",
    "- Robust to outliers  \n",
    "- Handles weakly correlated features and non-linear effects  \n",
    "- Maintains consistent feature structure between training and inference\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 7. Pipeline Usage\n",
    "- **Training:** `pipeline.fit_from_file(train_path)`  \n",
    "- **Prediction:** `predict(pl.DataFrame)` for batch inference  \n",
    "- **Saving/Loading:** `pipeline.save_model('sp500_model.pkl')` / `pipeline.load_model('sp500_model.pkl')`  \n",
    "- **Server Deployment:** `kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)`  \n",
    "\n",
    "\n",
    "**Note:** Polars is used for batch input; features are converted to Pandas internally.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 8. Key Recommendations\n",
    "1. Start modeling with **tree-based ensembles** (LGBM, XGBoost, RF)  \n",
    "2. Use **PCA or feature selection** to reduce dimensionality and multicollinearity  \n",
    "3. Include **lag and rolling features** for unstable categories  \n",
    "4. Apply **robust scaling/clipping** to mitigate extreme outliers  \n",
    "5. Implement **time-series aware validation** (avoid random splits)  \n",
    "6. Monitor **feature stability** for future iterations and potential new engineered features\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Summary:**  \n",
    "This pipeline integrates insights from EDA, feature stability analysis, and target characteristics into an end-to-end, deployable model for predicting S&P 500 forward returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T14:26:06.759254Z",
     "iopub.status.busy": "2025-10-02T14:26:06.758964Z",
     "iopub.status.idle": "2025-10-02T14:26:34.261346Z",
     "shell.execute_reply": "2025-10-02T14:26:34.260644Z",
     "shell.execute_reply.started": "2025-10-02T14:26:06.759232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline classes loaded successfully!\n",
      "\n",
      "To train the model, run:\n",
      "  pipeline = RegularizedSP500Pipeline()\n",
      "  pipeline.fit_from_file('train.csv')\n",
      "\n",
      "To use for inference:\n",
      "  def predict(test: pl.DataFrame) -> float:\n",
      "      test_df = test.to_pandas()\n",
      "      return float(pipeline.predict_batch(test_df))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "import torch\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import kaggle_evaluation.default_inference_server\n",
    "from model_my_own import SP500Predictor\n",
    "\n",
    "class SP500PredictorWrapper(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"sklearn-compatible wrapper for SP500Predictor\"\"\"\n",
    "    def __init__(self, feature_dim=None, d_model=64, num_heads=8, num_layers=4,\n",
    "                 d_ff=256, dropout=0.1, output_dim=32, transformer_epochs=10,\n",
    "                 transformer_lr=1e-3, combine_features=True, xgb_params=None,\n",
    "                 # -----------------\n",
    "                 # MODIFICATION: Added batch size\n",
    "                 transformer_batch_size=256\n",
    "                 # -----------------\n",
    "                 ):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "        self.output_dim = output_dim\n",
    "        self.transformer_epochs = transformer_epochs\n",
    "        self.transformer_lr = transformer_lr\n",
    "        self.combine_features = combine_features\n",
    "        self.xgb_params = xgb_params\n",
    "        # -----------------\n",
    "        # MODIFICATION: Store batch size\n",
    "        self.transformer_batch_size = transformer_batch_size\n",
    "        # -----------------\n",
    "        self.model = None\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the SP500Predictor model\"\"\"\n",
    "        if self.feature_dim is None:\n",
    "            self.feature_dim = X.shape[1]\n",
    "        \n",
    "        # Initialize the model\n",
    "        self.model = SP500Predictor(\n",
    "            feature_dim=self.feature_dim,\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            num_layers=self.num_layers,\n",
    "            d_ff=self.d_ff,\n",
    "            dropout=self.dropout,\n",
    "            output_dim=self.output_dim,\n",
    "            xgb_params=self.xgb_params,\n",
    "            combine_features=self.combine_features,\n",
    "            # -----------------\n",
    "            # MODIFICATION: Pass batch size to the model\n",
    "            transformer_batch_size=self.transformer_batch_size\n",
    "            # -----------------\n",
    "        )\n",
    "        \n",
    "        # Convert to torch tensors - check for invalid values\n",
    "        X_array = X.values if hasattr(X, 'values') else X\n",
    "        y_array = y.values if hasattr(y, 'values') else y\n",
    "        \n",
    "        # Replace any remaining NaN/inf with 0\n",
    "        X_array = np.nan_to_num(X_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        y_array = np.nan_to_num(y_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        X_tensor = torch.FloatTensor(X_array)\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.fit(\n",
    "            X_tensor,\n",
    "            y_array,\n",
    "            transformer_epochs=self.transformer_epochs,\n",
    "            transformer_lr=self.transformer_lr,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        \n",
    "        X_array = X.values if hasattr(X, 'values') else X\n",
    "        X_array = np.nan_to_num(X_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        X_tensor = torch.FloatTensor(X_array)\n",
    "        \n",
    "        return self.model.predict(X_tensor)\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator\"\"\"\n",
    "        return {\n",
    "            'feature_dim': self.feature_dim,\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'num_layers': self.num_layers,\n",
    "            'd_ff': self.d_ff,\n",
    "            'dropout': self.dropout,\n",
    "            'output_dim': self.output_dim,\n",
    "            'transformer_epochs': self.transformer_epochs,\n",
    "            'transformer_lr': self.transformer_lr,\n",
    "            'combine_features': self.combine_features,\n",
    "            'xgb_params': self.xgb_params,\n",
    "            # -----------------\n",
    "            # MODIFICATION: Add to params\n",
    "            'transformer_batch_size': self.transformer_batch_size\n",
    "            # -----------------\n",
    "        }\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set parameters for this estimator\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "class RegularizedSP500Pipeline:\n",
    "    \"\"\"Simplified pipeline with regularization to prevent overfitting\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = RobustScaler()\n",
    "       \n",
    "        # Heavy regularization to prevent overfitting\n",
    "        self.models = {\n",
    "            'lgb': lgb.LGBMRegressor(\n",
    "                objective='regression',\n",
    "                metric='rmse',\n",
    "                num_leaves=15,\n",
    "                learning_rate=0.01,\n",
    "                n_estimators=100,\n",
    "                min_child_samples=20,\n",
    "                subsample=0.7,\n",
    "                colsample_bytree=0.7,\n",
    "                reg_alpha=1.0,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42,\n",
    "                verbose=-1  # Suppress output\n",
    "            ),\n",
    "            'xgb': xgb.XGBRegressor(\n",
    "                objective='reg:squarederror',\n",
    "                n_estimators=50,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.03,\n",
    "                min_child_weight=5,\n",
    "                subsample=0.7,\n",
    "                colsample_bytree=0.7,\n",
    "                reg_alpha=1.0,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42,\n",
    "                verbosity=0\n",
    "            ),\n",
    "            'sp500_transformer': SP500PredictorWrapper(\n",
    "                d_model=64,\n",
    "                num_heads=8,\n",
    "                num_layers=4,\n",
    "                d_ff=128,\n",
    "                dropout=0.2,\n",
    "                output_dim=16,\n",
    "                transformer_epochs=5,\n",
    "                transformer_lr=1e-3,\n",
    "                combine_features=True,\n",
    "                xgb_params={\n",
    "                    'n_estimators': 50,\n",
    "                    'max_depth': 5,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'reg_alpha': 1.0,\n",
    "                    'reg_lambda': 1.0\n",
    "                }\n",
    "            )\n",
    "        }\n",
    "\n",
    "        self.model_weights = {'lgb': 0.35, 'xgb': 0.35, 'sp500_transformer': 0.30}\n",
    "        self.feature_names = None\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def create_features(self, df):\n",
    "        \"\"\"Minimal feature engineering to reduce overfitting risk - FIXED VERSION\"\"\"\n",
    "        df_features = df.copy()\n",
    "        # Exclude target-related columns from features\n",
    "        excluded_cols = ['date_id', 'forward_returns', 'is_scored', 'risk_free_rate', 'market_forward_excess_returns']\n",
    "        # FIX: Get feature columns ONCE before creating any derived features\n",
    "        original_feature_cols = [c for c in df.columns if c not in excluded_cols]\n",
    "       \n",
    "        print(f\"  Original features: {len(original_feature_cols)}\")\n",
    "       \n",
    "        # Only 1-2 simple lags - use 0 fill instead of forward fill\n",
    "        for lag in [1, 2]:\n",
    "            for col in original_feature_cols:  # FIX: Use original_feature_cols, not feature_cols\n",
    "                df_features[f'{col}_lag{lag}'] = df_features[col].shift(lag).fillna(0)\n",
    "       \n",
    "        # Only one rolling window\n",
    "        for col in original_feature_cols:  # FIX: Use original_feature_cols\n",
    "            df_features[f'{col}_roll_mean_5'] = df_features[col].rolling(5, min_periods=1).mean()\n",
    "       \n",
    "        # Count total features after engineering\n",
    "        final_feature_cols = [c for c in df_features.columns if c not in excluded_cols]\n",
    "        print(f\"  Features after engineering: {len(final_feature_cols)}\")\n",
    "       \n",
    "        return df_features\n",
    "\n",
    "    def prepare_features(self, df, is_training=False):\n",
    "        print(\"Creating features...\")\n",
    "        df_enh = self.create_features(df)\n",
    "       \n",
    "        # Exclude target-related columns\n",
    "        excluded_cols = ['date_id', 'forward_returns', 'is_scored', 'risk_free_rate', 'market_forward_excess_returns']\n",
    "        feature_cols = [c for c in df_enh.columns if c not in excluded_cols]\n",
    "        X = df_enh[feature_cols].copy()\n",
    "\n",
    "        print(f\"Handling missing values and outliers...\")\n",
    "        # First fill NaN before computing quantiles\n",
    "        X = X.fillna(X.median()).fillna(0)\n",
    "       \n",
    "        # Then clip outliers\n",
    "        for col in X.columns:\n",
    "            try:\n",
    "                q05, q95 = X[col].quantile(0.05), X[col].quantile(0.95)\n",
    "                if not np.isnan(q05) and not np.isnan(q95):\n",
    "                    X[col] = X[col].clip(q05, q95)\n",
    "            except:\n",
    "                pass  # Skip columns that can't be clipped\n",
    "\n",
    "        # Final NaN check\n",
    "        X = X.fillna(0)\n",
    "\n",
    "        if is_training:\n",
    "            print(f\"Fitting scaler on {X.shape[0]} samples with {X.shape[1]} features...\")\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "            self.feature_names = list(X.columns)\n",
    "        else:\n",
    "            # Ensure all expected features exist\n",
    "            for col in self.feature_names:\n",
    "                if col not in X.columns:\n",
    "                    X[col] = 0\n",
    "            X = X[self.feature_names]\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "\n",
    "        return pd.DataFrame(X_scaled, columns=self.feature_names, index=df.index)\n",
    "\n",
    "    def fit_from_file(self, train_path, target_col='forward_returns'):\n",
    "        print(\"=\"*60)\n",
    "        print(\"STARTING TRAINING PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\n[1/6] Loading training data from {train_path}...\")\n",
    "        # Read CSV with proper NA handling\n",
    "        df = pd.read_csv(train_path, na_values=['', 'NA', 'N/A', 'null']).iloc[1000:] # Skip first 1000 rows as per original logic\n",
    "        print(f\"  ✓ Loaded {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "        y = df[target_col].fillna(0)\n",
    "        print(f\"  ✓ Target variable: {target_col} (mean={y.mean():.6f}, std={y.std():.6f})\")\n",
    "       \n",
    "        self.y_mean, self.y_std = 0, 1\n",
    "       \n",
    "        print(f\"\\n[2/6] Preparing features...\")\n",
    "        X_full = self.prepare_features(df, is_training=True)\n",
    "        print(f\"  ✓ Final feature matrix: {X_full.shape[0]} rows × {X_full.shape[1]} features\")\n",
    "\n",
    "        # TimeSeriesSplit CV\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        cv_results = {name: {'rmse': [], 'r2': [], 'var_y': []} for name in self.models}\n",
    "\n",
    "        print(f\"\\n[3/6] Running TimeSeriesSplit Cross-Validation (5 folds)...\")\n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_full), 1):\n",
    "            X_train, X_val = X_full.iloc[train_idx], X_full.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            print(f\"\\n  Fold {fold}/5: train_size={len(X_train)}, val_size={len(X_val)}\")\n",
    "\n",
    "            for name, model in self.models.items():\n",
    "                print(f\"    [{name}] Training...\", end=' ', flush=True)\n",
    "                # Clone model to avoid state carryover\n",
    "                fold_models = {} # Store clones for cleanup\n",
    "                if name == 'lgb':\n",
    "                    model_clone = lgb.LGBMRegressor(**model.get_params())\n",
    "                elif name == 'xgb':\n",
    "                    model_clone = xgb.XGBRegressor(**model.get_params())\n",
    "                else:  # sp500_transformer\n",
    "                    model_clone = SP500PredictorWrapper(**model.get_params())\n",
    "                #print(f\"fititng {name} model\")\n",
    "                model_clone.fit(X_train, y_train)\n",
    "                preds = model_clone.predict(X_val)\n",
    "                rmse = np.sqrt(np.mean((preds - y_val)**2))\n",
    "                r2 = r2_score(y_val, preds)\n",
    "                var_y = np.var(y_val)\n",
    "                cv_results[name]['rmse'].append(rmse)\n",
    "                cv_results[name]['r2'].append(r2)\n",
    "                cv_results[name]['var_y'].append(var_y)\n",
    "                print(f\"RMSE={rmse:.6f}, R²={r2:.6f}\")\n",
    "                fold_models[name] = model_clone  # Store for cleanup\n",
    "            \n",
    "            print(f\"    [Memory] Clearing memory for fold {fold}...\")\n",
    "            \n",
    "            # Explicitly delete the cloned models and large tensors\n",
    "            del fold_models, X_train, X_val, y_train, y_val, preds\n",
    "            \n",
    "            # Run Python's garbage collector\n",
    "            gc.collect()\n",
    "            \n",
    "            # Empty the PyTorch CUDA cache (the most important step)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                print(f\"    [Memory] Fold {fold} memory cleared.\")\n",
    "\n",
    "        # Summary statistics\n",
    "        print(f\"\\n[4/6] Cross-Validation Results Summary:\")\n",
    "        print(\"=\"*60)\n",
    "        for name, stats in cv_results.items():\n",
    "            print(f\"\\n{name.upper()}:\")\n",
    "            for metric in ['rmse','r2','var_y']:\n",
    "                values = stats[metric]\n",
    "                if values:\n",
    "                    print(f\"  {metric.upper()}: mean={np.mean(values):.6f}, std={np.std(values):.6f}\")\n",
    "\n",
    "        # Check for overfitting\n",
    "        for name, stats in cv_results.items():\n",
    "            if stats['r2']:\n",
    "                avg_r2 = np.mean(stats['r2'])\n",
    "                if avg_r2 > 0.5:\n",
    "                    print(f\"\\n⚠️  WARNING: {name} shows high R² ({avg_r2:.3f}) - likely overfitting!\")\n",
    "                if avg_r2 < 0:\n",
    "                    print(f\"\\n⚠️  WARNING: {name} has negative R² ({avg_r2:.3f}) - worse than baseline!\")\n",
    "\n",
    "        # Refit models on full dataset\n",
    "        print(f\"\\n[5/6] Refitting models on full dataset...\")\n",
    "        for name, model in self.models.items():\n",
    "            try:\n",
    "                print(f\"  [{name}] Training on full data...\", end=' ', flush=True)\n",
    "                model.fit(X_full, y)\n",
    "                print(\"✓\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                self.model_weights[name] = 0\n",
    "\n",
    "        # Adjust weights based on inverse RMSE\n",
    "        print(f\"\\n[6/6] Calculating ensemble weights...\")\n",
    "        inv_rmse = {}\n",
    "        for k in self.models:\n",
    "            if cv_results[k]['rmse']:\n",
    "                mean_rmse = np.mean(cv_results[k]['rmse'])\n",
    "                inv_rmse[k] = 1.0 / mean_rmse if mean_rmse > 0 else 0\n",
    "            else:\n",
    "                inv_rmse[k] = 0\n",
    "       \n",
    "        total = sum(inv_rmse.values())\n",
    "        if total > 0:\n",
    "            self.model_weights = {k: v/total for k,v in inv_rmse.items()}\n",
    "\n",
    "        print(f\"  Final model weights:\")\n",
    "        for name, weight in self.model_weights.items():\n",
    "            print(f\"    {name}: {weight:.3f}\")\n",
    "\n",
    "        self.is_fitted = True\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✓ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        return self\n",
    "\n",
    "    def predict_batch(self, df_batch):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted\")\n",
    "        X_full = self.prepare_features(df_batch, is_training=False)\n",
    "        preds = np.zeros(len(X_full))\n",
    "        for name, model in self.models.items():\n",
    "            weight = self.model_weights.get(name, 0)\n",
    "            if weight > 0:\n",
    "                preds += weight * model.predict(X_full)\n",
    "        return preds[0] if len(preds)==1 else preds\n",
    "\n",
    "    def save_model(self, filepath='regularized_sp500_model.pkl'):\n",
    "        \"\"\"Save the entire pipeline including PyTorch models\"\"\"\n",
    "        models_to_save = {}\n",
    "        for name, model in self.models.items():\n",
    "            if name == 'sp500_transformer' and model.model is not None:\n",
    "                models_to_save[name] = {\n",
    "                    'params': model.get_params(),\n",
    "                    'transformer_state': model.model.transformer.state_dict(),\n",
    "                    'xgboost_model': model.model.xgboost_model,\n",
    "                    'combine_features': model.model.combine_features\n",
    "                }\n",
    "            else:\n",
    "                models_to_save[name] = model\n",
    "       \n",
    "        model_data = {\n",
    "            'models': models_to_save,\n",
    "            'model_weights': self.model_weights,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_names': self.feature_names,\n",
    "            'is_fitted': self.is_fitted,\n",
    "            'y_mean': self.y_mean,\n",
    "            'y_std': self.y_std\n",
    "        }\n",
    "        with open(filepath,'wb') as f:\n",
    "            pickle.dump(model_data,f)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def load_model(self, filepath='regularized_sp500_model.pkl'):\n",
    "        \"\"\"Load the entire pipeline including PyTorch models\"\"\"\n",
    "        with open(filepath,'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "       \n",
    "        # Restore models\n",
    "        self.models = {}\n",
    "        for name, model_obj in model_data['models'].items():\n",
    "            if name == 'sp500_transformer' and isinstance(model_obj, dict):\n",
    "                wrapper = SP500PredictorWrapper(**model_obj['params'])\n",
    "                wrapper.feature_dim = model_obj['params']['feature_dim']\n",
    "               \n",
    "                wrapper.model = SP500Predictor(\n",
    "                    feature_dim=wrapper.feature_dim,\n",
    "                    d_model=wrapper.d_model,\n",
    "                    num_heads=wrapper.num_heads,\n",
    "                    num_layers=wrapper.num_layers,\n",
    "                    d_ff=wrapper.d_ff,\n",
    "                    dropout=wrapper.dropout,\n",
    "                    output_dim=wrapper.output_dim,\n",
    "                    xgb_params=wrapper.xgb_params,\n",
    "                    combine_features=model_obj['combine_features']\n",
    "                )\n",
    "               \n",
    "                wrapper.model.transformer.load_state_dict(model_obj['transformer_state'])\n",
    "                wrapper.model.xgboost_model = model_obj['xgboost_model']\n",
    "                wrapper.model.combine_features = model_obj['combine_features']\n",
    "               \n",
    "                self.models[name] = wrapper\n",
    "            else:\n",
    "                self.models[name] = model_obj\n",
    "       \n",
    "        self.model_weights = model_data['model_weights']\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.feature_names = model_data['feature_names']\n",
    "        self.is_fitted = model_data['is_fitted']\n",
    "        self.y_mean = model_data.get('y_mean',0)\n",
    "        self.y_std = model_data.get('y_std',1)\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "\n",
    "# DON'T auto-run training in notebook - let user control it\n",
    "print(\"✓ Pipeline classes loaded successfully!\")\n",
    "print(\"\\nTo train the model, run:\")\n",
    "print(\"  pipeline = RegularizedSP500Pipeline()\")\n",
    "print(\"  pipeline.fit_from_file('train.csv')\")\n",
    "print(\"\\nTo use for inference:\")\n",
    "print(\"  def predict(test: pl.DataFrame) -> float:\")\n",
    "print(\"      test_df = test.to_pandas()\")\n",
    "print(\"      return float(pipeline.predict_batch(test_df))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Pipeline\n",
    "\n",
    "\n",
    "Now you can manually control the training process:\n",
    "\n",
    "\n",
    "### Option 1: Train on Full Dataset\n",
    "```python\n",
    "pipeline = RegularizedSP500Pipeline()\n",
    "pipeline.fit_from_file('train.csv')\n",
    "pipeline.save_model('sp500_model.pkl')\n",
    "```\n",
    "\n",
    "\n",
    "### Option 2: Quick Test with Subset (Recommended First)\n",
    "```python\n",
    "# Test with first 1000 rows to verify everything works\n",
    "import pandas as pd\n",
    "df_small = pd.read_csv('train.csv', nrows=1000)\n",
    "df_small.to_csv('train_small.csv', index=False)\n",
    "\n",
    "\n",
    "pipeline = RegularizedSP500Pipeline()\n",
    "pipeline.fit_from_file('train_small.csv')\n",
    "```\n",
    "\n",
    "\n",
    "### Option 3: For Kaggle Submission\n",
    "After training, create the inference function:\n",
    "```python\n",
    "def create_prediction_function():\n",
    "    pipeline = RegularizedSP500Pipeline()\n",
    "    pipeline.load_model('sp500_model.pkl')  # Load pre-trained model\n",
    "    \n",
    "    def predict(test: pl.DataFrame) -> float:\n",
    "        try:\n",
    "            test_df = test.to_pandas()\n",
    "            pred = pipeline.predict_batch(test_df)\n",
    "            signal = np.clip(pred * 50.0 + 1.0, 0.0, 2.0)\n",
    "            return float(signal)\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            return 1.0\n",
    "    return predict\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING PIPELINE\n",
      "============================================================\n",
      "\n",
      "[1/6] Loading training data from kaggle/train.csv...\n",
      "  ✓ Loaded 7990 rows × 98 columns\n",
      "  ✓ Target variable: forward_returns (mean=0.000478, std=0.010836)\n",
      "\n",
      "[2/6] Preparing features...\n",
      "Creating features...\n",
      "  Original features: 94\n",
      "  Features after engineering: 376\n",
      "Handling missing values and outliers...\n",
      "Fitting scaler on 7990 samples with 376 features...\n",
      "  ✓ Final feature matrix: 7990 rows × 376 features\n",
      "\n",
      "[3/6] Running TimeSeriesSplit Cross-Validation (5 folds)...\n",
      "\n",
      "  Fold 1/5: train_size=1335, val_size=1331\n",
      "    [lgb] Training... RMSE=0.012821, R²=-0.005594\n",
      "    [xgb] Training... RMSE=0.012821, R²=-0.005594\n",
      "    [sp500_transformer] Training... RMSE=0.012821, R²=-0.005594\n",
      "    [Memory] Clearing memory for fold 1...\n",
      "    [Memory] Fold 1 memory cleared.\n",
      "\n",
      "  Fold 2/5: train_size=2666, val_size=1331\n",
      "    [lgb] Training... RMSE=0.012126, R²=0.001386\n",
      "    [xgb] Training... RMSE=0.012132, R²=0.000270\n",
      "    [sp500_transformer] Training... RMSE=0.012137, R²=-0.000414\n",
      "    [Memory] Clearing memory for fold 2...\n",
      "    [Memory] Fold 2 memory cleared.\n",
      "\n",
      "  Fold 3/5: train_size=3997, val_size=1331\n",
      "    [lgb] Training... RMSE=0.009790, R²=-0.001790\n",
      "    [xgb] Training... RMSE=0.009776, R²=0.000907\n",
      "    [sp500_transformer] Training... RMSE=0.009785, R²=-0.000874\n",
      "    [Memory] Clearing memory for fold 3...\n",
      "    [Memory] Fold 3 memory cleared.\n",
      "\n",
      "  Fold 4/5: train_size=5328, val_size=1331\n",
      "    [lgb] Training... RMSE=0.009913, R²=-0.001674\n",
      "    [xgb] Training... RMSE=0.009908, R²=-0.000757\n",
      "    [sp500_transformer] Training... RMSE=0.009905, R²=-0.000004\n",
      "    [Memory] Clearing memory for fold 4...\n",
      "    [Memory] Fold 4 memory cleared.\n",
      "\n",
      "  Fold 5/5: train_size=6659, val_size=1331\n",
      "    [lgb] Training... RMSE=0.010476, R²=0.000999\n",
      "    [xgb] Training... RMSE=0.010476, R²=0.000954\n",
      "    [sp500_transformer] Training... RMSE=0.010484, R²=-0.000544\n",
      "    [Memory] Clearing memory for fold 5...\n",
      "    [Memory] Fold 5 memory cleared.\n",
      "\n",
      "[4/6] Cross-Validation Results Summary:\n",
      "============================================================\n",
      "\n",
      "LGB:\n",
      "  RMSE: mean=0.011025, std=0.001225\n",
      "  R2: mean=-0.001335, std=0.002503\n",
      "  VAR_Y: mean=0.000123, std=0.000027\n",
      "\n",
      "XGB:\n",
      "  RMSE: mean=0.011023, std=0.001229\n",
      "  R2: mean=-0.000844, std=0.002454\n",
      "  VAR_Y: mean=0.000123, std=0.000027\n",
      "\n",
      "SP500_TRANSFORMER:\n",
      "  RMSE: mean=0.011026, std=0.001228\n",
      "  R2: mean=-0.001486, std=0.002073\n",
      "  VAR_Y: mean=0.000123, std=0.000027\n",
      "\n",
      "⚠️  WARNING: lgb has negative R² (-0.001) - worse than baseline!\n",
      "\n",
      "⚠️  WARNING: xgb has negative R² (-0.001) - worse than baseline!\n",
      "\n",
      "⚠️  WARNING: sp500_transformer has negative R² (-0.001) - worse than baseline!\n",
      "\n",
      "[5/6] Refitting models on full dataset...\n",
      "  [lgb] Training on full data... ✓\n",
      "  [xgb] Training on full data... ✓\n",
      "  [sp500_transformer] Training on full data... ✓\n",
      "\n",
      "[6/6] Calculating ensemble weights...\n",
      "  Final model weights:\n",
      "    lgb: 0.333\n",
      "    xgb: 0.333\n",
      "    sp500_transformer: 0.333\n",
      "\n",
      "============================================================\n",
      "✓ TRAINING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RegularizedSP500Pipeline at 0x7a4b574b9250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the FAST optimized version\n",
    "pipeline = RegularizedSP500Pipeline()\n",
    "pipeline.fit_from_file('kaggle/train.csv')\n",
    "#pipeline.save_model('features/sp500_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13750964,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ml_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
