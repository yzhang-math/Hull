{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# XGBoost FQI Agent with Time-Series Cross-Validation\n",
    "\n",
    "This notebook implements a Fitted Q-Iteration (FQI) agent using XGBoost and evaluates it using rigorous time-series cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "imports-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from collections import deque\n",
    "\n",
    "# Set up the device for XGBoost\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-section",
   "metadata": {},
   "source": [
    "## 2. Competition Evaluation Function\n",
    "\n",
    "This is the official scoring metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "calculate-score-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_competition_score(y_true_df: pd.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "    ''' Calculates the competition score based on true values and predicted signals. '''\n",
    "    solution = y_true_df.copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "    solution['position'] = y_pred_signals\n",
    "    solution['strategy_returns'] = (\n",
    "        solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "        solution['position'] * solution['forward_returns']\n",
    "    )\n",
    "    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "    strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "    strategy_std = solution['strategy_returns'].std()\n",
    "    \n",
    "    # Handle zero std dev (e.g., if agent learns to always output 0)\n",
    "    if strategy_std < 1e-10: return 0.0 \n",
    "    \n",
    "    trading_days_per_yr = 252\n",
    "    sharpe = strategy_geo_mean / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "    market_std = solution['forward_returns'].std()\n",
    "    market_volatility = market_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    strategy_volatility = strategy_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    \n",
    "    # Handle zero market vol\n",
    "    if market_volatility < 1e-8: \n",
    "        excess_vol = 0.0\n",
    "    else:\n",
    "        excess_vol = max(0, strategy_volatility / market_volatility - 1.2)\n",
    "        \n",
    "    vol_penalty = 1 + excess_vol\n",
    "    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "    market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "    return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * trading_days_per_yr)\n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    \n",
    "    return adjusted_sharpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-section",
   "metadata": {},
   "source": [
    "## 3. Reinforcement Learning Environment\n",
    "\n",
    "This environment provides a simple, step-by-step reward based on portfolio returns for training the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "rl-env-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RlTradingEnv:\n",
    "    \"\"\"\n",
    "    A simple RL environment for trading.\n",
    "    It provides a per-step reward based on the 'target' column.\n",
    "    It uses 'calculate_competition_score' for final evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, targets, scorer_info, transaction_cost=0.0001):\n",
    "        self.features = features.to_numpy()\n",
    "        self.targets = targets.to_numpy().flatten()\n",
    "        self.scorer_info_df = scorer_info.to_pandas() \n",
    "        \n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.reward_scale = 100.0\n",
    "        \n",
    "        self.n_steps = len(self.features)\n",
    "        self.n_features = self.features.shape[1]\n",
    "        self.action_space_dim = 1\n",
    "        self.max_action = 2.0\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.last_leverage = 0.0\n",
    "        self.nan_fallback_counter = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment and returns the first state.\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.last_leverage = 0.0\n",
    "        self.nan_fallback_counter = 0\n",
    "        return self.features[self.current_step]\n",
    "\n",
    "    def step(self, action_leverage):\n",
    "        \"\"\"\n",
    "        Takes an action, calculates reward, and returns the next state.\n",
    "        \"\"\"\n",
    "        if self.current_step >= self.n_steps - 2:\n",
    "            return self.features[self.current_step], 0.0, True\n",
    "\n",
    "        leverage = np.clip(action_leverage, 0.0, self.max_action)[0]\n",
    "        target_excess_return = self.targets[self.current_step]\n",
    "        \n",
    "        cost = self.transaction_cost * abs(leverage - self.last_leverage)\n",
    "        reward = (leverage * target_excess_return) - cost\n",
    "        \n",
    "        if np.isnan(reward) or np.isinf(reward):\n",
    "            reward = 0.0 \n",
    "            self.nan_fallback_counter += 1\n",
    "\n",
    "        self.last_leverage = leverage\n",
    "        self.current_step += 1\n",
    "        next_state = self.features[self.current_step]\n",
    "        done = (self.current_step == self.n_steps - 2)\n",
    "        \n",
    "        return next_state, (reward * self.reward_scale), done\n",
    "\n",
    "    def run_evaluation(self, policy_agent):\n",
    "        \"\"\"\n",
    "        Runs a full backtest with a given policy (agent)\n",
    "        and returns the final adjusted Sharpe score.\n",
    "        \n",
    "        This can accept both PyTorch and XGBoost agents.\n",
    "        \"\"\"\n",
    "        state = self.reset()\n",
    "        signals = []\n",
    "        \n",
    "        for t in range(self.n_steps - 1):\n",
    "            # Check if it's a PyTorch agent (like SAC or TD3)\n",
    "            # Note: We aren't using this now, but it keeps the env flexible.\n",
    "            if hasattr(policy_agent, 'actor') and isinstance(policy_agent.actor, torch.nn.Module):\n",
    "                actor_device = next(policy_agent.actor.parameters()).device\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(actor_device)\n",
    "                if hasattr(policy_agent, 'select_action_deterministic'):\n",
    "                    action = policy_agent.select_action_deterministic(state_tensor)\n",
    "                else: # Fallback for TD3-style agent\n",
    "                    action = policy_agent.actor(state_tensor).cpu().data.numpy().flatten()\n",
    "            \n",
    "            # Check if it's our XGBoost FQI agent\n",
    "            elif isinstance(policy_agent, XGBoost_FQI_Agent):\n",
    "                action = [policy_agent.select_action(state, exploration_rate=0.0)]\n",
    "            \n",
    "            else:\n",
    "                raise TypeError(\"Unknown agent type passed to run_evaluation\")\n",
    "                \n",
    "            signals.append(np.clip(action, 0.0, self.max_action)[0])\n",
    "            \n",
    "            # We don't need reward here, just the next state\n",
    "            if t < self.n_steps - 2:\n",
    "                state = self.features[t + 1]\n",
    "        \n",
    "        # Score the *entire* run\n",
    "        scorer_df_trimmed = self.scorer_info_df.iloc[:len(signals)]\n",
    "        return calculate_competition_score(scorer_df_trimmed, np.array(signals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buffer-section",
   "metadata": {},
   "source": [
    "## 4. Replay Buffer\n",
    "\n",
    "A simple deque-based replay buffer to store (s, a, r, s', d) transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "replay-buffer-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A simple replay buffer for off-policy learning.\"\"\"\n",
    "    def __init__(self, max_size=1_000_000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (np.array(states), np.array(actions), \n",
    "                np.array(rewards).reshape(-1, 1), \n",
    "                np.array(next_states), \n",
    "                np.array(dones).reshape(-1, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-section",
   "metadata": {},
   "source": [
    "## 5. Data Loading and Feature Engineering\n",
    "\n",
    "This section contains the feature generation logic and the main data loading function. `load_full_data_for_cv` is designed to load the *entire* dataset, which we can then split using `TimeSeriesSplit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "data-loading-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_7 (df: pl.DataFrame) -> pl.DataFrame:\n",
    "  \"\"\"Generates new features from the base data.\n",
    "    This function is the target of the evolutionary algorithm.\n",
    "  \n",
    "    Available Feature Categories:\n",
    "    - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "    - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "    - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "    - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "    - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "    - S* (Sentiment features): 12 columns (S1-S12)\n",
    "    - V* (Volatility features): 13 columns (V1-V13)\n",
    "  \"\"\"\n",
    "  new_features = pl.DataFrame({\n",
    "      # --- 20 Pairwise Interactions ---\n",
    "      'feat_M1_x_V1': df['M1'] * df['V1'],\n",
    "      'feat_P1_add_E1': df['P1'] + df['E1'],\n",
    "      'feat_S1_sub_I1': df['S1'] - df['I1'],\n",
    "      'feat_M10_div_V10': df['M10'] / (df['V10'] + 1e-6),\n",
    "      'feat_P10_x_E10': df['P10'] * df['E10'],\n",
    "      'feat_M2_x_S3': df['M2'] * df['S3'],\n",
    "      'feat_V2_div_P2': df['V2'] / (df['P2'] + 1e-6),\n",
    "      'feat_E4_sub_I3': df['E4'] - df['I3'],\n",
    "      'feat_S7_add_M12': df['S7'] + df['M12'],\n",
    "      'feat_I5_x_V11': df['I5'] * df['V11'],\n",
    "      'feat_P5_div_S8': df['P5'] / (df['S8'] + 1e-6),\n",
    "      'feat_E12_x_I9': df['E12'] * df['I9'],\n",
    "      'feat_M1_div_S1': df['M1'] / (df['S1'] + 1e-6),\n",
    "      'feat_V1_add_P1': df['V1'] + df['P1'],\n",
    "      'feat_E1_sub_I1': df['E1'] - df['I1'],\n",
    "      'feat_M2_div_V2': df['M2'] / (df['V2'] + 1e-6),\n",
    "      'feat_P2_x_S3': df['P2'] * df['S3'],\n",
    "      'feat_E4_add_M10': df['E4'] + df['M10'],\n",
    "      'feat_I3_sub_V10': df['I3'] - df['V10'],\n",
    "      'feat_S7_x_P10': df['S7'] * df['P10'],\n",
    "      # --- 10 Rolling Window Features ---\n",
    "      'feat_V2_roll_mean_5': df['V2'].rolling_mean(window_size=5),\n",
    "      'feat_V1_roll_std_5': df['V1'].rolling_std(window_size=5),\n",
    "      'feat_M1_roll_mean_20': df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_M3_roll_std_20': df['M3'].rolling_std(window_size=20),\n",
    "      'feat_P1_roll_max_10': df['P1'].rolling_max(window_size=10),\n",
    "      'feat_P1_roll_min_10': df['P1'].rolling_min(window_size=10),\n",
    "      'feat_E5_roll_mean_50': df['E5'].rolling_mean(window_size=50),\n",
    "      'feat_S1_roll_std_50': df['S1'].rolling_std(window_size=50),\n",
    "      'feat_I1_roll_mean_10': df['I1'].rolling_mean(window_size=10),\n",
    "      'feat_V10_roll_std_10': df['V10'].rolling_std(window_size=10),\n",
    "      # --- 10 Complex Interactions (3+ elements) ---\n",
    "      'feat_M1_V1_div_P1': (df['M1'] * df['V1']) / (df['P1'] + 1e-6),\n",
    "      'feat_E1_S1_add_I1': df['E1'] + df['S1'] - df['I1'],\n",
    "      'feat_M2_P2_sub_V2': df['M2'] + df['P2'] - df['V2'],\n",
    "      'feat_S7_div_E4_I3': df['S7'] / (df['E4'] + df['I3'] + 1e-6),\n",
    "      'feat_P5_x_M10_x_V10': df['P5'] * df['M10'] * df['V10'],\n",
    "      'feat_roll_diff_M1_5_20': df['M1'].rolling_mean(window_size=5) - df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_roll_diff_V1_5_20': df['V1'].rolling_mean(window_size=5) - df['V1'].rolling_mean(window_size=20),\n",
    "      'feat_M_S_P_combo': (df['M12'] - df['M1']) / (df['S1'] + df['P1'] + 1e-6),\n",
    "      'feat_V_E_I_combo': (df['V11'] + df['V2']) * (df['E1'] - df['I1']),\n",
    "      'feat_ratio_of_ratios': (df['M1']/(df['V1']+1e-6)) / (df['P1']/(df['S1']+1e-6)),\n",
    "      # --- 10 New Features ---\n",
    "      'feat_M1_x_V1_x_P1': df['M1'] * df['V1'] * df['P1'],\n",
    "      'feat_E1_div_S1': df['E1'] / (df['S1'] + 1e-6),\n",
    "      'feat_I1_sub_V1': df['I1'] - df['V1'],\n",
    "      'feat_M10_add_V10': df['M10'] + df['V10'],\n",
    "      'feat_P10_div_E10': df['P10'] / (df['E10'] + 1e-6),\n",
    "      'feat_M2_add_S3': df['M2'] + df['S3'],\n",
    "      'feat_V2_x_P2': df['V2'] * df['P2'],\n",
    "      'feat_E4_add_I3': df['E4'] + df['I3'],\n",
    "      'feat_S7_div_M12': df['S7'] / (df['M12'] + 1e-6),\n",
    "      'feat_I5_div_V11': df['I5'] / (df['V11'] + 1e-6),\n",
    "  })\n",
    "  # Fill any nulls created by rolling windows\n",
    "  return new_features.with_columns(pl.all().forward_fill())\n",
    "\n",
    "\n",
    "def load_full_data_for_cv(train_path, spy_path, slice_start=2000):\n",
    "    \"\"\"\n",
    "    Loads and processes the *entire* dataset for use in cross-validation.\n",
    "    This is the robust version from your SAC CV loop.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load base data and slice\n",
    "    full_train_df = pl.read_csv(train_path)\n",
    "    df_raw = full_train_df.slice(slice_start)\n",
    "    \n",
    "    # 2. Basic cleaning (robust version)\n",
    "    df = df_raw.with_columns(\n",
    "        pl.selectors.float().replace([np.inf, -np.inf], None) \n",
    "    ).with_columns(\n",
    "        pl.selectors.numeric().fill_null(0.0) \n",
    "    )\n",
    "    df = df.with_columns(pl.col(\"date_id\").cast(pl.Int64))\n",
    "\n",
    "    # 3. Add weekday feature\n",
    "    spy_df = pl.read_csv(spy_path)\n",
    "    weekday_df = spy_df.with_columns(\n",
    "        pl.col(\"Date\").str.to_date().dt.weekday().alias(\"weekday\")\n",
    "    ).select([\"date_id\", \"weekday\"])\n",
    "    df_with_weekday = df.join(weekday_df, on=\"date_id\", how=\"left\").fill_null(0.0)\n",
    "    \n",
    "    # 4. Prep for feature generation\n",
    "    base_df = df_with_weekday.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    if 'E7' in base_df.columns:\n",
    "        base_df = base_df.drop('E7')\n",
    "        \n",
    "    # Pre-clean base_df\n",
    "    base_df = base_df.with_columns(\n",
    "        pl.selectors.float().replace([np.inf, -np.inf], None)\n",
    "    ).with_columns(\n",
    "        pl.all().fill_null(0.0).forward_fill().backward_fill()\n",
    "    )\n",
    "\n",
    "    # 5. Generate and combine features\n",
    "    new_features_df = generate_features_7(base_df) \n",
    "    processed_df = pl.concat([base_df, new_features_df], how=\"horizontal\")\n",
    "    \n",
    "    # 6. Finalize X, y, and scorer_info\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_feature_names = new_features_df.columns\n",
    "    ALL_FEATURES = base_features + new_feature_names\n",
    "    \n",
    "    Xy = pl.concat([processed_df.select(ALL_FEATURES), \n",
    "                    processed_df.select(\"target\"), \n",
    "                    processed_df.select([\"forward_returns\", \"risk_free_rate\"])], \n",
    "                   how=\"horizontal\")\n",
    "    \n",
    "    # Final robust cleaning\n",
    "    Xy = Xy.with_columns(\n",
    "        pl.selectors.float().replace([np.inf, -np.inf], None)\n",
    "    )\n",
    "    original_rows = Xy.height\n",
    "    Xy = Xy.drop_nulls()\n",
    "    cleaned_rows = Xy.height\n",
    "    \n",
    "    X = Xy.select(ALL_FEATURES)\n",
    "    y = Xy.select(\"target\")\n",
    "    scorer_info_df = Xy.select([\"forward_returns\", \"risk_free_rate\"])\n",
    "    \n",
    "    print(f\"Data ready. Total cleaned rows: {cleaned_rows} (from {original_rows})\")\n",
    "    print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "    \n",
    "    return X, y, scorer_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-section",
   "metadata": {},
   "source": [
    "## 6. XGBoost FQI Agent\n",
    "\n",
    "This is the Fitted Q-Iteration (FQI) agent using XGBoost. It learns a Q-function (value of state-action pairs) offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "xgb-fqi-agent-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost_FQI_Agent:\n",
    "    \"\"\"\n",
    "    Fitted Q-Iteration (FQI) agent using XGBoost.\n",
    "    This requires a DISCRETE action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, gamma=0.95, n_actions=11):\n",
    "        # 1. DISCRETIZE the action space\n",
    "        self.action_space = np.linspace(0.0, 2.0, num=n_actions)\n",
    "        self.n_actions = len(self.action_space)\n",
    "        print(f\"XGBoost agent initialized with {self.n_actions} discrete actions: {np.round(self.action_space, 2)}\")\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # We use a MultiOutputRegressor to fit one XGB model per action\n",
    "        self.model = MultiOutputRegressor(\n",
    "            xgb.XGBRegressor(\n",
    "                objective='reg:squarederror', n_estimators=20,\n",
    "                learning_rate=0.05, max_depth=5, subsample=0.8, \n",
    "                colsample_bytree=0.8, n_jobs=-1, random_state=42,\n",
    "                device=device \n",
    "            )\n",
    "        )\n",
    "        # Fit with dummy data to initialize\n",
    "        self.model.fit(np.random.rand(1, state_dim), np.random.rand(1, self.n_actions))\n",
    "\n",
    "    def select_action(self, state, exploration_rate=0.1):\n",
    "        \"\"\"Selects the best action (epsilon-greedy).\"\"\"\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return np.random.choice(self.action_space)\n",
    "        \n",
    "        q_values = self.model.predict(state.reshape(1, -1))[0]\n",
    "        best_action_idx = np.argmax(q_values)\n",
    "        return self.action_space[best_action_idx]\n",
    "\n",
    "    def train_fqi(self, replay_buffer, iterations=3, batch_size=500_000):\n",
    "        \"\"\"\n",
    "        Trains the XGBoost model using Fitted Q-Iteration.\n",
    "        This is an OFFLINE, BATCH process.\n",
    "        It samples from the buffer to keep training time reasonable.\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Starting XGBoost FQI Training ({iterations} iterations) ---\")\n",
    "        \n",
    "        if len(replay_buffer) < 1000:\n",
    "             print(\"Buffer not full enough. Need at least 1000 samples. Skipping training.\")\n",
    "             return\n",
    "        \n",
    "        # Sample a large batch from the buffer\n",
    "        # Use min() to avoid error if buffer is smaller than batch_size\n",
    "        sample_size = min(len(replay_buffer), batch_size)\n",
    "        print(f\"Sampling {sample_size} transitions from buffer...\")\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(sample_size)\n",
    "        \n",
    "        for k in range(iterations):\n",
    "            print(f\"FQI Iteration {k+1}/{iterations}...\")\n",
    "            \n",
    "            # 1. Calculate the target Q-value (Bellman update)\n",
    "            # Q_target(s, a) = r + gamma * max_a'(Q_k(s', a'))\n",
    "            \n",
    "            # Predict Q(s', a') for all next_states\n",
    "            next_q_values = self.model.predict(next_states)\n",
    "            \n",
    "            # Find max_a'(Q_k(s', a'))\n",
    "            max_next_q = np.max(next_q_values, axis=1)\n",
    "            \n",
    "            # The target: r + gamma * max_Q (or just r if done)\n",
    "            target_q = rewards.flatten() + (1.0 - dones.flatten()) * self.gamma * max_next_q\n",
    "            \n",
    "            # 2. Create the training set for the *new* XGBoost model\n",
    "            # We want to train Q_k+1(s, a) -> target_q\n",
    "            X_train = states\n",
    "            \n",
    "            # Start with the model's current predictions as the base\n",
    "            y_train = self.model.predict(X_train)\n",
    "            \n",
    "            # Update y_train at the index of the action *actually taken*\n",
    "            for i in range(len(states)):\n",
    "                action_taken = actions[i][0]\n",
    "                # Find the *closest* index of the action in our discrete space\n",
    "                action_idx = (np.abs(self.action_space - action_taken)).argmin()\n",
    "                y_train[i, action_idx] = target_q[i]\n",
    "\n",
    "            # 4. Train a new XGBoost model on these (s, a) -> target_q pairs\n",
    "            print(\"Fitting new XGBoost model...\")\n",
    "            self.model.fit(X_train, y_train)\n",
    "            \n",
    "        print(\"--- FQI Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cv-section",
   "metadata": {},
   "source": [
    "## 7. Main Cross-Validation Execution\n",
    "\n",
    "This is the main script to run the cross-validation for the `XGBoost_FQI_Agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "main-cv-loop-code",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing full dataset for CV...\n",
      "Data ready. Total cleaned rows: 6941 (from 6990)\n",
      "Features shape: (6941, 144), Target shape: (6941, 1)\n",
      "\n",
      "==================================================\n",
      "Starting XGBoost FQI Cross-Validation with 20 Folds\n",
      "==================================================\n",
      "\n",
      "--- Starting Fold 1/20 ---\n",
      "Train indices: 341, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 341 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Buffer not full enough. Need at least 1000 samples. Skipping training.\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 1 Complete. Test Sharpe: 0.0645 ---\n",
      "\n",
      "--- Starting Fold 2/20 ---\n",
      "Train indices: 671, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 671 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Buffer not full enough. Need at least 1000 samples. Skipping training.\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 2 Complete. Test Sharpe: 0.0000 ---\n",
      "\n",
      "--- Starting Fold 3/20 ---\n",
      "Train indices: 1001, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 1001 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Buffer not full enough. Need at least 1000 samples. Skipping training.\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 3 Complete. Test Sharpe: -0.5732 ---\n",
      "\n",
      "--- Starting Fold 4/20 ---\n",
      "Train indices: 1331, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 1331 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 1329 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 4 Complete. Test Sharpe: 0.9078 ---\n",
      "\n",
      "--- Starting Fold 5/20 ---\n",
      "Train indices: 1661, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 1661 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 1659 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 5 Complete. Test Sharpe: 0.4559 ---\n",
      "\n",
      "--- Starting Fold 6/20 ---\n",
      "Train indices: 1991, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 1991 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 1989 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 6 Complete. Test Sharpe: 0.3217 ---\n",
      "\n",
      "--- Starting Fold 7/20 ---\n",
      "Train indices: 2321, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 2321 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 2319 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 7 Complete. Test Sharpe: -0.3624 ---\n",
      "\n",
      "--- Starting Fold 8/20 ---\n",
      "Train indices: 2651, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 2651 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 2649 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 8 Complete. Test Sharpe: -0.0609 ---\n",
      "\n",
      "--- Starting Fold 9/20 ---\n",
      "Train indices: 2981, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 2981 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 2979 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 9 Complete. Test Sharpe: 0.6921 ---\n",
      "\n",
      "--- Starting Fold 10/20 ---\n",
      "Train indices: 3311, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 3311 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 3309 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 10 Complete. Test Sharpe: 0.2388 ---\n",
      "\n",
      "--- Starting Fold 11/20 ---\n",
      "Train indices: 3641, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 3641 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 3639 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 11 Complete. Test Sharpe: 1.0272 ---\n",
      "\n",
      "--- Starting Fold 12/20 ---\n",
      "Train indices: 3971, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 3971 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 3969 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 12 Complete. Test Sharpe: 0.5926 ---\n",
      "\n",
      "--- Starting Fold 13/20 ---\n",
      "Train indices: 4301, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 4301 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 4299 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 13 Complete. Test Sharpe: 0.1387 ---\n",
      "\n",
      "--- Starting Fold 14/20 ---\n",
      "Train indices: 4631, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 4631 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 4629 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 14 Complete. Test Sharpe: -0.2275 ---\n",
      "\n",
      "--- Starting Fold 15/20 ---\n",
      "Train indices: 4961, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 4961 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 4959 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 15 Complete. Test Sharpe: 0.3313 ---\n",
      "\n",
      "--- Starting Fold 16/20 ---\n",
      "Train indices: 5291, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 5291 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 5289 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 16 Complete. Test Sharpe: 0.2521 ---\n",
      "\n",
      "--- Starting Fold 17/20 ---\n",
      "Train indices: 5621, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 5621 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 5619 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 17 Complete. Test Sharpe: 1.4919 ---\n",
      "\n",
      "--- Starting Fold 18/20 ---\n",
      "Train indices: 5951, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 5951 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 5949 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 18 Complete. Test Sharpe: 0.3332 ---\n",
      "\n",
      "--- Starting Fold 19/20 ---\n",
      "Train indices: 6281, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 6281 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 6279 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 19 Complete. Test Sharpe: 1.0399 ---\n",
      "\n",
      "--- Starting Fold 20/20 ---\n",
      "Train indices: 6611, Test indices: 330\n",
      "XGBoost agent initialized with 5 discrete actions: [0.  0.5 1.  1.5 2. ]\n",
      "Populating replay buffer with 6611 transitions...\n",
      "\n",
      "--- Starting XGBoost FQI Training (3 iterations) ---\n",
      "Sampling 6609 transitions from buffer...\n",
      "FQI Iteration 1/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 2/3...\n",
      "Fitting new XGBoost model...\n",
      "FQI Iteration 3/3...\n",
      "Fitting new XGBoost model...\n",
      "--- FQI Training Complete ---\n",
      "Evaluating agent on test fold...\n",
      "--- Fold 20 Complete. Test Sharpe: 0.4518 ---\n",
      "\n",
      "==================================================\n",
      "Cross-Validation Complete - Aggregated Results\n",
      "==================================================\n",
      "Scores per fold: [ 0.0645  0.     -0.5732  0.9078  0.4559  0.3217 -0.3624 -0.0609  0.6921\n",
      "  0.2388  1.0272  0.5926  0.1387 -0.2275  0.3313  0.2521  1.4919  0.3332\n",
      "  1.0399  0.4518]\n",
      "\n",
      "--- Final Model Performance ---\n",
      "Mean Adjusted Sharpe: 0.3558\n",
      "Std Dev of Sharpe:  0.4940\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define Paths and CV Parameters ---\n",
    "TRAIN_DATA_PATH = \"./kaggle/train.csv\"\n",
    "SPY_DATA_PATH = \"./kaggle/spy-historical.csv\"\n",
    "\n",
    "N_SPLITS = 20              # Number of folds for cross-validation\n",
    "FQI_ITERATIONS = 3         # Number of FQI iterations per fold\n",
    "FQI_BATCH_SIZE = 1000_000   # Max samples to use for training FQI\n",
    "TRANSACTION_COST = 0 \n",
    "\n",
    "# --- 2. Load Full Dataset ---\n",
    "print(\"Loading and preparing full dataset for CV...\")\n",
    "X_full, y_full, scorer_info_full = \\\n",
    "    load_full_data_for_cv(TRAIN_DATA_PATH, SPY_DATA_PATH, slice_start=2000)\n",
    "\n",
    "# --- 3. Setup CV Loop ---\n",
    "tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "all_fold_scores = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Starting XGBoost FQI Cross-Validation with {N_SPLITS} Folds\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fold_num = 0\n",
    "for train_index, test_index in tscv.split(X_full):\n",
    "    fold_num += 1\n",
    "    print(f\"\\n--- Starting Fold {fold_num}/{N_SPLITS} ---\")\n",
    "    print(f\"Train indices: {len(train_index)}, Test indices: {len(test_index)}\")\n",
    "\n",
    "    # --- 4. Create data and environments for THIS fold ---\n",
    "    X_train, X_test = X_full[train_index], X_full[test_index]\n",
    "    y_train, y_test = y_full[train_index], y_full[test_index]\n",
    "    scorer_train, scorer_test = scorer_info_full[train_index], scorer_info_full[test_index]\n",
    "    \n",
    "    # Use 0 transaction cost for data collection to get cleaner Q-values\n",
    "    train_env_fold = RlTradingEnv(X_train, y_train, scorer_train, transaction_cost=0)\n",
    "    # Use real transaction cost for evaluation\n",
    "    test_env_fold = RlTradingEnv(X_test, y_test, scorer_test, transaction_cost=TRANSACTION_COST)\n",
    "\n",
    "    # --- 5. CRITICAL: Re-initialize Agent and Buffer for each fold ---\n",
    "    state_dim = train_env_fold.n_features\n",
    "    agent_fqi_fold = XGBoost_FQI_Agent(state_dim, gamma=0.95, n_actions=5)\n",
    "    buffer_fqi_fold = ReplayBuffer(max_size=2_000_000) # Large buffer\n",
    "    \n",
    "    # --- 6. Populate Buffer (Offline Data Collection) ---\n",
    "    # We use a simple random policy to explore the state-action space\n",
    "    print(f\"Populating replay buffer with {train_env_fold.n_steps} transitions...\")\n",
    "    state = train_env_fold.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.random.choice(agent_fqi_fold.action_space)\n",
    "        next_state, reward, done = train_env_fold.step(np.array([action]))\n",
    "        buffer_fqi_fold.add(state, np.array([action]), reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "    # --- 7. Train Agent (Offline FQI) ---\n",
    "    agent_fqi_fold.train_fqi(\n",
    "        buffer_fqi_fold, \n",
    "        iterations=FQI_ITERATIONS, \n",
    "        batch_size=FQI_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # --- 8. Evaluate Agent on Test Fold ---\n",
    "    print(\"Evaluating agent on test fold...\")\n",
    "    # We use run_evaluation with exploration_rate=0.0 (deterministic policy)\n",
    "    eval_score = test_env_fold.run_evaluation(agent_fqi_fold)\n",
    "    all_fold_scores.append(eval_score)\n",
    "    print(f\"--- Fold {fold_num} Complete. Test Sharpe: {eval_score:.4f} ---\")\n",
    "\n",
    "# --- 9. Final Results ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Cross-Validation Complete - Aggregated Results\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "scores_array = np.array(all_fold_scores)\n",
    "\n",
    "print(f\"Scores per fold: {np.round(scores_array, 4)}\")\n",
    "print(\"\\n--- Final Model Performance ---\")\n",
    "print(f\"Mean Adjusted Sharpe: {np.mean(scores_array):.4f}\")\n",
    "print(f\"Std Dev of Sharpe:  {np.std(scores_array):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
