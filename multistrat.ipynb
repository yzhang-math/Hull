{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0959d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# The training data path should be updated to your actual training file.\n",
    "TRAIN_DATA_PATH = \"./kaggle/train.csv\"\n",
    "SPY_DATA_PATH = \"./kaggle/spy-historical.csv\"\n",
    "\n",
    "def generate_features_7 (df: pl.DataFrame) -> pl.DataFrame:\n",
    "  \"\"\"Generates new features from the base data.\n",
    "    This function is the target of the evolutionary algorithm.\n",
    "  \n",
    "    Available Feature Categories:\n",
    "    - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "    - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "    - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "    - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "    - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "    - S* (Sentiment features): 12 columns (S1-S12)\n",
    "    - V* (Volatility features): 13 columns (V1-V13)\n",
    "  \"\"\"\n",
    "  new_features = pl.DataFrame({\n",
    "      # --- 20 Pairwise Interactions ---\n",
    "      'feat_M1_x_V1': df['M1'] * df['V1'],\n",
    "      'feat_P1_add_E1': df['P1'] + df['E1'],\n",
    "      'feat_S1_sub_I1': df['S1'] - df['I1'],\n",
    "      'feat_M10_div_V10': df['M10'] / (df['V10'] + 1e-6),\n",
    "      'feat_P10_x_E10': df['P10'] * df['E10'],\n",
    "      'feat_M2_x_S3': df['M2'] * df['S3'],\n",
    "      'feat_V2_div_P2': df['V2'] / (df['P2'] + 1e-6),\n",
    "      'feat_E4_sub_I3': df['E4'] - df['I3'],\n",
    "      'feat_S7_add_M12': df['S7'] + df['M12'],\n",
    "      'feat_I5_x_V11': df['I5'] * df['V11'],\n",
    "      'feat_P5_div_S8': df['P5'] / (df['S8'] + 1e-6),\n",
    "      'feat_E12_x_I9': df['E12'] * df['I9'],\n",
    "      'feat_M1_div_S1': df['M1'] / (df['S1'] + 1e-6),\n",
    "      'feat_V1_add_P1': df['V1'] + df['P1'],\n",
    "      'feat_E1_sub_I1': df['E1'] - df['I1'],\n",
    "      'feat_M2_div_V2': df['M2'] / (df['V2'] + 1e-6),\n",
    "      'feat_P2_x_S3': df['P2'] * df['S3'],\n",
    "      'feat_E4_add_M10': df['E4'] + df['M10'],\n",
    "      'feat_I3_sub_V10': df['I3'] - df['V10'],\n",
    "      'feat_S7_x_P10': df['S7'] * df['P10'],\n",
    "      # --- 10 Rolling Window Features ---\n",
    "      'feat_V2_roll_mean_5': df['V2'].rolling_mean(window_size=5),\n",
    "      'feat_V1_roll_std_5': df['V1'].rolling_std(window_size=5),\n",
    "      'feat_M1_roll_mean_20': df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_M3_roll_std_20': df['M3'].rolling_std(window_size=20),\n",
    "      'feat_P1_roll_max_10': df['P1'].rolling_max(window_size=10),\n",
    "      'feat_P1_roll_min_10': df['P1'].rolling_min(window_size=10),\n",
    "      'feat_E5_roll_mean_50': df['E5'].rolling_mean(window_size=50),\n",
    "      'feat_S1_roll_std_50': df['S1'].rolling_std(window_size=50),\n",
    "      'feat_I1_roll_mean_10': df['I1'].rolling_mean(window_size=10),\n",
    "      'feat_V10_roll_std_10': df['V10'].rolling_std(window_size=10),\n",
    "      # --- 10 Complex Interactions (3+ elements) ---\n",
    "      'feat_M1_V1_div_P1': (df['M1'] * df['V1']) / (df['P1'] + 1e-6),\n",
    "      'feat_E1_S1_add_I1': df['E1'] + df['S1'] - df['I1'],\n",
    "      'feat_M2_P2_sub_V2': df['M2'] + df['P2'] - df['V2'],\n",
    "      'feat_S7_div_E4_I3': df['S7'] / (df['E4'] + df['I3'] + 1e-6),\n",
    "      'feat_P5_x_M10_x_V10': df['P5'] * df['M10'] * df['V10'],\n",
    "      'feat_roll_diff_M1_5_20': df['M1'].rolling_mean(window_size=5) - df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_roll_diff_V1_5_20': df['V1'].rolling_mean(window_size=5) - df['V1'].rolling_mean(window_size=20),\n",
    "      'feat_M_S_P_combo': (df['M12'] - df['M1']) / (df['S1'] + df['P1'] + 1e-6),\n",
    "      'feat_V_E_I_combo': (df['V11'] + df['V2']) * (df['E1'] - df['I1']),\n",
    "      'feat_ratio_of_ratios': (df['M1']/(df['V1']+1e-6)) / (df['P1']/(df['S1']+1e-6)),\n",
    "      # --- 10 New Features ---\n",
    "      'feat_M1_x_V1_x_P1': df['M1'] * df['V1'] * df['P1'],\n",
    "      'feat_E1_div_S1': df['E1'] / (df['S1'] + 1e-6),\n",
    "      'feat_I1_sub_V1': df['I1'] - df['V1'],\n",
    "      'feat_M10_add_V10': df['M10'] + df['V10'],\n",
    "      'feat_P10_div_E10': df['P10'] / (df['E10'] + 1e-6),\n",
    "      'feat_M2_add_S3': df['M2'] + df['S3'],\n",
    "      'feat_V2_x_P2': df['V2'] * df['P2'],\n",
    "      'feat_E4_add_I3': df['E4'] + df['I3'],\n",
    "      'feat_S7_div_M12': df['S7'] / (df['M12'] + 1e-6),\n",
    "      'feat_I5_div_V11': df['I5'] / (df['V11'] + 1e-6),\n",
    "      #'feat_M1_log_P1': np.log(df['M1'] + 1e-6) / np.log(df['P1'] + 1e-6),\n",
    "      # --- SAFER LOGIC HERE ---\n",
    "      #'feat_M1_log_P1': pl.when( (df['M1'] > 0) & (df['P1'] > 0) & (df['P1'] != 1) ).then( df['M1'].log() / df['P1'].log() ).otherwise(0),\n",
    "      # --- END SAFER LOGIC ---\n",
    "  })\n",
    "  # Fill any nulls created by rolling windows\n",
    "  return new_features.with_columns(pl.all().forward_fill())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83e335f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solve: no reranking\n",
    "\n",
    "# --- Make sure all your feature generators are defined above this ---\n",
    "# (We will only use generate_features_7, but it must be defined)\n",
    "# e.g., generate_features_1, ... generate_features_7\n",
    "\n",
    "def solve(df: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Runs a time-series cross-validation process using a \"top-k leader\"\n",
    "    ensemble strategy.\n",
    "    \n",
    "    All N strategies use the *same* feature set (gen_features_7)\n",
    "    but are trained on *random subsets* of the training data to \n",
    "    create \"orthogonal\" models.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Ensemble Configuration ---\n",
    "    N_STRATEGIES = 10           # We'll create 10 strategies\n",
    "    SUBSAMPLE_RATIO = 0.4       # Each strategy sees 70% of the training data\n",
    "    FEATURE_GENERATOR = generate_features_7 # All strategies use this\n",
    "    \n",
    "    K_LEADERS = 2               # How many leaders to follow (e.g., 3)\n",
    "    ROLLING_WINDOW_DAYS = 30    # Lookback period to find leaders\n",
    "    MIN_HISTORY_DAYS = 15       # Min days needed to start picking leaders\n",
    "    TRADING_DAYS_PER_YR = 252\n",
    "    nsplits = 20\n",
    "\n",
    "    # --- Helper functions ---\n",
    "    \n",
    "    def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "        ''' Calculates the competition score based on true values and predicted signals. '''\n",
    "        solution = y_true_df.to_pandas()\n",
    "        solution['position'] = y_pred_signals\n",
    "        solution['strategy_returns'] = (\n",
    "            solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "            solution['position'] * solution['forward_returns']\n",
    "        )\n",
    "        strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "        strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        strategy_std = solution['strategy_returns'].std()\n",
    "        if strategy_std == 0: return 0.0\n",
    "        \n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "        market_std = solution['forward_returns'].std()\n",
    "        market_volatility = market_std * np.sqrt(TRADING_DAYS_PER_YR) * 100\n",
    "        strategy_volatility = strategy_std * np.sqrt(TRADING_DAYS_PER_YR) * 100\n",
    "        excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "        vol_penalty = 1 + excess_vol\n",
    "        market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "        market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * TRADING_DAYS_PER_YR)\n",
    "        return_penalty = 1 + (return_gap**2) / 100\n",
    "        adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "        print(f\"Strategy Volatility: {strategy_volatility:.2f}%, Market Volatility: {market_volatility:.2f}%, Sharpe: {sharpe:.4f}, Adjusted Sharpe: {adjusted_sharpe:.4f}\")\n",
    "        return adjusted_sharpe\n",
    "\n",
    "    def convert_to_signal(predictions: np.ndarray, multiplier: float = 400.0) -> np.ndarray:\n",
    "        ''' Converts raw model predictions into trading signals in the range [0, 2]. '''\n",
    "        signals = predictions * multiplier + 1\n",
    "        return np.clip(signals, 0.0, 2.0)\n",
    "\n",
    "    def calculate_strategy_returns_pl(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> pl.Series:\n",
    "        \"\"\" Calculates strategy returns using Polars for history tracking. \"\"\"\n",
    "        signals_series = pl.Series(\"position\", y_pred_signals)\n",
    "        df_with_pos = y_true_df.with_columns(signals_series)\n",
    "        strategy_returns = (\n",
    "            df_with_pos['risk_free_rate'] * (1 - df_with_pos['position']) +\n",
    "            df_with_pos['position'] * df_with_pos['forward_returns']\n",
    "        )\n",
    "        return strategy_returns.alias(\"strategy_returns\")\n",
    "\n",
    "    def calculate_sharpe_for_leaderboard(hist_window: pl.DataFrame, strategy_col: str) -> float:\n",
    "        \"\"\" Calculates the geometric Sharpe ratio for a single strategy from the history. \"\"\"\n",
    "        if hist_window.height == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        strategy_returns = hist_window[strategy_col]\n",
    "        risk_free_rate = hist_window['risk_free_rate']\n",
    "        \n",
    "        # Calculate excess returns\n",
    "        strategy_excess_returns = (strategy_returns - risk_free_rate).to_numpy()\n",
    "        \n",
    "        # Handle potential NaNs or Infs\n",
    "        if not np.all(np.isfinite(strategy_excess_returns)):\n",
    "            return -np.inf # Penalize bad calculations\n",
    "\n",
    "        # Calculate geometric mean of (1 + excess_returns)\n",
    "        try:\n",
    "            log_returns = np.log1p(strategy_excess_returns)\n",
    "            strategy_geo_mean = np.exp(np.mean(log_returns)) - 1\n",
    "        except (ValueError, FloatingPointError):\n",
    "            return -np.inf # Bad strategy if 1+ret <= 0\n",
    "\n",
    "        # Std of *total* returns, as in original score\n",
    "        strategy_std = strategy_returns.std()\n",
    "        \n",
    "        if strategy_std is None or strategy_std == 0 or not np.isfinite(strategy_std):\n",
    "            return 0.0\n",
    "            \n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "        \n",
    "        return sharpe if np.isfinite(sharpe) else -np.inf\n",
    "\n",
    "    # --- Data Preparation (More efficient) ---\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    base_df = df.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    \n",
    "    # Handle 'E7' or other potential bad columns if necessary\n",
    "    if 'E7' in base_df.columns:\n",
    "        base_df = base_df.drop('E7')\n",
    "        \n",
    "    base_df = base_df.with_columns(pl.all().forward_fill())\n",
    "    print(f\"Base DataFrame shape after cleaning: {base_df.shape}\")\n",
    "\n",
    "    # --- Generate ALL features ONCE ---\n",
    "    print(f\"Generating features using {FEATURE_GENERATOR.__name__}...\")\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_features_df = FEATURE_GENERATOR(base_df)\n",
    "    \n",
    "    X_all_features = pl.concat([base_df.select(base_features), new_features_df], how=\"horizontal\")\n",
    "    print(f\"Full feature set shape: {X_all_features.shape}\")\n",
    "\n",
    "    TARGET_COL = \"target\"\n",
    "    y = base_df.select(TARGET_COL)\n",
    "    scorer_info_df = base_df.select([\"date_id\", \"forward_returns\", \"risk_free_rate\"])\n",
    "\n",
    "    # --- Time-Series Cross-Validation ---\n",
    "    print(f\"Starting ensemble CV with {N_STRATEGIES} data-subset strategies, following top {K_LEADERS} leaders.\")\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=nsplits)\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    # Master history of OOS performance for all N strategies\n",
    "    history_df = pl.DataFrame()\n",
    "    \n",
    "    # Store final ensembled signals and truths for overall score\n",
    "    overall_final_signals = []\n",
    "    overall_y_true = []\n",
    "    \n",
    "    # We need a reproducible random seed for sampling\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(base_df)):\n",
    "        print(f\"--- Starting Fold {i+1}/{nsplits} ---\")\n",
    "        \n",
    "        # Get the full train/test slices for this fold\n",
    "        X_train_fold_full = X_all_features[train_index]\n",
    "        y_train_fold_full = y[train_index]\n",
    "        X_test_fold = X_all_features[test_index]\n",
    "        \n",
    "        y_test_info = scorer_info_df[test_index]\n",
    "\n",
    "        # Store proposals (signals, returns) for all N models for *this* fold\n",
    "        current_fold_proposals_df = y_test_info.clone()\n",
    "        \n",
    "        # --- 1. Train N strategies on data subsets ---\n",
    "        n_samples = X_train_fold_full.height\n",
    "        subset_size = int(n_samples * SUBSAMPLE_RATIO)\n",
    "        \n",
    "        for j in range(N_STRATEGIES):\n",
    "            \n",
    "            # Create the random subset for strategy j\n",
    "            subset_fold_indices = rng.choice(n_samples, subset_size, replace=False)\n",
    "            \n",
    "            # Slice the training data for this strategy\n",
    "            X_train_j = X_train_fold_full[subset_fold_indices]\n",
    "            y_train_j = y_train_fold_full[subset_fold_indices]\n",
    "\n",
    "            # Define the model\n",
    "            model = xgb.XGBRegressor(\n",
    "                objective='reg:absoluteerror', n_estimators=20, device='cuda',\n",
    "                learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8,\n",
    "                n_jobs=-1, random_state=42 + j # Add random_state jitter for model\n",
    "            )\n",
    "            \n",
    "            # Train the model on the subset\n",
    "            model.fit(X_train_j, y_train_j, verbose=False)\n",
    "\n",
    "            # Get signals and returns for this strategy\n",
    "            # IMPORTANT: Predict on the *full* OOS test set\n",
    "            predictions = model.predict(X_test_fold) \n",
    "            signals = convert_to_signal(predictions)\n",
    "            returns = calculate_strategy_returns_pl(y_test_info, signals)\n",
    "            \n",
    "            # Store in the fold's proposal DataFrame\n",
    "            current_fold_proposals_df = current_fold_proposals_df.with_columns(\n",
    "                pl.Series(f'signals_{j}', signals),\n",
    "                returns.alias(f'returns_{j}')\n",
    "            )\n",
    "\n",
    "        # --- 2. Select Top k Leaders ---\n",
    "        # Look at the history *before* this fold\n",
    "        hist_window = history_df.tail(ROLLING_WINDOW_DAYS)\n",
    "        \n",
    "        if hist_window.height < MIN_HISTORY_DAYS:\n",
    "            # Not enough history, just average all strategies\n",
    "            print(f\"  Not enough history (< {MIN_HISTORY_DAYS} days), averaging all {N_STRATEGIES} strategies.\")\n",
    "            top_k_indices = list(range(N_STRATEGIES))\n",
    "        else:\n",
    "            # We have history, find the leaders\n",
    "            leaderboard = []\n",
    "            for j in range(N_STRATEGIES):\n",
    "                sharpe = calculate_sharpe_for_leaderboard(hist_window, f'returns_{j}')\n",
    "                leaderboard.append((sharpe, j))\n",
    "            \n",
    "            # Sort by Sharpe (descending)\n",
    "            leaderboard.sort(key=lambda x: x[0], reverse=True)\n",
    "            top_k_indices = [j for sharpe, j in leaderboard[:K_LEADERS]]\n",
    "            print(f\"  Top {K_LEADERS} leaders: {top_k_indices} (Sharpes: {[f'{s:.3f}' for s, j in leaderboard[:K_LEADERS]]})\")\n",
    "\n",
    "        # --- 3. Ensemble Positions ---\n",
    "        # Average the signals from the top k leaders\n",
    "        ensembled_signals = np.zeros(len(X_test_fold))\n",
    "        for idx in top_k_indices:\n",
    "            ensembled_signals += current_fold_proposals_df[f'signals_{idx}'].to_numpy()\n",
    "            \n",
    "        ensembled_signals /= len(top_k_indices)\n",
    "        \n",
    "        # --- 4. Score and Update History ---\n",
    "        print(\"  Ensemble Score for this fold:\")\n",
    "        score = calculate_competition_score(y_test_info, ensembled_signals)\n",
    "        cv_scores.append(score)\n",
    "        \n",
    "        # Store for overall score calculation\n",
    "        overall_y_true.append(y_test_info)\n",
    "        overall_final_signals.append(ensembled_signals)\n",
    "        \n",
    "        # Add this fold's results to the master history *after* scoring\n",
    "        history_df = pl.concat([history_df, current_fold_proposals_df])\n",
    "\n",
    "    # --- Final Evaluation ---\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    print(f\"\\n--- CV Finished ---\")\n",
    "    print(f\"Mean Ensembled CV Score: {mean_score:.4f}, std: {np.std(cv_scores):.4f}\")\n",
    "    \n",
    "    # Calculate overall score on all OOS predictions\n",
    "    overall_y_true_df = pl.concat(overall_y_true)\n",
    "    overall_final_signals_arr = np.concatenate(overall_final_signals)\n",
    "    \n",
    "    print(\"\\n--- Overall OOS Performance ---\")\n",
    "    overall_score = calculate_competition_score(overall_y_true_df, overall_final_signals_arr)\n",
    "    print(f\" Overall Ensembled Score: {overall_score:.4f}\")\n",
    "    \n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56459772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solve: daily reranking\n",
    "\n",
    "def solve(df: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Runs a time-series cross-validation process using a \"top-k leader\"\n",
    "    ensemble strategy with DAILY re-ranking.\n",
    "    \n",
    "    1. Trains N models at the start of each fold.\n",
    "    2. Iterates DAY-BY-DAY through the test set.\n",
    "    3. Each day, it finds the top k leaders based on the most recent \n",
    "       (e.g., 30-day) history.\n",
    "    4. It ensembles the positions from *only* those leaders for that day.\n",
    "    5. Appends the daily performance of ALL N strategies to the master \n",
    "       history to be used for the next day's ranking.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Ensemble Configuration ---\n",
    "    N_STRATEGIES = 10           # We'll create 10 strategies\n",
    "    SUBSAMPLE_RATIO = 0.5       # Each strategy sees 70% of the training data\n",
    "    FEATURE_GENERATOR = generate_features_7 # All strategies use this\n",
    "    \n",
    "    K_LEADERS = 3               # How many leaders to follow (e.g., 3)\n",
    "    ROLLING_WINDOW_DAYS = 40    # Lookback period to find leaders\n",
    "    MIN_HISTORY_DAYS = 15       # Min days needed to start picking leaders\n",
    "    TRADING_DAYS_PER_YR = 252\n",
    "\n",
    "    nsplits = 20\n",
    "\n",
    "    # --- Helper functions (unchanged) ---\n",
    "    \n",
    "    def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "        ''' Calculates the competition score based on true values and predicted signals. '''\n",
    "        solution = y_true_df.to_pandas()\n",
    "        solution['position'] = y_pred_signals\n",
    "        solution['strategy_returns'] = (\n",
    "            solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "            solution['position'] * solution['forward_returns']\n",
    "        )\n",
    "        strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "        strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        strategy_std = solution['strategy_returns'].std()\n",
    "        if strategy_std == 0: return 0.0\n",
    "        \n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "        market_std = solution['forward_returns'].std()\n",
    "        market_volatility = market_std * np.sqrt(TRADING_DAYS_PER_YR) * 100\n",
    "        strategy_volatility = strategy_std * np.sqrt(TRADING_DAYS_PER_YR) * 100\n",
    "        excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "        vol_penalty = 1 + excess_vol\n",
    "        market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "        market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * TRADING_DAYS_PER_YR)\n",
    "        return_penalty = 1 + (return_gap**2) / 100\n",
    "        adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "        print(f\"  Fold {i+1} (Daily): Strat Vol: {strategy_volatility:.2f}%, Mkt Vol: {market_volatility:.2f}%, Sharpe: {sharpe:.4f}, Adj Sharpe: {adjusted_sharpe:.4f}\")\n",
    "        return adjusted_sharpe\n",
    "\n",
    "    def convert_to_signal(predictions: np.ndarray, multiplier: float = 400.0) -> np.ndarray:\n",
    "        ''' Converts raw model predictions into trading signals in the range [0, 2]. '''\n",
    "        signals = predictions * multiplier + 0.9\n",
    "        signals = -np.abs(predictions-0.001) * 1000 + 3\n",
    "        return np.clip(signals, 0.0, 2.0)\n",
    "\n",
    "    def calculate_strategy_returns_pl(y_true_df_1_day: pl.DataFrame, y_pred_signal_1_day: np.ndarray) -> pl.Series:\n",
    "        \"\"\" Calculates strategy returns for a *single day*. \"\"\"\n",
    "        signals_series = pl.Series(\"position\", y_pred_signal_1_day)\n",
    "        df_with_pos = y_true_df_1_day.with_columns(signals_series)\n",
    "        strategy_returns = (\n",
    "            df_with_pos['risk_free_rate'] * (1 - df_with_pos['position']) +\n",
    "            df_with_pos['position'] * df_with_pos['forward_returns']\n",
    "        )\n",
    "        return strategy_returns.alias(\"strategy_returns\")\n",
    "\n",
    "    def calculate_sharpe_for_leaderboard(hist_window: pl.DataFrame, strategy_col: str) -> float:\n",
    "        \"\"\" Calculates the geometric Sharpe ratio for a single strategy from the history. \"\"\"\n",
    "        if hist_window.height == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        strategy_returns = hist_window[strategy_col]\n",
    "        risk_free_rate = hist_window['risk_free_rate']\n",
    "        \n",
    "        strategy_excess_returns = (strategy_returns - risk_free_rate).to_numpy()\n",
    "        \n",
    "        if not np.all(np.isfinite(strategy_excess_returns)):\n",
    "            print(f\"Bad data for {strategy_col} in leaderboard calculation.\")\n",
    "            return -np.inf\n",
    "\n",
    "        log_returns = np.log1p(strategy_excess_returns)\n",
    "        strategy_geo_mean = np.exp(np.mean(log_returns)) - 1\n",
    "\n",
    "        strategy_std = strategy_returns.std()\n",
    "        \n",
    "        if strategy_std is None or strategy_std == 0 or not np.isfinite(strategy_std):\n",
    "            print(f\"Bad std for {strategy_col} in leaderboard calculation.\")\n",
    "            return 0.0\n",
    "            \n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "        \n",
    "        return sharpe if np.isfinite(sharpe) else -np.inf\n",
    "\n",
    "    # --- Data Preparation (Efficient) ---\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    base_df = df.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    \n",
    "    if 'E7' in base_df.columns:\n",
    "        base_df = base_df.drop('E7')\n",
    "        \n",
    "    base_df = base_df.with_columns(pl.all().forward_fill())\n",
    "    print(f\"Base DataFrame shape after cleaning: {base_df.shape}\")\n",
    "\n",
    "    # --- Generate ALL features ONCE ---\n",
    "    print(f\"Generating features using {FEATURE_GENERATOR.__name__}...\")\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_features_df = FEATURE_GENERATOR(base_df)\n",
    "    \n",
    "    X_all_features = pl.concat([base_df.select(base_features), new_features_df], how=\"horizontal\")\n",
    "    print(f\"Full feature set shape: {X_all_features.shape}\")\n",
    "\n",
    "    TARGET_COL = \"target\"\n",
    "    y = base_df.select(TARGET_COL)\n",
    "    scorer_info_df = base_df.select([\"date_id\", \"forward_returns\", \"risk_free_rate\"])\n",
    "\n",
    "    # --- Time-Series Cross-Validation ---\n",
    "    print(f\"Starting DAILY re-ranking CV with {N_STRATEGIES} strategies, following top {K_LEADERS} leaders.\")\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=nsplits)\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    # Master history of OOS performance for all N strategies\n",
    "    # This will be updated DAY-BY-DAY\n",
    "    history_df = pl.DataFrame()\n",
    "    \n",
    "    # Store final ensembled signals and truths for overall score\n",
    "    overall_final_signals = []\n",
    "    overall_y_true = []\n",
    "    \n",
    "    rng = np.random.default_rng(seed=12)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(base_df)):\n",
    "        print(f\"--- Starting Fold {i+1}/{nsplits} (Days {test_index[0]} to {test_index[-1]}) ---\")\n",
    "        \n",
    "        # Get the full train/test slices for this fold\n",
    "        X_train_fold_full = X_all_features[train_index]\n",
    "        y_train_fold_full = y[train_index]\n",
    "        \n",
    "        X_test_fold = X_all_features[test_index]\n",
    "        y_test_info_fold = scorer_info_df[test_index] # All \"truth\" data for this fold\n",
    "\n",
    "        # --- 1. Train N Models (Once per fold) ---\n",
    "        # print(\"  Training N models...\")\n",
    "        N_signals_fold = [] # Will store signals for all N models\n",
    "        n_samples = X_train_fold_full.height\n",
    "        subset_size = int(n_samples * SUBSAMPLE_RATIO)\n",
    "        \n",
    "        for j in range(N_STRATEGIES):\n",
    "            subset_fold_indices = rng.choice(n_samples, subset_size, replace=False)\n",
    "            X_train_j = X_train_fold_full[subset_fold_indices]\n",
    "            y_train_j = y_train_fold_full[subset_fold_indices]\n",
    "\n",
    "            model = xgb.XGBRegressor(\n",
    "                objective='reg:absoluteerror', n_estimators=20, device='cuda',\n",
    "                learning_rate=0.05, max_depth=4, subsample=0.8, colsample_bytree=0.8,\n",
    "                n_jobs=-1, random_state=42 + j \n",
    "            )\n",
    "            \n",
    "            model.fit(X_train_j, y_train_j, verbose=False)\n",
    "\n",
    "            # Predict on the *entire* test fold and store signals\n",
    "            predictions = model.predict(X_test_fold) \n",
    "            signals = convert_to_signal(predictions)\n",
    "            N_signals_fold.append(signals)\n",
    "            \n",
    "        # Stack into a [days, strategies] numpy array\n",
    "        N_signals_fold = np.stack(N_signals_fold, axis=1) \n",
    "        \n",
    "        # --- 2. Iterate Day-By-Day through the test fold ---\n",
    "        print(f\"  Iterating daily and re-ranking leaders...\")\n",
    "        \n",
    "        # Store the *final* ensembled signals for this fold\n",
    "        fold_ensembled_signals = [] \n",
    "\n",
    "        for day_idx in range(len(test_index)):\n",
    "            \n",
    "            # --- 2a. Find Top k Leaders (for today) ---\n",
    "            # Look at history *up to this point*\n",
    "            hist_window = history_df.tail(ROLLING_WINDOW_DAYS)\n",
    "            \n",
    "            if hist_window.height < MIN_HISTORY_DAYS:\n",
    "                # Not enough history, average all strategies\n",
    "                top_k_indices = list(range(N_STRATEGIES))\n",
    "                if day_idx % 50 == 0: # Print status periodically\n",
    "                   print(f\"    Day {day_idx}: Not enough history, averaging all {N_STRATEGIES}.\")\n",
    "            else:\n",
    "                # We have history, find the leaders\n",
    "                leaderboard = []\n",
    "                for j in range(N_STRATEGIES):\n",
    "                    sharpe = calculate_sharpe_for_leaderboard(hist_window, f'returns_{j}')\n",
    "                    leaderboard.append((sharpe, j))\n",
    "                \n",
    "                leaderboard.sort(key=lambda x: x[0], reverse=True)\n",
    "                top_k_indices = [j for sharpe, j in leaderboard[:K_LEADERS]]\n",
    "                if day_idx % 60 == 0: # Print status periodically\n",
    "                    print(f\"    Day {day_idx}: Top {K_LEADERS} leaders: {top_k_indices}, Sharpes: {[f'{s:.3f}' for s, j in leaderboard[:K_LEADERS]]}\")\n",
    "\n",
    "            # --- 2b. Ensemble Position (for today) ---\n",
    "            # Get the pre-calculated signals for *all N* strategies *for today*\n",
    "            today_N_signals = N_signals_fold[day_idx, :]\n",
    "            \n",
    "            # Get signals from *only* the leaders\n",
    "            signals_from_leaders = today_N_signals[top_k_indices]\n",
    "            \n",
    "            # Average them to get the final position\n",
    "            final_signal_today = np.mean(signals_from_leaders)\n",
    "            fold_ensembled_signals.append(final_signal_today)\n",
    "\n",
    "            # --- 2c. Update Master History (for tomorrow's ranking) ---\n",
    "            # Get today's \"truth\" data (1-row DataFrame)\n",
    "            today_test_info = y_test_info_fold[day_idx]\n",
    "            \n",
    "            # Calculate the returns for *all N* individual strategies for today\n",
    "            today_returns_N = []\n",
    "            for j in range(N_STRATEGIES):\n",
    "                signal_j_today = np.array([today_N_signals[j]])\n",
    "                return_j_series = calculate_strategy_returns_pl(today_test_info, signal_j_today)\n",
    "                today_returns_N.append(return_j_series[0]) # Get the single float value\n",
    "            \n",
    "            # Build the new history row\n",
    "            new_history_row_data = {\n",
    "                \"date_id\": today_test_info[\"date_id\"][0],\n",
    "                \"forward_returns\": today_test_info[\"forward_returns\"][0],\n",
    "                \"risk_free_rate\": today_test_info[\"risk_free_rate\"][0],\n",
    "                **{f\"returns_{j}\": ret for j, ret in enumerate(today_returns_N)}\n",
    "            }\n",
    "            new_history_row_df = pl.DataFrame(new_history_row_data)\n",
    "\n",
    "            # Append to the master history_df\n",
    "            history_df = pl.concat([history_df, new_history_row_df])\n",
    "\n",
    "        # --- 3. Score Fold (after iterating all days) ---\n",
    "        print(\"  Fold iteration complete. Scoring fold...\")\n",
    "        fold_final_signals_arr = np.array(fold_ensembled_signals)\n",
    "        score = calculate_competition_score(y_test_info_fold, fold_final_signals_arr)\n",
    "        cv_scores.append(score)\n",
    "        \n",
    "        # Store for overall score calculation\n",
    "        overall_final_signals.append(fold_final_signals_arr)\n",
    "        overall_y_true.append(y_test_info_fold)\n",
    "\n",
    "    # --- 4. Final Evaluation ---\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    print(f\"\\n--- CV Finished ---\")\n",
    "    print(f\"Mean Ensembled CV Score (Daily Re-ranking): {mean_score:.4f}, std: {np.std(cv_scores):.4f}\")\n",
    "    \n",
    "    overall_y_true_df = pl.concat(overall_y_true)\n",
    "    overall_final_signals_arr = np.concatenate(overall_final_signals)\n",
    "    \n",
    "    print(\"\\n--- Overall OOS Performance (Daily Re-ranking) ---\")\n",
    "    # This score is the most important one\n",
    "    overall_score = calculate_competition_score(overall_y_true_df, overall_final_signals_arr)\n",
    "    print(f\" Overall Ensembled Score: {overall_score:.4f}\")\n",
    "    \n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d9829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4990, 98)\n",
      "\n",
      "--- Joining weekday feature onto sliced data ---\n",
      "Initial DataFrame shape: (4990, 99)\n",
      "Base DataFrame shape after cleaning: (4990, 98)\n",
      "Generating features using generate_features_7...\n",
      "Full feature set shape: (4990, 144)\n",
      "Starting DAILY re-ranking CV with 10 strategies, following top 3 leaders.\n",
      "--- Starting Fold 1/20 (Days 250 to 486) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10323/540012060.py:17: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.selectors.numeric().rolling_mean(window_size=5, min_periods=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Not enough history, averaging all 10.\n",
      "    Day 60: Top 3 leaders: [0, 2, 7], Sharpes: ['2.969', '2.174', '2.112']\n",
      "    Day 120: Top 3 leaders: [2, 9, 7], Sharpes: ['5.685', '4.512', '4.479']\n",
      "    Day 180: Top 3 leaders: [2, 1, 8], Sharpes: ['-0.598', '-1.340', '-1.623']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 1 (Daily): Strat Vol: 22.90%, Mkt Vol: 13.21%, Sharpe: 0.2804, Adj Sharpe: 0.1741\n",
      "--- Starting Fold 2/20 (Days 487 to 723) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [2, 1, 9], Sharpes: ['2.736', '2.215', '1.942']\n",
      "    Day 60: Top 3 leaders: [9, 2, 7], Sharpes: ['-0.137', '-0.660', '-0.818']\n",
      "    Day 120: Top 3 leaders: [8, 9, 4], Sharpes: ['-0.229', '-0.253', '-0.388']\n",
      "    Day 180: Top 3 leaders: [6, 7, 9], Sharpes: ['-1.208', '-2.823', '-2.894']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 2 (Daily): Strat Vol: 36.92%, Mkt Vol: 21.92%, Sharpe: -1.0904, Adj Sharpe: -0.1992\n",
      "--- Starting Fold 3/20 (Days 724 to 960) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [3, 2, 4], Sharpes: ['-0.539', '-0.553', '-0.595']\n",
      "    Day 60: Top 3 leaders: [0, 2, 9], Sharpes: ['1.017', '0.915', '0.906']\n",
      "    Day 120: Top 3 leaders: [7, 2, 0], Sharpes: ['0.621', '0.035', '-0.035']\n",
      "    Day 180: Top 3 leaders: [0, 3, 5], Sharpes: ['3.797', '3.386', '3.182']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 3 (Daily): Strat Vol: 34.04%, Mkt Vol: 33.66%, Sharpe: -0.4342, Adj Sharpe: -0.2896\n",
      "--- Starting Fold 4/20 (Days 961 to 1197) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [2, 8, 9], Sharpes: ['5.767', '5.676', '4.891']\n",
      "    Day 60: Top 3 leaders: [0, 8, 2], Sharpes: ['2.754', '2.583', '2.569']\n",
      "    Day 120: Top 3 leaders: [0, 4, 3], Sharpes: ['0.671', '0.238', '0.073']\n",
      "    Day 180: Top 3 leaders: [2, 4, 3], Sharpes: ['-1.815', '-2.012', '-2.027']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 4 (Daily): Strat Vol: 27.33%, Mkt Vol: 18.61%, Sharpe: 0.3961, Adj Sharpe: 0.3122\n",
      "--- Starting Fold 5/20 (Days 1198 to 1434) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [2, 5, 3], Sharpes: ['0.810', '-0.453', '-0.494']\n",
      "    Day 60: Top 3 leaders: [5, 4, 6], Sharpes: ['4.647', '4.440', '4.377']\n",
      "    Day 120: Top 3 leaders: [2, 6, 9], Sharpes: ['7.911', '5.763', '5.580']\n",
      "    Day 180: Top 3 leaders: [2, 9, 3], Sharpes: ['4.378', '1.954', '1.842']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 5 (Daily): Strat Vol: 22.38%, Mkt Vol: 13.14%, Sharpe: 1.7136, Adj Sharpe: 1.1395\n",
      "--- Starting Fold 6/20 (Days 1435 to 1671) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [2, 5, 6], Sharpes: ['0.913', '0.779', '0.666']\n",
      "    Day 60: Top 3 leaders: [6, 7, 8], Sharpes: ['1.848', '1.792', '1.739']\n",
      "    Day 120: Top 3 leaders: [1, 8, 6], Sharpes: ['1.258', '0.880', '0.794']\n",
      "    Day 180: Top 3 leaders: [8, 0, 5], Sharpes: ['1.865', '0.943', '0.852']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 6 (Daily): Strat Vol: 41.53%, Mkt Vol: 22.43%, Sharpe: 0.1709, Adj Sharpe: 0.1028\n",
      "--- Starting Fold 7/20 (Days 1672 to 1908) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [8, 3, 7], Sharpes: ['0.588', '0.059', '-0.040']\n",
      "    Day 60: Top 3 leaders: [2, 6, 4], Sharpes: ['3.291', '3.254', '2.806']\n",
      "    Day 120: Top 3 leaders: [3, 9, 0], Sharpes: ['2.922', '1.935', '1.377']\n",
      "    Day 180: Top 3 leaders: [3, 8, 0], Sharpes: ['5.725', '3.637', '3.615']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 7 (Daily): Strat Vol: 15.15%, Mkt Vol: 11.85%, Sharpe: 1.6704, Adj Sharpe: 1.5490\n",
      "--- Starting Fold 8/20 (Days 1909 to 2145) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [7, 9, 2], Sharpes: ['4.876', '3.562', '3.362']\n",
      "    Day 60: Top 3 leaders: [9, 7, 2], Sharpes: ['0.608', '0.253', '0.248']\n",
      "    Day 120: Top 3 leaders: [5, 9, 7], Sharpes: ['4.729', '4.279', '4.267']\n",
      "    Day 180: Top 3 leaders: [5, 0, 4], Sharpes: ['0.929', '0.532', '0.491']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 8 (Daily): Strat Vol: 19.25%, Mkt Vol: 10.99%, Sharpe: 1.3886, Adj Sharpe: 0.8951\n",
      "--- Starting Fold 9/20 (Days 2146 to 2382) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [5, 3, 8], Sharpes: ['1.968', '1.799', '1.659']\n",
      "    Day 60: Top 3 leaders: [5, 4, 3], Sharpes: ['0.980', '0.871', '0.729']\n",
      "    Day 120: Top 3 leaders: [8, 4, 1], Sharpes: ['1.271', '0.945', '0.893']\n",
      "    Day 180: Top 3 leaders: [0, 8, 2], Sharpes: ['0.760', '0.611', '0.533']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 9 (Daily): Strat Vol: 21.71%, Mkt Vol: 11.69%, Sharpe: 1.0266, Adj Sharpe: 0.6194\n",
      "--- Starting Fold 10/20 (Days 2383 to 2619) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [2, 7, 9], Sharpes: ['0.995', '0.908', '0.846']\n",
      "    Day 60: Top 3 leaders: [2, 1, 3], Sharpes: ['0.067', '-0.128', '-0.217']\n",
      "    Day 120: Top 3 leaders: [1, 2, 7], Sharpes: ['3.341', '2.591', '2.246']\n",
      "    Day 180: Top 3 leaders: [3, 7, 1], Sharpes: ['-3.152', '-3.188', '-3.329']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 10 (Daily): Strat Vol: 32.08%, Mkt Vol: 16.89%, Sharpe: -0.0607, Adj Sharpe: -0.0354\n",
      "--- Starting Fold 11/20 (Days 2620 to 2856) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [1, 7, 5], Sharpes: ['5.329', '5.167', '5.156']\n",
      "    Day 60: Top 3 leaders: [6, 2, 4], Sharpes: ['1.555', '1.341', '1.160']\n",
      "    Day 120: Top 3 leaders: [6, 3, 9], Sharpes: ['0.523', '0.466', '0.070']\n",
      "    Day 180: Top 3 leaders: [0, 4, 2], Sharpes: ['4.634', '4.457', '4.405']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 11 (Daily): Strat Vol: 18.39%, Mkt Vol: 9.95%, Sharpe: 1.7805, Adj Sharpe: 1.0808\n",
      "--- Starting Fold 12/20 (Days 2857 to 3093) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [0, 9, 3], Sharpes: ['4.757', '4.470', '4.438']\n",
      "    Day 60: Top 3 leaders: [9, 4, 1], Sharpes: ['3.869', '3.769', '3.734']\n",
      "    Day 120: Top 3 leaders: [2, 7, 0], Sharpes: ['1.414', '0.907', '0.896']\n",
      "    Day 180: Top 3 leaders: [0, 9, 6], Sharpes: ['4.623', '4.151', '4.111']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 12 (Daily): Strat Vol: 19.32%, Mkt Vol: 10.09%, Sharpe: 1.4840, Adj Sharpe: 0.8651\n",
      "--- Starting Fold 13/20 (Days 3094 to 3330) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [9, 7, 4], Sharpes: ['0.972', '0.830', '0.719']\n",
      "    Day 60: Top 3 leaders: [3, 6, 4], Sharpes: ['2.728', '2.701', '2.096']\n",
      "    Day 120: Top 3 leaders: [1, 6, 0], Sharpes: ['3.334', '2.763', '2.701']\n",
      "    Day 180: Top 3 leaders: [4, 1, 9], Sharpes: ['-1.908', '-2.008', '-2.017']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 13 (Daily): Strat Vol: 29.75%, Mkt Vol: 16.21%, Sharpe: -0.1514, Adj Sharpe: -0.0864\n",
      "--- Starting Fold 14/20 (Days 3331 to 3567) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [2, 0, 3], Sharpes: ['2.358', '1.439', '1.281']\n",
      "    Day 60: Top 3 leaders: [6, 9, 4], Sharpes: ['4.788', '4.512', '4.508']\n",
      "    Day 120: Top 3 leaders: [1, 0, 8], Sharpes: ['6.321', '6.299', '6.269']\n",
      "    Day 180: Top 3 leaders: [0, 2, 8], Sharpes: ['2.600', '2.501', '2.498']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 14 (Daily): Strat Vol: 22.17%, Mkt Vol: 11.79%, Sharpe: 1.4889, Adj Sharpe: 0.8858\n",
      "--- Starting Fold 15/20 (Days 3568 to 3804) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [1, 5, 7], Sharpes: ['4.986', '4.915', '4.703']\n",
      "    Day 60: Top 3 leaders: [7, 3, 8], Sharpes: ['-1.454', '-1.649', '-1.941']\n",
      "    Day 120: Top 3 leaders: [8, 4, 7], Sharpes: ['2.700', '2.696', '2.696']\n",
      "    Day 180: Top 3 leaders: [7, 1, 5], Sharpes: ['0.858', '0.711', '0.672']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 15 (Daily): Strat Vol: 47.01%, Mkt Vol: 26.01%, Sharpe: 0.6864, Adj Sharpe: 0.4271\n",
      "--- Starting Fold 16/20 (Days 3805 to 4041) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [3, 4, 2], Sharpes: ['3.107', '3.100', '3.068']\n",
      "    Day 60: Top 3 leaders: [6, 2, 8], Sharpes: ['1.389', '1.163', '1.161']\n",
      "    Day 120: Top 3 leaders: [6, 2, 7], Sharpes: ['1.751', '1.514', '1.285']\n",
      "    Day 180: Top 3 leaders: [8, 5, 0], Sharpes: ['1.914', '1.818', '1.668']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 16 (Daily): Strat Vol: 22.75%, Mkt Vol: 12.50%, Sharpe: 1.5745, Adj Sharpe: 0.9717\n",
      "--- Starting Fold 17/20 (Days 4042 to 4278) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [2, 6, 0], Sharpes: ['2.236', '2.216', '2.157']\n",
      "    Day 60: Top 3 leaders: [5, 6, 7], Sharpes: ['-2.664', '-2.784', '-2.819']\n",
      "    Day 120: Top 3 leaders: [0, 6, 9], Sharpes: ['-3.399', '-3.505', '-3.524']\n",
      "    Day 180: Top 3 leaders: [9, 8, 1], Sharpes: ['3.836', '3.603', '3.484']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 17 (Daily): Strat Vol: 45.18%, Mkt Vol: 23.74%, Sharpe: -1.1326, Adj Sharpe: -0.0589\n",
      "--- Starting Fold 18/20 (Days 4279 to 4515) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [5, 4, 3], Sharpes: ['-1.151', '-1.304', '-1.344']\n",
      "    Day 60: Top 3 leaders: [3, 7, 1], Sharpes: ['1.857', '1.749', '1.636']\n",
      "    Day 120: Top 3 leaders: [5, 6, 2], Sharpes: ['0.450', '0.410', '0.406']\n",
      "    Day 180: Top 3 leaders: [7, 1, 5], Sharpes: ['4.405', '4.346', '4.327']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 18 (Daily): Strat Vol: 29.16%, Mkt Vol: 14.79%, Sharpe: 0.4678, Adj Sharpe: 0.2642\n",
      "--- Starting Fold 19/20 (Days 4516 to 4752) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [5, 2, 4], Sharpes: ['-2.166', '-2.172', '-2.185']\n",
      "    Day 60: Top 3 leaders: [7, 9, 1], Sharpes: ['4.057', '3.762', '3.696']\n",
      "    Day 120: Top 3 leaders: [1, 7, 3], Sharpes: ['0.522', '-0.010', '-0.049']\n",
      "    Day 180: Top 3 leaders: [2, 3, 4], Sharpes: ['5.282', '5.044', '5.016']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 19 (Daily): Strat Vol: 23.23%, Mkt Vol: 12.39%, Sharpe: 2.2855, Adj Sharpe: 1.3648\n",
      "--- Starting Fold 20/20 (Days 4753 to 4989) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 3 leaders: [1, 3, 8], Sharpes: ['4.220', '4.075', '4.071']\n",
      "    Day 60: Top 3 leaders: [6, 3, 1], Sharpes: ['1.415', '1.270', '1.270']\n",
      "    Day 120: Top 3 leaders: [6, 9, 0], Sharpes: ['-2.512', '-2.521', '-2.526']\n",
      "    Day 180: Top 3 leaders: [2, 3, 5], Sharpes: ['4.016', '4.016', '4.016']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 20 (Daily): Strat Vol: 31.94%, Mkt Vol: 16.28%, Sharpe: 0.5339, Adj Sharpe: 0.3031\n",
      "\n",
      "--- CV Finished ---\n",
      "Mean Ensembled CV Score (Daily Re-ranking): 0.5143, std: 0.5348\n",
      "\n",
      "--- Overall OOS Performance (Daily Re-ranking) ---\n",
      "  Fold 20 (Daily): Strat Vol: 29.46%, Mkt Vol: 17.49%, Sharpe: 0.4589, Adj Sharpe: 0.3091\n",
      " Overall Ensembled Score: 0.3091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.30907668470960253"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def evaluate(excessarg: int) -> float:\n",
    "    \"\"\"\n",
    "    Main evaluation function for FunSearch. It loads the data\n",
    "    and runs the solver which performs cross-validation.\n",
    "    \"\"\"\n",
    "    full_train_df = pl.read_csv(TRAIN_DATA_PATH)\n",
    "    # Use a slice of data for faster evaluation runs during development\n",
    "    df_raw = full_train_df.slice(4000)\n",
    "    print(df_raw.shape)\n",
    "\n",
    "    #fill nulls in df with mean\n",
    "    df = df_raw.with_columns(\n",
    "        # Select all numeric columns for the operation\n",
    "        pl.selectors.numeric()\n",
    "          # Step 1: Attempt to fill with the rolling mean of each respective column\n",
    "          .fill_null(\n",
    "              pl.selectors.numeric().rolling_mean(window_size=5, min_periods=1)\n",
    "          )\n",
    "          # Step 2: Fall back to the global column mean for any remaining nulls\n",
    "          #.fill_null(strategy='mean')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "    pl.col(\"date_id\").cast(pl.Int64)\n",
    "    )\n",
    "    \n",
    "    weekday_df = add_weekday_column(SPY_DATA_PATH)\n",
    "    print(\"\\n--- Joining weekday feature onto sliced data ---\")\n",
    "    # Join the weekday information onto the sliced training data.\n",
    "    # A 'left' join ensures we keep all rows from the original `df`.\n",
    "    df_with_features = df.join(weekday_df, on=\"date_id\", how=\"left\")\n",
    "    # print(\"DataFrame after join:\")\n",
    "    # print(df_with_features.shape)\n",
    "    return solve(df_with_features)\n",
    "  \n",
    "def add_weekday_column(input_csv_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file, adds a 'weekday' column based on the 'Date' column,\n",
    "    and saves the result to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): The path to the source CSV file.\n",
    "        output_csv_path (str): The path where the output CSV will be saved.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a Polars DataFrame\n",
    "    df = pl.read_csv(input_csv_path)\n",
    "\n",
    "    # Add a new column named 'weekday'\n",
    "    # 1. Select the 'Date' column.\n",
    "    # 2. Convert the string representation to a proper date type.\n",
    "    # 3. Use the .dt.weekday() function to get the day of the week (Monday=1, Sunday=7).\n",
    "    # 4. Alias the new expression to 'weekday'.\n",
    "    df_with_weekday = df.with_columns(\n",
    "        pl.col(\"Date\").str.to_date().dt.weekday().alias(\"weekday\")\n",
    "    )\n",
    "\n",
    "    # Print the transformed DataFrame to the console to show the result\n",
    "    returned_df = df_with_weekday.select([\"date_id\", \"weekday\"])\n",
    "    return returned_df\n",
    "\n",
    "evaluate(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
