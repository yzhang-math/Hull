{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0959d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# The training data path should be updated to your actual training file.\n",
    "TRAIN_DATA_PATH = \"./kaggle/train.csv\"\n",
    "SPY_DATA_PATH = \"./kaggle/spy-historical.csv\"\n",
    "\n",
    "def generate_features_7 (df: pl.DataFrame) -> pl.DataFrame:\n",
    "  \"\"\"Generates new features from the base data.\n",
    "    This function is the target of the evolutionary algorithm.\n",
    "  \n",
    "    Available Feature Categories:\n",
    "    - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "    - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "    - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "    - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "    - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "    - S* (Sentiment features): 12 columns (S1-S12)\n",
    "    - V* (Volatility features): 13 columns (V1-V13)\n",
    "  \"\"\"\n",
    "  new_features = pl.DataFrame({\n",
    "      # --- 20 Pairwise Interactions ---\n",
    "      'feat_M1_x_V1': df['M1'] * df['V1'],\n",
    "      'feat_P1_add_E1': df['P1'] + df['E1'],\n",
    "      'feat_S1_sub_I1': df['S1'] - df['I1'],\n",
    "      'feat_M10_div_V10': df['M10'] / (df['V10'] + 1e-6),\n",
    "      'feat_P10_x_E10': df['P10'] * df['E10'],\n",
    "      'feat_M2_x_S3': df['M2'] * df['S3'],\n",
    "      'feat_V2_div_P2': df['V2'] / (df['P2'] + 1e-6),\n",
    "      'feat_E4_sub_I3': df['E4'] - df['I3'],\n",
    "      'feat_S7_add_M12': df['S7'] + df['M12'],\n",
    "      'feat_I5_x_V11': df['I5'] * df['V11'],\n",
    "      'feat_P5_div_S8': df['P5'] / (df['S8'] + 1e-6),\n",
    "      'feat_E12_x_I9': df['E12'] * df['I9'],\n",
    "      'feat_M1_div_S1': df['M1'] / (df['S1'] + 1e-6),\n",
    "      'feat_V1_add_P1': df['V1'] + df['P1'],\n",
    "      'feat_E1_sub_I1': df['E1'] - df['I1'],\n",
    "      'feat_M2_div_V2': df['M2'] / (df['V2'] + 1e-6),\n",
    "      'feat_P2_x_S3': df['P2'] * df['S3'],\n",
    "      'feat_E4_add_M10': df['E4'] + df['M10'],\n",
    "      'feat_I3_sub_V10': df['I3'] - df['V10'],\n",
    "      'feat_S7_x_P10': df['S7'] * df['P10'],\n",
    "      # --- 10 Rolling Window Features ---\n",
    "      'feat_V2_roll_mean_5': df['V2'].rolling_mean(window_size=5),\n",
    "      'feat_V1_roll_std_5': df['V1'].rolling_std(window_size=5),\n",
    "      'feat_M1_roll_mean_20': df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_M3_roll_std_20': df['M3'].rolling_std(window_size=20),\n",
    "      'feat_P1_roll_max_10': df['P1'].rolling_max(window_size=10),\n",
    "      'feat_P1_roll_min_10': df['P1'].rolling_min(window_size=10),\n",
    "      'feat_E5_roll_mean_50': df['E5'].rolling_mean(window_size=50),\n",
    "      'feat_S1_roll_std_50': df['S1'].rolling_std(window_size=50),\n",
    "      'feat_I1_roll_mean_10': df['I1'].rolling_mean(window_size=10),\n",
    "      'feat_V10_roll_std_10': df['V10'].rolling_std(window_size=10),\n",
    "      # --- 10 Complex Interactions (3+ elements) ---\n",
    "      'feat_M1_V1_div_P1': (df['M1'] * df['V1']) / (df['P1'] + 1e-6),\n",
    "      'feat_E1_S1_add_I1': df['E1'] + df['S1'] - df['I1'],\n",
    "      'feat_M2_P2_sub_V2': df['M2'] + df['P2'] - df['V2'],\n",
    "      'feat_S7_div_E4_I3': df['S7'] / (df['E4'] + df['I3'] + 1e-6),\n",
    "      'feat_P5_x_M10_x_V10': df['P5'] * df['M10'] * df['V10'],\n",
    "      'feat_roll_diff_M1_5_20': df['M1'].rolling_mean(window_size=5) - df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_roll_diff_V1_5_20': df['V1'].rolling_mean(window_size=5) - df['V1'].rolling_mean(window_size=20),\n",
    "      'feat_M_S_P_combo': (df['M12'] - df['M1']) / (df['S1'] + df['P1'] + 1e-6),\n",
    "      'feat_V_E_I_combo': (df['V11'] + df['V2']) * (df['E1'] - df['I1']),\n",
    "      'feat_ratio_of_ratios': (df['M1']/(df['V1']+1e-6)) / (df['P1']/(df['S1']+1e-6)),\n",
    "      # --- 10 New Features ---\n",
    "      'feat_M1_x_V1_x_P1': df['M1'] * df['V1'] * df['P1'],\n",
    "      'feat_E1_div_S1': df['E1'] / (df['S1'] + 1e-6),\n",
    "      'feat_I1_sub_V1': df['I1'] - df['V1'],\n",
    "      'feat_M10_add_V10': df['M10'] + df['V10'],\n",
    "      'feat_P10_div_E10': df['P10'] / (df['E10'] + 1e-6),\n",
    "      'feat_M2_add_S3': df['M2'] + df['S3'],\n",
    "      'feat_V2_x_P2': df['V2'] * df['P2'],\n",
    "      'feat_E4_add_I3': df['E4'] + df['I3'],\n",
    "      'feat_S7_div_M12': df['S7'] / (df['M12'] + 1e-6),\n",
    "      'feat_I5_div_V11': df['I5'] / (df['V11'] + 1e-6),\n",
    "      #'feat_M1_log_P1': np.log(df['M1'] + 1e-6) / np.log(df['P1'] + 1e-6),\n",
    "      # --- SAFER LOGIC HERE ---\n",
    "      #'feat_M1_log_P1': pl.when( (df['M1'] > 0) & (df['P1'] > 0) & (df['P1'] != 1) ).then( df['M1'].log() / df['P1'].log() ).otherwise(0),\n",
    "      # --- END SAFER LOGIC ---\n",
    "  })\n",
    "  # Fill any nulls created by rolling windows\n",
    "  return new_features.with_columns(pl.all().forward_fill())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "83e335f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solve: no reranking\n",
    "\n",
    "# --- Make sure all your feature generators are defined above this ---\n",
    "# (We will only use generate_features_7, but it must be defined)\n",
    "# e.g., generate_features_1, ... generate_features_7\n",
    "\n",
    "def solve(df: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Runs a time-series cross-validation process using a \"top-k leader\"\n",
    "    ensemble strategy.\n",
    "    \n",
    "    All N strategies use the *same* feature set (gen_features_7)\n",
    "    but are trained on *random subsets* of the training data to \n",
    "    create \"orthogonal\" models.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Ensemble Configuration ---\n",
    "    N_STRATEGIES = 10           # We'll create 10 strategies\n",
    "    SUBSAMPLE_RATIO = 0.4       # Each strategy sees 70% of the training data\n",
    "    FEATURE_GENERATOR = generate_features_7 # All strategies use this\n",
    "    \n",
    "    K_LEADERS = 3               # How many leaders to follow (e.g., 3)\n",
    "    ROLLING_WINDOW_DAYS = 30    # Lookback period to find leaders\n",
    "    MIN_HISTORY_DAYS = 15       # Min days needed to start picking leaders\n",
    "    TRADING_DAYS_PER_YR = 252\n",
    "    nsplits = 20\n",
    "\n",
    "    # --- Helper functions ---\n",
    "    \n",
    "    def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "        ''' Calculates the competition score based on true values and predicted signals. '''\n",
    "        solution = y_true_df.to_pandas()\n",
    "        solution['position'] = y_pred_signals\n",
    "        solution['strategy_returns'] = (\n",
    "            solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "            solution['position'] * solution['forward_returns']\n",
    "        )\n",
    "        strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "        strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        strategy_std = solution['strategy_returns'].std()\n",
    "        if strategy_std == 0: return 0.0\n",
    "        \n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "        market_std = solution['forward_returns'].std()\n",
    "        market_volatility = market_std * np.sqrt(TRADING_DAYS_PER_YR) * 100\n",
    "        strategy_volatility = strategy_std * np.sqrt(TRADING_DAYS_PER_YR) * 100\n",
    "        excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "        vol_penalty = 1 + excess_vol\n",
    "        market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "        market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * TRADING_DAYS_PER_YR)\n",
    "        return_penalty = 1 + (return_gap**2) / 100\n",
    "        adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "        print(f\"Strategy Volatility: {strategy_volatility:.2f}%, Market Volatility: {market_volatility:.2f}%, Sharpe: {sharpe:.4f}, Adjusted Sharpe: {adjusted_sharpe:.4f}\")\n",
    "        return adjusted_sharpe\n",
    "\n",
    "    def convert_to_signal(predictions: np.ndarray, multiplier: float = 400.0) -> np.ndarray:\n",
    "        ''' Converts raw model predictions into trading signals in the range [0, 2]. '''\n",
    "        signals = predictions * multiplier + 1\n",
    "        return np.clip(signals, 0.0, 2.0)\n",
    "\n",
    "    def calculate_strategy_returns_pl(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> pl.Series:\n",
    "        \"\"\" Calculates strategy returns using Polars for history tracking. \"\"\"\n",
    "        signals_series = pl.Series(\"position\", y_pred_signals)\n",
    "        df_with_pos = y_true_df.with_columns(signals_series)\n",
    "        strategy_returns = (\n",
    "            df_with_pos['risk_free_rate'] * (1 - df_with_pos['position']) +\n",
    "            df_with_pos['position'] * df_with_pos['forward_returns']\n",
    "        )\n",
    "        return strategy_returns.alias(\"strategy_returns\")\n",
    "\n",
    "    def calculate_sharpe_for_leaderboard(hist_window: pl.DataFrame, strategy_col: str) -> float:\n",
    "        \"\"\" Calculates the geometric Sharpe ratio for a single strategy from the history. \"\"\"\n",
    "        if hist_window.height == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        strategy_returns = hist_window[strategy_col]\n",
    "        risk_free_rate = hist_window['risk_free_rate']\n",
    "        \n",
    "        # Calculate excess returns\n",
    "        strategy_excess_returns = (strategy_returns - risk_free_rate).to_numpy()\n",
    "        \n",
    "        # Handle potential NaNs or Infs\n",
    "        if not np.all(np.isfinite(strategy_excess_returns)):\n",
    "            return -np.inf # Penalize bad calculations\n",
    "\n",
    "        # Calculate geometric mean of (1 + excess_returns)\n",
    "        try:\n",
    "            log_returns = np.log1p(strategy_excess_returns)\n",
    "            strategy_geo_mean = np.exp(np.mean(log_returns)) - 1\n",
    "        except (ValueError, FloatingPointError):\n",
    "            return -np.inf # Bad strategy if 1+ret <= 0\n",
    "\n",
    "        # Std of *total* returns, as in original score\n",
    "        strategy_std = strategy_returns.std()\n",
    "        \n",
    "        if strategy_std is None or strategy_std == 0 or not np.isfinite(strategy_std):\n",
    "            return 0.0\n",
    "            \n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "        \n",
    "        return sharpe if np.isfinite(sharpe) else -np.inf\n",
    "\n",
    "    # --- Data Preparation (More efficient) ---\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    base_df = df.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    \n",
    "    # Handle 'E7' or other potential bad columns if necessary\n",
    "    if 'E7' in base_df.columns:\n",
    "        base_df = base_df.drop('E7')\n",
    "        \n",
    "    base_df = base_df.with_columns(pl.all().forward_fill())\n",
    "    print(f\"Base DataFrame shape after cleaning: {base_df.shape}\")\n",
    "\n",
    "    # --- Generate ALL features ONCE ---\n",
    "    print(f\"Generating features using {FEATURE_GENERATOR.__name__}...\")\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_features_df = FEATURE_GENERATOR(base_df)\n",
    "    \n",
    "    X_all_features = pl.concat([base_df.select(base_features), new_features_df], how=\"horizontal\")\n",
    "    print(f\"Full feature set shape: {X_all_features.shape}\")\n",
    "\n",
    "    TARGET_COL = \"target\"\n",
    "    y = base_df.select(TARGET_COL)\n",
    "    scorer_info_df = base_df.select([\"date_id\", \"forward_returns\", \"risk_free_rate\"])\n",
    "\n",
    "    # --- Time-Series Cross-Validation ---\n",
    "    print(f\"Starting ensemble CV with {N_STRATEGIES} data-subset strategies, following top {K_LEADERS} leaders.\")\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=nsplits)\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    # Master history of OOS performance for all N strategies\n",
    "    history_df = pl.DataFrame()\n",
    "    \n",
    "    # Store final ensembled signals and truths for overall score\n",
    "    overall_final_signals = []\n",
    "    overall_y_true = []\n",
    "    \n",
    "    # We need a reproducible random seed for sampling\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(base_df)):\n",
    "        print(f\"--- Starting Fold {i+1}/{nsplits} ---\")\n",
    "        \n",
    "        # Get the full train/test slices for this fold\n",
    "        X_train_fold_full = X_all_features[train_index]\n",
    "        y_train_fold_full = y[train_index]\n",
    "        X_test_fold = X_all_features[test_index]\n",
    "        \n",
    "        y_test_info = scorer_info_df[test_index]\n",
    "\n",
    "        # Store proposals (signals, returns) for all N models for *this* fold\n",
    "        current_fold_proposals_df = y_test_info.clone()\n",
    "        \n",
    "        # --- 1. Train N strategies on data subsets ---\n",
    "        n_samples = X_train_fold_full.height\n",
    "        subset_size = int(n_samples * SUBSAMPLE_RATIO)\n",
    "        \n",
    "        for j in range(N_STRATEGIES):\n",
    "            \n",
    "            # Create the random subset for strategy j\n",
    "            subset_fold_indices = rng.choice(n_samples, subset_size, replace=False)\n",
    "            \n",
    "            # Slice the training data for this strategy\n",
    "            X_train_j = X_train_fold_full[subset_fold_indices]\n",
    "            y_train_j = y_train_fold_full[subset_fold_indices]\n",
    "\n",
    "            # Define the model\n",
    "            model = xgb.XGBRegressor(\n",
    "                objective='reg:absoluteerror', n_estimators=20, device='cuda',\n",
    "                learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8,\n",
    "                n_jobs=-1, random_state=42 + j # Add random_state jitter for model\n",
    "            )\n",
    "            \n",
    "            # Train the model on the subset\n",
    "            model.fit(X_train_j, y_train_j, verbose=False)\n",
    "\n",
    "            # Get signals and returns for this strategy\n",
    "            # IMPORTANT: Predict on the *full* OOS test set\n",
    "            predictions = model.predict(X_test_fold) \n",
    "            signals = convert_to_signal(predictions)\n",
    "            returns = calculate_strategy_returns_pl(y_test_info, signals)\n",
    "            \n",
    "            # Store in the fold's proposal DataFrame\n",
    "            current_fold_proposals_df = current_fold_proposals_df.with_columns(\n",
    "                pl.Series(f'signals_{j}', signals),\n",
    "                returns.alias(f'returns_{j}')\n",
    "            )\n",
    "\n",
    "        # --- 2. Select Top k Leaders ---\n",
    "        # Look at the history *before* this fold\n",
    "        hist_window = history_df.tail(ROLLING_WINDOW_DAYS)\n",
    "        \n",
    "        if hist_window.height < MIN_HISTORY_DAYS:\n",
    "            # Not enough history, just average all strategies\n",
    "            print(f\"  Not enough history (< {MIN_HISTORY_DAYS} days), averaging all {N_STRATEGIES} strategies.\")\n",
    "            top_k_indices = list(range(N_STRATEGIES))\n",
    "        else:\n",
    "            # We have history, find the leaders\n",
    "            leaderboard = []\n",
    "            for j in range(N_STRATEGIES):\n",
    "                sharpe = calculate_sharpe_for_leaderboard(hist_window, f'returns_{j}')\n",
    "                leaderboard.append((sharpe, j))\n",
    "            \n",
    "            # Sort by Sharpe (descending)\n",
    "            leaderboard.sort(key=lambda x: x[0], reverse=True)\n",
    "            top_k_indices = [j for sharpe, j in leaderboard[:K_LEADERS]]\n",
    "            print(f\"  Top {K_LEADERS} leaders: {top_k_indices} (Sharpes: {[f'{s:.3f}' for s, j in leaderboard[:K_LEADERS]]})\")\n",
    "\n",
    "        # --- 3. Ensemble Positions ---\n",
    "        # Average the signals from the top k leaders\n",
    "        ensembled_signals = np.zeros(len(X_test_fold))\n",
    "        for idx in top_k_indices:\n",
    "            ensembled_signals += current_fold_proposals_df[f'signals_{idx}'].to_numpy()\n",
    "            \n",
    "        ensembled_signals /= len(top_k_indices)\n",
    "        \n",
    "        # --- 4. Score and Update History ---\n",
    "        print(\"  Ensemble Score for this fold:\")\n",
    "        score = calculate_competition_score(y_test_info, ensembled_signals)\n",
    "        cv_scores.append(score)\n",
    "        \n",
    "        # Store for overall score calculation\n",
    "        overall_y_true.append(y_test_info)\n",
    "        overall_final_signals.append(ensembled_signals)\n",
    "        \n",
    "        # Add this fold's results to the master history *after* scoring\n",
    "        history_df = pl.concat([history_df, current_fold_proposals_df])\n",
    "\n",
    "    # --- Final Evaluation ---\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    print(f\"\\n--- CV Finished ---\")\n",
    "    print(f\"Mean Ensembled CV Score: {mean_score:.4f}, std: {np.std(cv_scores):.4f}\")\n",
    "    \n",
    "    # Calculate overall score on all OOS predictions\n",
    "    overall_y_true_df = pl.concat(overall_y_true)\n",
    "    overall_final_signals_arr = np.concatenate(overall_final_signals)\n",
    "    \n",
    "    print(\"\\n--- Overall OOS Performance ---\")\n",
    "    overall_score = calculate_competition_score(overall_y_true_df, overall_final_signals_arr)\n",
    "    print(f\" Overall Ensembled Score: {overall_score:.4f}\")\n",
    "    \n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56459772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solve: daily reranking\n",
    "\n",
    "def solve(df: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Runs a time-series cross-validation process using a \"top-k leader\"\n",
    "    ensemble strategy with DAILY re-ranking.\n",
    "    \n",
    "    1. Trains N models at the start of each fold.\n",
    "    2. Iterates DAY-BY-DAY through the test set.\n",
    "    3. Each day, it finds the top k leaders based on the most recent \n",
    "       (e.g., 30-day) history.\n",
    "    4. It ensembles the positions from *only* those leaders for that day.\n",
    "    5. Appends the daily performance of ALL N strategies to the master \n",
    "       history to be used for the next day's ranking.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Ensemble Configuration ---\n",
    "    N_STRATEGIES = 20           # We'll create 10 strategies\n",
    "    SUBSAMPLE_RATIO = 0.2       # Each strategy sees 70% of the training data\n",
    "    FEATURE_GENERATOR = generate_features_7 # All strategies use this\n",
    "    \n",
    "    K_LEADERS = 4               # How many leaders to follow (e.g., 3)\n",
    "    ROLLING_WINDOW_DAYS = 40    # Lookback period to find leaders\n",
    "    MIN_HISTORY_DAYS = 15       # Min days needed to start picking leaders\n",
    "    TRADING_DAYS_PER_YR = 252\n",
    "\n",
    "    nsplits = 20\n",
    "\n",
    "    # --- Helper functions (unchanged) ---\n",
    "    \n",
    "    def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "        ''' Calculates the competition score based on true values and predicted signals. '''\n",
    "        solution = y_true_df.to_pandas()\n",
    "        solution['position'] = y_pred_signals\n",
    "        solution['strategy_returns'] = (\n",
    "            solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "            solution['position'] * solution['forward_returns']\n",
    "        )\n",
    "        strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "        strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        strategy_std = solution['strategy_returns'].std()\n",
    "        if strategy_std == 0: return 0.0\n",
    "        \n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "        market_std = solution['forward_returns'].std()\n",
    "        market_volatility = market_std * np.sqrt(TRADING_DAYS_PER_YR) * 100\n",
    "        strategy_volatility = strategy_std * np.sqrt(TRADING_DAYS_PER_YR) * 100\n",
    "        excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "        vol_penalty = 1 + excess_vol\n",
    "        market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "        market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * TRADING_DAYS_PER_YR)\n",
    "        return_penalty = 1 + (return_gap**2) / 100\n",
    "        adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "        print(f\"  Fold {i+1} (Daily): Strat Vol: {strategy_volatility:.2f}%, Mkt Vol: {market_volatility:.2f}%, Sharpe: {sharpe:.4f}, Adj Sharpe: {adjusted_sharpe:.4f}\")\n",
    "        return adjusted_sharpe\n",
    "\n",
    "    def convert_to_signal(predictions: np.ndarray, multiplier: float = 400.0) -> np.ndarray:\n",
    "        ''' Converts raw model predictions into trading signals in the range [0, 2]. '''\n",
    "        signals = predictions * multiplier + 0.8\n",
    "        return np.clip(signals, 0.0, 2.0)\n",
    "\n",
    "    def calculate_strategy_returns_pl(y_true_df_1_day: pl.DataFrame, y_pred_signal_1_day: np.ndarray) -> pl.Series:\n",
    "        \"\"\" Calculates strategy returns for a *single day*. \"\"\"\n",
    "        signals_series = pl.Series(\"position\", y_pred_signal_1_day)\n",
    "        df_with_pos = y_true_df_1_day.with_columns(signals_series)\n",
    "        strategy_returns = (\n",
    "            df_with_pos['risk_free_rate'] * (1 - df_with_pos['position']) +\n",
    "            df_with_pos['position'] * df_with_pos['forward_returns']\n",
    "        )\n",
    "        return strategy_returns.alias(\"strategy_returns\")\n",
    "\n",
    "    def calculate_sharpe_for_leaderboard(hist_window: pl.DataFrame, strategy_col: str) -> float:\n",
    "        \"\"\" Calculates the geometric Sharpe ratio for a single strategy from the history. \"\"\"\n",
    "        if hist_window.height == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        strategy_returns = hist_window[strategy_col]\n",
    "        risk_free_rate = hist_window['risk_free_rate']\n",
    "        \n",
    "        strategy_excess_returns = (strategy_returns - risk_free_rate).to_numpy()\n",
    "        \n",
    "        if not np.all(np.isfinite(strategy_excess_returns)):\n",
    "            print(f\"Bad data for {strategy_col} in leaderboard calculation.\")\n",
    "            return -np.inf\n",
    "\n",
    "        log_returns = np.log1p(strategy_excess_returns)\n",
    "        strategy_geo_mean = np.exp(np.mean(log_returns)) - 1\n",
    "\n",
    "        strategy_std = strategy_returns.std()\n",
    "        \n",
    "        if strategy_std is None or strategy_std == 0 or not np.isfinite(strategy_std):\n",
    "            print(f\"Bad std for {strategy_col} in leaderboard calculation.\")\n",
    "            return 0.0\n",
    "            \n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "        \n",
    "        return sharpe if np.isfinite(sharpe) else -np.inf\n",
    "\n",
    "    # --- Data Preparation (Efficient) ---\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    base_df = df.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    \n",
    "    if 'E7' in base_df.columns:\n",
    "        base_df = base_df.drop('E7')\n",
    "        \n",
    "    base_df = base_df.with_columns(pl.all().forward_fill())\n",
    "    print(f\"Base DataFrame shape after cleaning: {base_df.shape}\")\n",
    "\n",
    "    # --- Generate ALL features ONCE ---\n",
    "    print(f\"Generating features using {FEATURE_GENERATOR.__name__}...\")\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_features_df = FEATURE_GENERATOR(base_df)\n",
    "    \n",
    "    X_all_features = pl.concat([base_df.select(base_features), new_features_df], how=\"horizontal\")\n",
    "    print(f\"Full feature set shape: {X_all_features.shape}\")\n",
    "\n",
    "    TARGET_COL = \"target\"\n",
    "    y = base_df.select(TARGET_COL)\n",
    "    scorer_info_df = base_df.select([\"date_id\", \"forward_returns\", \"risk_free_rate\"])\n",
    "\n",
    "    # --- Time-Series Cross-Validation ---\n",
    "    print(f\"Starting DAILY re-ranking CV with {N_STRATEGIES} strategies, following top {K_LEADERS} leaders.\")\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=nsplits)\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    # Master history of OOS performance for all N strategies\n",
    "    # This will be updated DAY-BY-DAY\n",
    "    history_df = pl.DataFrame()\n",
    "    \n",
    "    # Store final ensembled signals and truths for overall score\n",
    "    overall_final_signals = []\n",
    "    overall_y_true = []\n",
    "    \n",
    "    rng = np.random.default_rng(seed=12)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(base_df)):\n",
    "        print(f\"--- Starting Fold {i+1}/{nsplits} (Days {test_index[0]} to {test_index[-1]}) ---\")\n",
    "        \n",
    "        # Get the full train/test slices for this fold\n",
    "        X_train_fold_full = X_all_features[train_index]\n",
    "        y_train_fold_full = y[train_index]\n",
    "        \n",
    "        X_test_fold = X_all_features[test_index]\n",
    "        y_test_info_fold = scorer_info_df[test_index] # All \"truth\" data for this fold\n",
    "\n",
    "        # --- 1. Train N Models (Once per fold) ---\n",
    "        # print(\"  Training N models...\")\n",
    "        N_signals_fold = [] # Will store signals for all N models\n",
    "        n_samples = X_train_fold_full.height\n",
    "        subset_size = int(n_samples * SUBSAMPLE_RATIO)\n",
    "        \n",
    "        for j in range(N_STRATEGIES):\n",
    "            subset_fold_indices = rng.choice(n_samples, subset_size, replace=False)\n",
    "            X_train_j = X_train_fold_full[subset_fold_indices]\n",
    "            y_train_j = y_train_fold_full[subset_fold_indices]\n",
    "\n",
    "            model = xgb.XGBRegressor(\n",
    "                objective='reg:absoluteerror', n_estimators=20, device='cuda',\n",
    "                learning_rate=0.05, max_depth=4, subsample=0.8, colsample_bytree=0.8,\n",
    "                n_jobs=-1, random_state=42 + j \n",
    "            )\n",
    "            \n",
    "            model.fit(X_train_j, y_train_j, verbose=False)\n",
    "\n",
    "            # Predict on the *entire* test fold and store signals\n",
    "            predictions = model.predict(X_test_fold) \n",
    "            signals = convert_to_signal(predictions)\n",
    "            N_signals_fold.append(signals)\n",
    "            \n",
    "        # Stack into a [days, strategies] numpy array\n",
    "        N_signals_fold = np.stack(N_signals_fold, axis=1) \n",
    "        \n",
    "        # --- 2. Iterate Day-By-Day through the test fold ---\n",
    "        print(f\"  Iterating daily and re-ranking leaders...\")\n",
    "        \n",
    "        # Store the *final* ensembled signals for this fold\n",
    "        fold_ensembled_signals = [] \n",
    "\n",
    "        for day_idx in range(len(test_index)):\n",
    "            \n",
    "            # --- 2a. Find Top k Leaders (for today) ---\n",
    "            # Look at history *up to this point*\n",
    "            hist_window = history_df.tail(ROLLING_WINDOW_DAYS)\n",
    "            \n",
    "            if hist_window.height < MIN_HISTORY_DAYS:\n",
    "                # Not enough history, average all strategies\n",
    "                top_k_indices = list(range(N_STRATEGIES))\n",
    "                if day_idx % 50 == 0: # Print status periodically\n",
    "                   print(f\"    Day {day_idx}: Not enough history, averaging all {N_STRATEGIES}.\")\n",
    "            else:\n",
    "                # We have history, find the leaders\n",
    "                leaderboard = []\n",
    "                for j in range(N_STRATEGIES):\n",
    "                    sharpe = calculate_sharpe_for_leaderboard(hist_window, f'returns_{j}')\n",
    "                    leaderboard.append((sharpe, j))\n",
    "                \n",
    "                leaderboard.sort(key=lambda x: x[0], reverse=True)\n",
    "                top_k_indices = [j for sharpe, j in leaderboard[:K_LEADERS]]\n",
    "                if day_idx % 50 == 0: # Print status periodically\n",
    "                    print(f\"    Day {day_idx}: Top {K_LEADERS} leaders: {top_k_indices}, Sharpes: {[f'{s:.3f}' for s, j in leaderboard[:K_LEADERS]]}\")\n",
    "\n",
    "            # --- 2b. Ensemble Position (for today) ---\n",
    "            # Get the pre-calculated signals for *all N* strategies *for today*\n",
    "            today_N_signals = N_signals_fold[day_idx, :]\n",
    "            \n",
    "            # Get signals from *only* the leaders\n",
    "            signals_from_leaders = today_N_signals[top_k_indices]\n",
    "            \n",
    "            # Average them to get the final position\n",
    "            final_signal_today = np.mean(signals_from_leaders)\n",
    "            fold_ensembled_signals.append(final_signal_today)\n",
    "\n",
    "            # --- 2c. Update Master History (for tomorrow's ranking) ---\n",
    "            # Get today's \"truth\" data (1-row DataFrame)\n",
    "            today_test_info = y_test_info_fold[day_idx]\n",
    "            \n",
    "            # Calculate the returns for *all N* individual strategies for today\n",
    "            today_returns_N = []\n",
    "            for j in range(N_STRATEGIES):\n",
    "                signal_j_today = np.array([today_N_signals[j]])\n",
    "                return_j_series = calculate_strategy_returns_pl(today_test_info, signal_j_today)\n",
    "                today_returns_N.append(return_j_series[0]) # Get the single float value\n",
    "            \n",
    "            # Build the new history row\n",
    "            new_history_row_data = {\n",
    "                \"date_id\": today_test_info[\"date_id\"][0],\n",
    "                \"forward_returns\": today_test_info[\"forward_returns\"][0],\n",
    "                \"risk_free_rate\": today_test_info[\"risk_free_rate\"][0],\n",
    "                **{f\"returns_{j}\": ret for j, ret in enumerate(today_returns_N)}\n",
    "            }\n",
    "            new_history_row_df = pl.DataFrame(new_history_row_data)\n",
    "\n",
    "            # Append to the master history_df\n",
    "            history_df = pl.concat([history_df, new_history_row_df])\n",
    "\n",
    "        # --- 3. Score Fold (after iterating all days) ---\n",
    "        print(\"  Fold iteration complete. Scoring fold...\")\n",
    "        fold_final_signals_arr = np.array(fold_ensembled_signals)\n",
    "        score = calculate_competition_score(y_test_info_fold, fold_final_signals_arr)\n",
    "        cv_scores.append(score)\n",
    "        \n",
    "        # Store for overall score calculation\n",
    "        overall_final_signals.append(fold_final_signals_arr)\n",
    "        overall_y_true.append(y_test_info_fold)\n",
    "\n",
    "    # --- 4. Final Evaluation ---\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    print(f\"\\n--- CV Finished ---\")\n",
    "    print(f\"Mean Ensembled CV Score (Daily Re-ranking): {mean_score:.4f}, std: {np.std(cv_scores):.4f}\")\n",
    "    \n",
    "    overall_y_true_df = pl.concat(overall_y_true)\n",
    "    overall_final_signals_arr = np.concatenate(overall_final_signals)\n",
    "    \n",
    "    print(\"\\n--- Overall OOS Performance (Daily Re-ranking) ---\")\n",
    "    # This score is the most important one\n",
    "    overall_score = calculate_competition_score(overall_y_true_df, overall_final_signals_arr)\n",
    "    print(f\" Overall Ensembled Score: {overall_score:.4f}\")\n",
    "    \n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51d9829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4990, 98)\n",
      "\n",
      "--- Joining weekday feature onto sliced data ---\n",
      "Initial DataFrame shape: (4990, 99)\n",
      "Base DataFrame shape after cleaning: (4990, 98)\n",
      "Generating features using generate_features_7...\n",
      "Full feature set shape: (4990, 144)\n",
      "Starting ensemble CV with 10 data-subset strategies, following top 3 leaders.\n",
      "--- Starting Fold 1/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104214/540012060.py:17: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.selectors.numeric().rolling_mean(window_size=5, min_periods=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Not enough history (< 15 days), averaging all 10 strategies.\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 14.92%, Market Volatility: 13.21%, Sharpe: 0.6691, Adjusted Sharpe: 0.6691\n",
      "--- Starting Fold 2/20 ---\n",
      "  Top 3 leaders: [9, 7, 0] (Sharpes: ['2.925', '2.786', '2.770'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 27.49%, Market Volatility: 21.92%, Sharpe: -0.9876, Adjusted Sharpe: -0.8454\n",
      "--- Starting Fold 3/20 ---\n",
      "  Top 3 leaders: [6, 7, 3] (Sharpes: ['0.247', '-0.361', '-0.417'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 46.25%, Market Volatility: 33.66%, Sharpe: -0.4097, Adjusted Sharpe: -0.1542\n",
      "--- Starting Fold 4/20 ---\n",
      "  Top 3 leaders: [5, 3, 7] (Sharpes: ['4.433', '3.823', '3.273'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 22.65%, Market Volatility: 18.61%, Sharpe: 0.1355, Adjusted Sharpe: 0.1093\n",
      "--- Starting Fold 5/20 ---\n",
      "  Top 3 leaders: [1, 6, 8] (Sharpes: ['4.327', '3.345', '3.327'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 13.94%, Market Volatility: 13.14%, Sharpe: 1.9686, Adjusted Sharpe: 1.9686\n",
      "--- Starting Fold 6/20 ---\n",
      "  Top 3 leaders: [1, 2, 7] (Sharpes: ['4.202', '4.122', '3.579'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 25.81%, Market Volatility: 22.43%, Sharpe: 0.5740, Adjusted Sharpe: 0.5740\n",
      "--- Starting Fold 7/20 ---\n",
      "  Top 3 leaders: [4, 8, 2] (Sharpes: ['4.515', '3.300', '3.253'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 10.25%, Market Volatility: 11.85%, Sharpe: 2.1968, Adjusted Sharpe: 2.1968\n",
      "--- Starting Fold 8/20 ---\n",
      "  Top 3 leaders: [1, 5, 4] (Sharpes: ['2.950', '2.949', '2.491'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 12.29%, Market Volatility: 10.99%, Sharpe: 1.2875, Adjusted Sharpe: 1.2571\n",
      "--- Starting Fold 9/20 ---\n",
      "  Top 3 leaders: [9, 3, 1] (Sharpes: ['1.827', '1.617', '1.344'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 14.65%, Market Volatility: 11.69%, Sharpe: 1.1509, Adjusted Sharpe: 1.0928\n",
      "--- Starting Fold 10/20 ---\n",
      "  Top 3 leaders: [2, 7, 5] (Sharpes: ['1.658', '1.609', '1.184'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 25.79%, Market Volatility: 16.89%, Sharpe: 0.1648, Adjusted Sharpe: 0.1242\n",
      "--- Starting Fold 11/20 ---\n",
      "  Top 3 leaders: [2, 7, 8] (Sharpes: ['4.277', '4.205', '4.066'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 11.62%, Market Volatility: 9.95%, Sharpe: 2.0863, Adjusted Sharpe: 2.0863\n",
      "--- Starting Fold 12/20 ---\n",
      "  Top 3 leaders: [6, 3, 4] (Sharpes: ['5.184', '4.780', '4.655'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 11.66%, Market Volatility: 10.09%, Sharpe: 1.2991, Adjusted Sharpe: 1.2737\n",
      "--- Starting Fold 13/20 ---\n",
      "  Top 3 leaders: [7, 0, 5] (Sharpes: ['0.281', '0.210', '0.120'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 24.18%, Market Volatility: 16.21%, Sharpe: 0.1134, Adjusted Sharpe: 0.0878\n",
      "--- Starting Fold 14/20 ---\n",
      "  Top 3 leaders: [8, 9, 4] (Sharpes: ['5.246', '4.936', '4.810'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 14.53%, Market Volatility: 11.79%, Sharpe: 1.5280, Adjusted Sharpe: 1.4805\n",
      "--- Starting Fold 15/20 ---\n",
      "  Top 3 leaders: [3, 4, 0] (Sharpes: ['7.276', '6.418', '6.307'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 37.65%, Market Volatility: 26.01%, Sharpe: 0.6183, Adjusted Sharpe: 0.4956\n",
      "--- Starting Fold 16/20 ---\n",
      "  Top 3 leaders: [7, 0, 9] (Sharpes: ['3.171', '3.130', '3.075'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 14.40%, Market Volatility: 12.50%, Sharpe: 1.5728, Adjusted Sharpe: 1.5726\n",
      "--- Starting Fold 17/20 ---\n",
      "  Top 3 leaders: [6, 8, 5] (Sharpes: ['0.705', '0.500', '0.080'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 39.99%, Market Volatility: 23.74%, Sharpe: -0.8106, Adjusted Sharpe: -0.1972\n",
      "--- Starting Fold 18/20 ---\n",
      "  Top 3 leaders: [7, 3, 1] (Sharpes: ['0.428', '0.212', '0.136'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 17.22%, Market Volatility: 14.79%, Sharpe: 0.5554, Adjusted Sharpe: 0.5554\n",
      "--- Starting Fold 19/20 ---\n",
      "  Top 3 leaders: [6, 9, 0] (Sharpes: ['-2.329', '-2.357', '-3.181'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 12.85%, Market Volatility: 12.39%, Sharpe: 2.6012, Adjusted Sharpe: 2.6012\n",
      "--- Starting Fold 20/20 ---\n",
      "  Top 3 leaders: [7, 3, 1] (Sharpes: ['2.186', '1.985', '1.882'])\n",
      "  Ensemble Score for this fold:\n",
      "Strategy Volatility: 20.69%, Market Volatility: 16.28%, Sharpe: 0.4666, Adjusted Sharpe: 0.4358\n",
      "\n",
      "--- CV Finished ---\n",
      "Mean Ensembled CV Score: 0.8692, std: 0.8974\n",
      "\n",
      "--- Overall OOS Performance ---\n",
      "Strategy Volatility: 23.22%, Market Volatility: 17.49%, Sharpe: 0.4286, Adjusted Sharpe: 0.3801\n",
      " Overall Ensembled Score: 0.3801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3800786510269276"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def evaluate(excessarg: int) -> float:\n",
    "    \"\"\"\n",
    "    Main evaluation function for FunSearch. It loads the data\n",
    "    and runs the solver which performs cross-validation.\n",
    "    \"\"\"\n",
    "    full_train_df = pl.read_csv(TRAIN_DATA_PATH)\n",
    "    # Use a slice of data for faster evaluation runs during development\n",
    "    df_raw = full_train_df.slice(4000)\n",
    "    print(df_raw.shape)\n",
    "\n",
    "    #fill nulls in df with mean\n",
    "    df = df_raw.with_columns(\n",
    "        # Select all numeric columns for the operation\n",
    "        pl.selectors.numeric()\n",
    "          # Step 1: Attempt to fill with the rolling mean of each respective column\n",
    "          .fill_null(\n",
    "              pl.selectors.numeric().rolling_mean(window_size=5, min_periods=1)\n",
    "          )\n",
    "          # Step 2: Fall back to the global column mean for any remaining nulls\n",
    "          #.fill_null(strategy='mean')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "    pl.col(\"date_id\").cast(pl.Int64)\n",
    "    )\n",
    "    \n",
    "    weekday_df = add_weekday_column(SPY_DATA_PATH)\n",
    "    print(\"\\n--- Joining weekday feature onto sliced data ---\")\n",
    "    # Join the weekday information onto the sliced training data.\n",
    "    # A 'left' join ensures we keep all rows from the original `df`.\n",
    "    df_with_features = df.join(weekday_df, on=\"date_id\", how=\"left\")\n",
    "    # print(\"DataFrame after join:\")\n",
    "    # print(df_with_features.shape)\n",
    "    return solve(df_with_features)\n",
    "  \n",
    "def add_weekday_column(input_csv_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file, adds a 'weekday' column based on the 'Date' column,\n",
    "    and saves the result to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): The path to the source CSV file.\n",
    "        output_csv_path (str): The path where the output CSV will be saved.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a Polars DataFrame\n",
    "    df = pl.read_csv(input_csv_path)\n",
    "\n",
    "    # Add a new column named 'weekday'\n",
    "    # 1. Select the 'Date' column.\n",
    "    # 2. Convert the string representation to a proper date type.\n",
    "    # 3. Use the .dt.weekday() function to get the day of the week (Monday=1, Sunday=7).\n",
    "    # 4. Alias the new expression to 'weekday'.\n",
    "    df_with_weekday = df.with_columns(\n",
    "        pl.col(\"Date\").str.to_date().dt.weekday().alias(\"weekday\")\n",
    "    )\n",
    "\n",
    "    # Print the transformed DataFrame to the console to show the result\n",
    "    returned_df = df_with_weekday.select([\"date_id\", \"weekday\"])\n",
    "    return returned_df\n",
    "\n",
    "evaluate(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
