{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0959d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# The training data path should be updated to your actual training file.\n",
    "TRAIN_DATA_PATH = \"./kaggle/train.csv\"\n",
    "SPY_DATA_PATH = \"./kaggle/spy-historical.csv\"\n",
    "\n",
    "def generate_features_7 (df: pl.DataFrame) -> pl.DataFrame:\n",
    "  \"\"\"Generates new features from the base data.\n",
    "    This function is the target of the evolutionary algorithm.\n",
    "  \n",
    "    Available Feature Categories:\n",
    "    - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "    - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "    - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "    - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "    - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "    - S* (Sentiment features): 12 columns (S1-S12)\n",
    "    - V* (Volatility features): 13 columns (V1-V13)\n",
    "  \"\"\"\n",
    "  new_features = pl.DataFrame({\n",
    "      # --- 20 Pairwise Interactions ---\n",
    "      'feat_M1_x_V1': df['M1'] * df['V1'],\n",
    "      'feat_P1_add_E1': df['P1'] + df['E1'],\n",
    "      'feat_S1_sub_I1': df['S1'] - df['I1'],\n",
    "      'feat_M10_div_V10': df['M10'] / (df['V10'] + 1e-6),\n",
    "      'feat_P10_x_E10': df['P10'] * df['E10'],\n",
    "      'feat_M2_x_S3': df['M2'] * df['S3'],\n",
    "      'feat_V2_div_P2': df['V2'] / (df['P2'] + 1e-6),\n",
    "      'feat_E4_sub_I3': df['E4'] - df['I3'],\n",
    "      'feat_S7_add_M12': df['S7'] + df['M12'],\n",
    "      'feat_I5_x_V11': df['I5'] * df['V11'],\n",
    "      'feat_P5_div_S8': df['P5'] / (df['S8'] + 1e-6),\n",
    "      'feat_E12_x_I9': df['E12'] * df['I9'],\n",
    "      'feat_M1_div_S1': df['M1'] / (df['S1'] + 1e-6),\n",
    "      'feat_V1_add_P1': df['V1'] + df['P1'],\n",
    "      'feat_E1_sub_I1': df['E1'] - df['I1'],\n",
    "      'feat_M2_div_V2': df['M2'] / (df['V2'] + 1e-6),\n",
    "      'feat_P2_x_S3': df['P2'] * df['S3'],\n",
    "      'feat_E4_add_M10': df['E4'] + df['M10'],\n",
    "      'feat_I3_sub_V10': df['I3'] - df['V10'],\n",
    "      'feat_S7_x_P10': df['S7'] * df['P10'],\n",
    "      # --- 10 Rolling Window Features ---\n",
    "      'feat_V2_roll_mean_5': df['V2'].rolling_mean(window_size=5),\n",
    "      'feat_V1_roll_std_5': df['V1'].rolling_std(window_size=5),\n",
    "      'feat_M1_roll_mean_20': df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_M3_roll_std_20': df['M3'].rolling_std(window_size=20),\n",
    "      'feat_P1_roll_max_10': df['P1'].rolling_max(window_size=10),\n",
    "      'feat_P1_roll_min_10': df['P1'].rolling_min(window_size=10),\n",
    "      'feat_E5_roll_mean_50': df['E5'].rolling_mean(window_size=50),\n",
    "      'feat_S1_roll_std_50': df['S1'].rolling_std(window_size=50),\n",
    "      'feat_I1_roll_mean_10': df['I1'].rolling_mean(window_size=10),\n",
    "      'feat_V10_roll_std_10': df['V10'].rolling_std(window_size=10),\n",
    "      # --- 10 Complex Interactions (3+ elements) ---\n",
    "      'feat_M1_V1_div_P1': (df['M1'] * df['V1']) / (df['P1'] + 1e-6),\n",
    "      'feat_E1_S1_add_I1': df['E1'] + df['S1'] - df['I1'],\n",
    "      'feat_M2_P2_sub_V2': df['M2'] + df['P2'] - df['V2'],\n",
    "      'feat_S7_div_E4_I3': df['S7'] / (df['E4'] + df['I3'] + 1e-6),\n",
    "      'feat_P5_x_M10_x_V10': df['P5'] * df['M10'] * df['V10'],\n",
    "      'feat_roll_diff_M1_5_20': df['M1'].rolling_mean(window_size=5) - df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_roll_diff_V1_5_20': df['V1'].rolling_mean(window_size=5) - df['V1'].rolling_mean(window_size=20),\n",
    "      'feat_M_S_P_combo': (df['M12'] - df['M1']) / (df['S1'] + df['P1'] + 1e-6),\n",
    "      'feat_V_E_I_combo': (df['V11'] + df['V2']) * (df['E1'] - df['I1']),\n",
    "      'feat_ratio_of_ratios': (df['M1']/(df['V1']+1e-6)) / (df['P1']/(df['S1']+1e-6)),\n",
    "      # --- 10 New Features ---\n",
    "      'feat_M1_x_V1_x_P1': df['M1'] * df['V1'] * df['P1'],\n",
    "      'feat_E1_div_S1': df['E1'] / (df['S1'] + 1e-6),\n",
    "      'feat_I1_sub_V1': df['I1'] - df['V1'],\n",
    "      'feat_M10_add_V10': df['M10'] + df['V10'],\n",
    "      'feat_P10_div_E10': df['P10'] / (df['E10'] + 1e-6),\n",
    "      'feat_M2_add_S3': df['M2'] + df['S3'],\n",
    "      'feat_V2_x_P2': df['V2'] * df['P2'],\n",
    "      'feat_E4_add_I3': df['E4'] + df['I3'],\n",
    "      'feat_S7_div_M12': df['S7'] / (df['M12'] + 1e-6),\n",
    "      'feat_I5_div_V11': df['I5'] / (df['V11'] + 1e-6),\n",
    "      #'feat_M1_log_P1': np.log(df['M1'] + 1e-6) / np.log(df['P1'] + 1e-6),\n",
    "      # --- SAFER LOGIC HERE ---\n",
    "      #'feat_M1_log_P1': pl.when( (df['M1'] > 0) & (df['P1'] > 0) & (df['P1'] != 1) ).then( df['M1'].log() / df['P1'].log() ).otherwise(0),\n",
    "      # --- END SAFER LOGIC ---\n",
    "  })\n",
    "  # Fill any nulls created by rolling windows\n",
    "  return new_features.with_columns(pl.all().forward_fill())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e335f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solve: no reranking\n",
    "\n",
    "# --- Make sure all your feature generators are defined above this ---\n",
    "# (We will only use generate_features_7, but it must be defined)\n",
    "# e.g., generate_features_1, ... generate_features_7\n",
    "\n",
    "def solve(df: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Runs a time-series cross-validation process using a \"top-k leader\"\n",
    "    ensemble strategy.\n",
    "    \n",
    "    All N strategies use the *same* feature set (gen_features_7)\n",
    "    but are trained on *random subsets* of the training data to \n",
    "    create \"orthogonal\" models.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Ensemble Configuration ---\n",
    "    N_STRATEGIES = 10           # We'll create 10 strategies\n",
    "    SUBSAMPLE_RATIO = 0.4       # Each strategy sees 70% of the training data\n",
    "    FEATURE_GENERATOR = generate_features_7 # All strategies use this\n",
    "    \n",
    "    K_LEADERS = 3               # How many leaders to follow (e.g., 3)\n",
    "    ROLLING_WINDOW_DAYS = 30    # Lookback period to find leaders\n",
    "    MIN_HISTORY_DAYS = 15       # Min days needed to start picking leaders\n",
    "    TRADING_DAYS_PER_YR = 252\n",
    "    nsplits = 20\n",
    "\n",
    "    # --- Helper functions ---\n",
    "    \n",
    "    def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "        ''' Calculates the competition score based on true values and predicted signals. '''\n",
    "        solution = y_true_df.to_pandas()\n",
    "        solution['position'] = y_pred_signals\n",
    "        solution['strategy_returns'] = (\n",
    "            solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "            solution['position'] * solution['forward_returns']\n",
    "        )\n",
    "        strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "        strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        strategy_std = solution['strategy_returns'].std()\n",
    "        if strategy_std == 0: return 0.0\n",
    "        \n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "        market_std = solution['forward_returns'].std()\n",
    "        market_volatility = market_std * np.sqrt(TRADING_DAYS_PER_YR) * 100\n",
    "        strategy_volatility = strategy_std * np.sqrt(TRADING_DAYS_PER_YR) * 100\n",
    "        excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "        vol_penalty = 1 + excess_vol\n",
    "        market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "        market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * TRADING_DAYS_PER_YR)\n",
    "        return_penalty = 1 + (return_gap**2) / 100\n",
    "        adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "        print(f\"Strategy Volatility: {strategy_volatility:.2f}%, Market Volatility: {market_volatility:.2f}%, Sharpe: {sharpe:.4f}, Adjusted Sharpe: {adjusted_sharpe:.4f}\")\n",
    "        return adjusted_sharpe\n",
    "\n",
    "    def convert_to_signal(predictions: np.ndarray, multiplier: float = 400.0) -> np.ndarray:\n",
    "        ''' Converts raw model predictions into trading signals in the range [0, 2]. '''\n",
    "        signals = predictions * multiplier + 1\n",
    "        return np.clip(signals, 0.0, 2.0)\n",
    "\n",
    "    def calculate_strategy_returns_pl(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> pl.Series:\n",
    "        \"\"\" Calculates strategy returns using Polars for history tracking. \"\"\"\n",
    "        signals_series = pl.Series(\"position\", y_pred_signals)\n",
    "        df_with_pos = y_true_df.with_columns(signals_series)\n",
    "        strategy_returns = (\n",
    "            df_with_pos['risk_free_rate'] * (1 - df_with_pos['position']) +\n",
    "            df_with_pos['position'] * df_with_pos['forward_returns']\n",
    "        )\n",
    "        return strategy_returns.alias(\"strategy_returns\")\n",
    "\n",
    "    def calculate_sharpe_for_leaderboard(hist_window: pl.DataFrame, strategy_col: str) -> float:\n",
    "        \"\"\" Calculates the geometric Sharpe ratio for a single strategy from the history. \"\"\"\n",
    "        if hist_window.height == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        strategy_returns = hist_window[strategy_col]\n",
    "        risk_free_rate = hist_window['risk_free_rate']\n",
    "        \n",
    "        # Calculate excess returns\n",
    "        strategy_excess_returns = (strategy_returns - risk_free_rate).to_numpy()\n",
    "        \n",
    "        # Handle potential NaNs or Infs\n",
    "        if not np.all(np.isfinite(strategy_excess_returns)):\n",
    "            return -np.inf # Penalize bad calculations\n",
    "\n",
    "        # Calculate geometric mean of (1 + excess_returns)\n",
    "        try:\n",
    "            log_returns = np.log1p(strategy_excess_returns)\n",
    "            strategy_geo_mean = np.exp(np.mean(log_returns)) - 1\n",
    "        except (ValueError, FloatingPointError):\n",
    "            return -np.inf # Bad strategy if 1+ret <= 0\n",
    "\n",
    "        # Std of *total* returns, as in original score\n",
    "        strategy_std = strategy_returns.std()\n",
    "        \n",
    "        if strategy_std is None or strategy_std == 0 or not np.isfinite(strategy_std):\n",
    "            return 0.0\n",
    "            \n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "        \n",
    "        return sharpe if np.isfinite(sharpe) else -np.inf\n",
    "\n",
    "    # --- Data Preparation (More efficient) ---\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    base_df = df.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    \n",
    "    # Handle 'E7' or other potential bad columns if necessary\n",
    "    if 'E7' in base_df.columns:\n",
    "        base_df = base_df.drop('E7')\n",
    "        \n",
    "    base_df = base_df.with_columns(pl.all().forward_fill())\n",
    "    print(f\"Base DataFrame shape after cleaning: {base_df.shape}\")\n",
    "\n",
    "    # --- Generate ALL features ONCE ---\n",
    "    print(f\"Generating features using {FEATURE_GENERATOR.__name__}...\")\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_features_df = FEATURE_GENERATOR(base_df)\n",
    "    \n",
    "    X_all_features = pl.concat([base_df.select(base_features), new_features_df], how=\"horizontal\")\n",
    "    print(f\"Full feature set shape: {X_all_features.shape}\")\n",
    "\n",
    "    TARGET_COL = \"target\"\n",
    "    y = base_df.select(TARGET_COL)\n",
    "    scorer_info_df = base_df.select([\"date_id\", \"forward_returns\", \"risk_free_rate\"])\n",
    "\n",
    "    # --- Time-Series Cross-Validation ---\n",
    "    print(f\"Starting ensemble CV with {N_STRATEGIES} data-subset strategies, following top {K_LEADERS} leaders.\")\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=nsplits)\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    # Master history of OOS performance for all N strategies\n",
    "    history_df = pl.DataFrame()\n",
    "    \n",
    "    # Store final ensembled signals and truths for overall score\n",
    "    overall_final_signals = []\n",
    "    overall_y_true = []\n",
    "    \n",
    "    # We need a reproducible random seed for sampling\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(base_df)):\n",
    "        print(f\"--- Starting Fold {i+1}/{nsplits} ---\")\n",
    "        \n",
    "        # Get the full train/test slices for this fold\n",
    "        X_train_fold_full = X_all_features[train_index]\n",
    "        y_train_fold_full = y[train_index]\n",
    "        X_test_fold = X_all_features[test_index]\n",
    "        \n",
    "        y_test_info = scorer_info_df[test_index]\n",
    "\n",
    "        # Store proposals (signals, returns) for all N models for *this* fold\n",
    "        current_fold_proposals_df = y_test_info.clone()\n",
    "        \n",
    "        # --- 1. Train N strategies on data subsets ---\n",
    "        n_samples = X_train_fold_full.height\n",
    "        subset_size = int(n_samples * SUBSAMPLE_RATIO)\n",
    "        \n",
    "        for j in range(N_STRATEGIES):\n",
    "            \n",
    "            # Create the random subset for strategy j\n",
    "            subset_fold_indices = rng.choice(n_samples, subset_size, replace=False)\n",
    "            \n",
    "            # Slice the training data for this strategy\n",
    "            X_train_j = X_train_fold_full[subset_fold_indices]\n",
    "            y_train_j = y_train_fold_full[subset_fold_indices]\n",
    "\n",
    "            # Define the model\n",
    "            model = xgb.XGBRegressor(\n",
    "                objective='reg:absoluteerror', n_estimators=20, device='cuda',\n",
    "                learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8,\n",
    "                n_jobs=-1, random_state=42 + j # Add random_state jitter for model\n",
    "            )\n",
    "            \n",
    "            # Train the model on the subset\n",
    "            model.fit(X_train_j, y_train_j, verbose=False)\n",
    "\n",
    "            # Get signals and returns for this strategy\n",
    "            # IMPORTANT: Predict on the *full* OOS test set\n",
    "            predictions = model.predict(X_test_fold) \n",
    "            signals = convert_to_signal(predictions)\n",
    "            returns = calculate_strategy_returns_pl(y_test_info, signals)\n",
    "            \n",
    "            # Store in the fold's proposal DataFrame\n",
    "            current_fold_proposals_df = current_fold_proposals_df.with_columns(\n",
    "                pl.Series(f'signals_{j}', signals),\n",
    "                returns.alias(f'returns_{j}')\n",
    "            )\n",
    "\n",
    "        # --- 2. Select Top k Leaders ---\n",
    "        # Look at the history *before* this fold\n",
    "        hist_window = history_df.tail(ROLLING_WINDOW_DAYS)\n",
    "        \n",
    "        if hist_window.height < MIN_HISTORY_DAYS:\n",
    "            # Not enough history, just average all strategies\n",
    "            print(f\"  Not enough history (< {MIN_HISTORY_DAYS} days), averaging all {N_STRATEGIES} strategies.\")\n",
    "            top_k_indices = list(range(N_STRATEGIES))\n",
    "        else:\n",
    "            # We have history, find the leaders\n",
    "            leaderboard = []\n",
    "            for j in range(N_STRATEGIES):\n",
    "                sharpe = calculate_sharpe_for_leaderboard(hist_window, f'returns_{j}')\n",
    "                leaderboard.append((sharpe, j))\n",
    "            \n",
    "            # Sort by Sharpe (descending)\n",
    "            leaderboard.sort(key=lambda x: x[0], reverse=True)\n",
    "            top_k_indices = [j for sharpe, j in leaderboard[:K_LEADERS]]\n",
    "            print(f\"  Top {K_LEADERS} leaders: {top_k_indices} (Sharpes: {[f'{s:.3f}' for s, j in leaderboard[:K_LEADERS]]})\")\n",
    "\n",
    "        # --- 3. Ensemble Positions ---\n",
    "        # Average the signals from the top k leaders\n",
    "        ensembled_signals = np.zeros(len(X_test_fold))\n",
    "        for idx in top_k_indices:\n",
    "            ensembled_signals += current_fold_proposals_df[f'signals_{idx}'].to_numpy()\n",
    "            \n",
    "        ensembled_signals /= len(top_k_indices)\n",
    "        \n",
    "        # --- 4. Score and Update History ---\n",
    "        print(\"  Ensemble Score for this fold:\")\n",
    "        score = calculate_competition_score(y_test_info, ensembled_signals)\n",
    "        cv_scores.append(score)\n",
    "        \n",
    "        # Store for overall score calculation\n",
    "        overall_y_true.append(y_test_info)\n",
    "        overall_final_signals.append(ensembled_signals)\n",
    "        \n",
    "        # Add this fold's results to the master history *after* scoring\n",
    "        history_df = pl.concat([history_df, current_fold_proposals_df])\n",
    "\n",
    "    # --- Final Evaluation ---\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    print(f\"\\n--- CV Finished ---\")\n",
    "    print(f\"Mean Ensembled CV Score: {mean_score:.4f}, std: {np.std(cv_scores):.4f}\")\n",
    "    \n",
    "    # Calculate overall score on all OOS predictions\n",
    "    overall_y_true_df = pl.concat(overall_y_true)\n",
    "    overall_final_signals_arr = np.concatenate(overall_final_signals)\n",
    "    \n",
    "    print(\"\\n--- Overall OOS Performance ---\")\n",
    "    overall_score = calculate_competition_score(overall_y_true_df, overall_final_signals_arr)\n",
    "    print(f\" Overall Ensembled Score: {overall_score:.4f}\")\n",
    "    \n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56459772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solve: daily reranking\n",
    "\n",
    "def solve(df: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Runs a time-series cross-validation process using a \"top-k leader\"\n",
    "    ensemble strategy with DAILY re-ranking.\n",
    "    \n",
    "    1. Trains N models at the start of each fold.\n",
    "    2. Iterates DAY-BY-DAY through the test set.\n",
    "    3. Each day, it finds the top k leaders based on the most recent \n",
    "       (e.g., 30-day) history.\n",
    "    4. It ensembles the positions from *only* those leaders for that day.\n",
    "    5. Appends the daily performance of ALL N strategies to the master \n",
    "       history to be used for the next day's ranking.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Ensemble Configuration ---\n",
    "    N_STRATEGIES = 20           # We'll create 10 strategies\n",
    "    SUBSAMPLE_RATIO = 0.2       # Each strategy sees 70% of the training data\n",
    "    FEATURE_GENERATOR = generate_features_7 # All strategies use this\n",
    "    \n",
    "    K_LEADERS = 4               # How many leaders to follow (e.g., 3)\n",
    "    ROLLING_WINDOW_DAYS = 40    # Lookback period to find leaders\n",
    "    MIN_HISTORY_DAYS = 15       # Min days needed to start picking leaders\n",
    "    TRADING_DAYS_PER_YR = 252\n",
    "\n",
    "    nsplits = 20\n",
    "\n",
    "    # --- Helper functions (unchanged) ---\n",
    "    \n",
    "    def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "        ''' Calculates the competition score based on true values and predicted signals. '''\n",
    "        solution = y_true_df.to_pandas()\n",
    "        solution['position'] = y_pred_signals\n",
    "        solution['strategy_returns'] = (\n",
    "            solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "            solution['position'] * solution['forward_returns']\n",
    "        )\n",
    "        strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "        strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        strategy_std = solution['strategy_returns'].std()\n",
    "        if strategy_std == 0: return 0.0\n",
    "        \n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "        market_std = solution['forward_returns'].std()\n",
    "        market_volatility = market_std * np.sqrt(TRADING_DAYS_PER_YR) * 100\n",
    "        strategy_volatility = strategy_std * np.sqrt(TRADING_DAYS_PER_YR) * 100\n",
    "        excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "        vol_penalty = 1 + excess_vol\n",
    "        market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "        market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "        return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * TRADING_DAYS_PER_YR)\n",
    "        return_penalty = 1 + (return_gap**2) / 100\n",
    "        adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "        print(f\"  Fold {i+1} (Daily): Strat Vol: {strategy_volatility:.2f}%, Mkt Vol: {market_volatility:.2f}%, Sharpe: {sharpe:.4f}, Adj Sharpe: {adjusted_sharpe:.4f}\")\n",
    "        return adjusted_sharpe\n",
    "\n",
    "    def convert_to_signal(predictions: np.ndarray, multiplier: float = 400.0) -> np.ndarray:\n",
    "        ''' Converts raw model predictions into trading signals in the range [0, 2]. '''\n",
    "        signals = predictions * multiplier + 0.9\n",
    "        signals = -np.abs(predictions-0.001) * 1000 + 3\n",
    "        return np.clip(signals, 0.0, 2.0)\n",
    "\n",
    "    def calculate_strategy_returns_pl(y_true_df_1_day: pl.DataFrame, y_pred_signal_1_day: np.ndarray) -> pl.Series:\n",
    "        \"\"\" Calculates strategy returns for a *single day*. \"\"\"\n",
    "        signals_series = pl.Series(\"position\", y_pred_signal_1_day)\n",
    "        df_with_pos = y_true_df_1_day.with_columns(signals_series)\n",
    "        strategy_returns = (\n",
    "            df_with_pos['risk_free_rate'] * (1 - df_with_pos['position']) +\n",
    "            df_with_pos['position'] * df_with_pos['forward_returns']\n",
    "        )\n",
    "        return strategy_returns.alias(\"strategy_returns\")\n",
    "\n",
    "    def calculate_sharpe_for_leaderboard(hist_window: pl.DataFrame, strategy_col: str) -> float:\n",
    "        \"\"\" Calculates the geometric Sharpe ratio for a single strategy from the history. \"\"\"\n",
    "        if hist_window.height == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        strategy_returns = hist_window[strategy_col]\n",
    "        risk_free_rate = hist_window['risk_free_rate']\n",
    "        \n",
    "        strategy_excess_returns = (strategy_returns - risk_free_rate).to_numpy()\n",
    "        \n",
    "        if not np.all(np.isfinite(strategy_excess_returns)):\n",
    "            print(f\"Bad data for {strategy_col} in leaderboard calculation.\")\n",
    "            return -np.inf\n",
    "\n",
    "        log_returns = np.log1p(strategy_excess_returns)\n",
    "        strategy_geo_mean = np.exp(np.mean(log_returns)) - 1\n",
    "\n",
    "        strategy_std = strategy_returns.std()\n",
    "        \n",
    "        if strategy_std is None or strategy_std == 0 or not np.isfinite(strategy_std):\n",
    "            print(f\"Bad std for {strategy_col} in leaderboard calculation.\")\n",
    "            return 0.0\n",
    "            \n",
    "        sharpe = strategy_geo_mean / strategy_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "        \n",
    "        return sharpe if np.isfinite(sharpe) else -np.inf\n",
    "\n",
    "    # --- Data Preparation (Efficient) ---\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    base_df = df.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    \n",
    "    if 'E7' in base_df.columns:\n",
    "        base_df = base_df.drop('E7')\n",
    "        \n",
    "    base_df = base_df.with_columns(pl.all().forward_fill())\n",
    "    print(f\"Base DataFrame shape after cleaning: {base_df.shape}\")\n",
    "\n",
    "    # --- Generate ALL features ONCE ---\n",
    "    print(f\"Generating features using {FEATURE_GENERATOR.__name__}...\")\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_features_df = FEATURE_GENERATOR(base_df)\n",
    "    \n",
    "    X_all_features = pl.concat([base_df.select(base_features), new_features_df], how=\"horizontal\")\n",
    "    print(f\"Full feature set shape: {X_all_features.shape}\")\n",
    "\n",
    "    TARGET_COL = \"target\"\n",
    "    y = base_df.select(TARGET_COL)\n",
    "    scorer_info_df = base_df.select([\"date_id\", \"forward_returns\", \"risk_free_rate\"])\n",
    "\n",
    "    # --- Time-Series Cross-Validation ---\n",
    "    print(f\"Starting DAILY re-ranking CV with {N_STRATEGIES} strategies, following top {K_LEADERS} leaders.\")\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=nsplits)\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    # Master history of OOS performance for all N strategies\n",
    "    # This will be updated DAY-BY-DAY\n",
    "    history_df = pl.DataFrame()\n",
    "    \n",
    "    # Store final ensembled signals and truths for overall score\n",
    "    overall_final_signals = []\n",
    "    overall_y_true = []\n",
    "    \n",
    "    rng = np.random.default_rng(seed=12)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(base_df)):\n",
    "        print(f\"--- Starting Fold {i+1}/{nsplits} (Days {test_index[0]} to {test_index[-1]}) ---\")\n",
    "        \n",
    "        # Get the full train/test slices for this fold\n",
    "        X_train_fold_full = X_all_features[train_index]\n",
    "        y_train_fold_full = y[train_index]\n",
    "        \n",
    "        X_test_fold = X_all_features[test_index]\n",
    "        y_test_info_fold = scorer_info_df[test_index] # All \"truth\" data for this fold\n",
    "\n",
    "        # --- 1. Train N Models (Once per fold) ---\n",
    "        # print(\"  Training N models...\")\n",
    "        N_signals_fold = [] # Will store signals for all N models\n",
    "        n_samples = X_train_fold_full.height\n",
    "        subset_size = int(n_samples * SUBSAMPLE_RATIO)\n",
    "        \n",
    "        for j in range(N_STRATEGIES):\n",
    "            subset_fold_indices = rng.choice(n_samples, subset_size, replace=False)\n",
    "            X_train_j = X_train_fold_full[subset_fold_indices]\n",
    "            y_train_j = y_train_fold_full[subset_fold_indices]\n",
    "\n",
    "            model = xgb.XGBRegressor(\n",
    "                objective='reg:absoluteerror', n_estimators=20, device='cuda',\n",
    "                learning_rate=0.05, max_depth=4, subsample=0.8, colsample_bytree=0.8,\n",
    "                n_jobs=-1, random_state=42 + j \n",
    "            )\n",
    "            \n",
    "            model.fit(X_train_j, y_train_j, verbose=False)\n",
    "\n",
    "            # Predict on the *entire* test fold and store signals\n",
    "            predictions = model.predict(X_test_fold) \n",
    "            signals = convert_to_signal(predictions)\n",
    "            N_signals_fold.append(signals)\n",
    "            \n",
    "        # Stack into a [days, strategies] numpy array\n",
    "        N_signals_fold = np.stack(N_signals_fold, axis=1) \n",
    "        \n",
    "        # --- 2. Iterate Day-By-Day through the test fold ---\n",
    "        print(f\"  Iterating daily and re-ranking leaders...\")\n",
    "        \n",
    "        # Store the *final* ensembled signals for this fold\n",
    "        fold_ensembled_signals = [] \n",
    "\n",
    "        for day_idx in range(len(test_index)):\n",
    "            \n",
    "            # --- 2a. Find Top k Leaders (for today) ---\n",
    "            # Look at history *up to this point*\n",
    "            hist_window = history_df.tail(ROLLING_WINDOW_DAYS)\n",
    "            \n",
    "            if hist_window.height < MIN_HISTORY_DAYS:\n",
    "                # Not enough history, average all strategies\n",
    "                top_k_indices = list(range(N_STRATEGIES))\n",
    "                if day_idx % 50 == 0: # Print status periodically\n",
    "                   print(f\"    Day {day_idx}: Not enough history, averaging all {N_STRATEGIES}.\")\n",
    "            else:\n",
    "                # We have history, find the leaders\n",
    "                leaderboard = []\n",
    "                for j in range(N_STRATEGIES):\n",
    "                    sharpe = calculate_sharpe_for_leaderboard(hist_window, f'returns_{j}')\n",
    "                    leaderboard.append((sharpe, j))\n",
    "                \n",
    "                leaderboard.sort(key=lambda x: x[0], reverse=True)\n",
    "                top_k_indices = [j for sharpe, j in leaderboard[:K_LEADERS]]\n",
    "                if day_idx % 60 == 0: # Print status periodically\n",
    "                    print(f\"    Day {day_idx}: Top {K_LEADERS} leaders: {top_k_indices}, Sharpes: {[f'{s:.3f}' for s, j in leaderboard[:K_LEADERS]]}\")\n",
    "\n",
    "            # --- 2b. Ensemble Position (for today) ---\n",
    "            # Get the pre-calculated signals for *all N* strategies *for today*\n",
    "            today_N_signals = N_signals_fold[day_idx, :]\n",
    "            \n",
    "            # Get signals from *only* the leaders\n",
    "            signals_from_leaders = today_N_signals[top_k_indices]\n",
    "            \n",
    "            # Average them to get the final position\n",
    "            final_signal_today = np.mean(signals_from_leaders)\n",
    "            fold_ensembled_signals.append(final_signal_today)\n",
    "\n",
    "            # --- 2c. Update Master History (for tomorrow's ranking) ---\n",
    "            # Get today's \"truth\" data (1-row DataFrame)\n",
    "            today_test_info = y_test_info_fold[day_idx]\n",
    "            \n",
    "            # Calculate the returns for *all N* individual strategies for today\n",
    "            today_returns_N = []\n",
    "            for j in range(N_STRATEGIES):\n",
    "                signal_j_today = np.array([today_N_signals[j]])\n",
    "                return_j_series = calculate_strategy_returns_pl(today_test_info, signal_j_today)\n",
    "                today_returns_N.append(return_j_series[0]) # Get the single float value\n",
    "            \n",
    "            # Build the new history row\n",
    "            new_history_row_data = {\n",
    "                \"date_id\": today_test_info[\"date_id\"][0],\n",
    "                \"forward_returns\": today_test_info[\"forward_returns\"][0],\n",
    "                \"risk_free_rate\": today_test_info[\"risk_free_rate\"][0],\n",
    "                **{f\"returns_{j}\": ret for j, ret in enumerate(today_returns_N)}\n",
    "            }\n",
    "            new_history_row_df = pl.DataFrame(new_history_row_data)\n",
    "\n",
    "            # Append to the master history_df\n",
    "            history_df = pl.concat([history_df, new_history_row_df])\n",
    "\n",
    "        # --- 3. Score Fold (after iterating all days) ---\n",
    "        print(\"  Fold iteration complete. Scoring fold...\")\n",
    "        fold_final_signals_arr = np.array(fold_ensembled_signals)\n",
    "        score = calculate_competition_score(y_test_info_fold, fold_final_signals_arr)\n",
    "        cv_scores.append(score)\n",
    "        \n",
    "        # Store for overall score calculation\n",
    "        overall_final_signals.append(fold_final_signals_arr)\n",
    "        overall_y_true.append(y_test_info_fold)\n",
    "\n",
    "    # --- 4. Final Evaluation ---\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    print(f\"\\n--- CV Finished ---\")\n",
    "    print(f\"Mean Ensembled CV Score (Daily Re-ranking): {mean_score:.4f}, std: {np.std(cv_scores):.4f}\")\n",
    "    \n",
    "    overall_y_true_df = pl.concat(overall_y_true)\n",
    "    overall_final_signals_arr = np.concatenate(overall_final_signals)\n",
    "    \n",
    "    print(\"\\n--- Overall OOS Performance (Daily Re-ranking) ---\")\n",
    "    # This score is the most important one\n",
    "    overall_score = calculate_competition_score(overall_y_true_df, overall_final_signals_arr)\n",
    "    print(f\" Overall Ensembled Score: {overall_score:.4f}\")\n",
    "    \n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51d9829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4990, 98)\n",
      "\n",
      "--- Joining weekday feature onto sliced data ---\n",
      "Initial DataFrame shape: (4990, 99)\n",
      "Base DataFrame shape after cleaning: (4990, 98)\n",
      "Generating features using generate_features_7...\n",
      "Full feature set shape: (4990, 144)\n",
      "Starting DAILY re-ranking CV with 20 strategies, following top 4 leaders.\n",
      "--- Starting Fold 1/20 (Days 250 to 486) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_128536/540012060.py:17: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.selectors.numeric().rolling_mean(window_size=5, min_periods=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Not enough history, averaging all 20.\n",
      "    Day 60: Top 4 leaders: [9, 1, 16, 14], Sharpes: ['3.215', '3.027', '2.671', '2.445']\n",
      "    Day 120: Top 4 leaders: [16, 3, 8, 6], Sharpes: ['5.801', '4.805', '4.582', '4.578']\n",
      "    Day 180: Top 4 leaders: [10, 1, 16, 12], Sharpes: ['-0.380', '-1.070', '-1.168', '-1.833']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 1 (Daily): Strat Vol: 21.13%, Mkt Vol: 13.21%, Sharpe: 0.7334, Adj Sharpe: 0.5240\n",
      "--- Starting Fold 2/20 (Days 487 to 723) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [5, 17, 10, 9], Sharpes: ['2.841', '2.346', '2.260', '2.060']\n",
      "    Day 60: Top 4 leaders: [6, 3, 7, 11], Sharpes: ['0.027', '-0.250', '-0.743', '-1.034']\n",
      "    Day 120: Top 4 leaders: [18, 14, 7, 3], Sharpes: ['0.156', '-0.117', '-0.197', '-0.545']\n",
      "    Day 180: Top 4 leaders: [14, 8, 3, 19], Sharpes: ['1.319', '-2.165', '-2.552', '-2.687']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 2 (Daily): Strat Vol: 33.31%, Mkt Vol: 21.92%, Sharpe: -1.4379, Adj Sharpe: -0.1609\n",
      "--- Starting Fold 3/20 (Days 724 to 960) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [18, 10, 4, 7], Sharpes: ['1.416', '0.396', '-0.079', '-0.565']\n",
      "    Day 60: Top 4 leaders: [1, 17, 9, 8], Sharpes: ['1.336', '1.320', '0.872', '0.634']\n",
      "    Day 120: Top 4 leaders: [12, 2, 18, 3], Sharpes: ['1.430', '0.697', '-0.467', '-0.759']\n",
      "    Day 180: Top 4 leaders: [10, 7, 19, 11], Sharpes: ['4.091', '3.288', '3.218', '2.667']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 3 (Daily): Strat Vol: 44.71%, Mkt Vol: 33.66%, Sharpe: -0.6362, Adj Sharpe: -0.1064\n",
      "--- Starting Fold 4/20 (Days 961 to 1197) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [9, 0, 3, 6], Sharpes: ['5.909', '5.742', '5.733', '5.662']\n",
      "    Day 60: Top 4 leaders: [7, 19, 0, 3], Sharpes: ['3.246', '3.174', '2.939', '2.853']\n",
      "    Day 120: Top 4 leaders: [19, 13, 14, 15], Sharpes: ['1.696', '1.146', '0.127', '-0.003']\n",
      "    Day 180: Top 4 leaders: [11, 19, 16, 7], Sharpes: ['-0.209', '-0.312', '-0.918', '-1.544']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 4 (Daily): Strat Vol: 23.69%, Mkt Vol: 18.61%, Sharpe: 0.5467, Adj Sharpe: 0.5093\n",
      "--- Starting Fold 5/20 (Days 1198 to 1434) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [10, 1, 11, 17], Sharpes: ['1.374', '1.262', '1.227', '0.618']\n",
      "    Day 60: Top 4 leaders: [12, 9, 0, 7], Sharpes: ['5.852', '5.668', '4.638', '4.543']\n",
      "    Day 120: Top 4 leaders: [5, 8, 9, 19], Sharpes: ['5.368', '5.234', '5.085', '5.005']\n",
      "    Day 180: Top 4 leaders: [18, 13, 14, 15], Sharpes: ['3.510', '3.271', '2.800', '2.632']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 5 (Daily): Strat Vol: 18.26%, Mkt Vol: 13.14%, Sharpe: 1.7273, Adj Sharpe: 1.4511\n",
      "--- Starting Fold 6/20 (Days 1435 to 1671) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [8, 0, 6, 10], Sharpes: ['1.532', '0.841', '0.800', '0.750']\n",
      "    Day 60: Top 4 leaders: [15, 6, 16, 0], Sharpes: ['3.736', '2.611', '2.309', '2.250']\n",
      "    Day 120: Top 4 leaders: [1, 17, 18, 0], Sharpes: ['1.773', '1.310', '1.258', '1.046']\n",
      "    Day 180: Top 4 leaders: [16, 17, 19, 15], Sharpes: ['1.888', '1.625', '1.456', '1.224']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 6 (Daily): Strat Vol: 35.20%, Mkt Vol: 22.43%, Sharpe: 0.3047, Adj Sharpe: 0.2225\n",
      "--- Starting Fold 7/20 (Days 1672 to 1908) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [5, 19, 15, 16], Sharpes: ['1.245', '1.191', '1.167', '0.749']\n",
      "    Day 60: Top 4 leaders: [2, 15, 12, 19], Sharpes: ['3.272', '3.057', '2.966', '2.820']\n",
      "    Day 120: Top 4 leaders: [11, 12, 10, 2], Sharpes: ['1.982', '1.721', '1.568', '1.482']\n",
      "    Day 180: Top 4 leaders: [8, 6, 14, 7], Sharpes: ['4.397', '3.080', '3.010', '2.995']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 7 (Daily): Strat Vol: 18.10%, Mkt Vol: 11.85%, Sharpe: 1.6652, Adj Sharpe: 1.2546\n",
      "--- Starting Fold 8/20 (Days 1909 to 2145) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [10, 11, 4, 6], Sharpes: ['4.251', '4.204', '4.125', '3.497']\n",
      "    Day 60: Top 4 leaders: [14, 18, 9, 7], Sharpes: ['1.053', '1.024', '0.704', '0.549']\n",
      "    Day 120: Top 4 leaders: [19, 10, 3, 17], Sharpes: ['5.134', '5.044', '4.526', '4.460']\n",
      "    Day 180: Top 4 leaders: [11, 18, 7, 16], Sharpes: ['2.856', '2.641', '1.961', '1.427']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 8 (Daily): Strat Vol: 17.01%, Mkt Vol: 10.99%, Sharpe: 1.6522, Adj Sharpe: 1.2258\n",
      "--- Starting Fold 9/20 (Days 2146 to 2382) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [13, 8, 10, 18], Sharpes: ['2.320', '2.142', '2.075', '1.731']\n",
      "    Day 60: Top 4 leaders: [12, 14, 9, 10], Sharpes: ['1.152', '0.972', '0.868', '0.539']\n",
      "    Day 120: Top 4 leaders: [5, 14, 4, 6], Sharpes: ['2.755', '1.943', '1.464', '1.164']\n",
      "    Day 180: Top 4 leaders: [8, 13, 6, 5], Sharpes: ['2.937', '1.359', '0.940', '0.761']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 9 (Daily): Strat Vol: 17.52%, Mkt Vol: 11.69%, Sharpe: 1.0662, Adj Sharpe: 0.8207\n",
      "--- Starting Fold 10/20 (Days 2383 to 2619) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [11, 6, 7, 5], Sharpes: ['3.229', '1.255', '1.120', '1.027']\n",
      "    Day 60: Top 4 leaders: [19, 7, 6, 5], Sharpes: ['0.842', '0.726', '0.409', '0.170']\n",
      "    Day 120: Top 4 leaders: [8, 7, 1, 3], Sharpes: ['3.260', '3.095', '2.831', '2.752']\n",
      "    Day 180: Top 4 leaders: [15, 4, 13, 3], Sharpes: ['-1.775', '-2.599', '-2.884', '-2.906']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 10 (Daily): Strat Vol: 28.58%, Mkt Vol: 16.89%, Sharpe: 0.1531, Adj Sharpe: 0.1026\n",
      "--- Starting Fold 11/20 (Days 2620 to 2856) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [3, 6, 12, 5], Sharpes: ['5.616', '5.518', '5.414', '5.251']\n",
      "    Day 60: Top 4 leaders: [14, 7, 9, 3], Sharpes: ['3.221', '2.063', '1.261', '1.245']\n",
      "    Day 120: Top 4 leaders: [7, 18, 3, 16], Sharpes: ['1.145', '0.759', '0.578', '0.229']\n",
      "    Day 180: Top 4 leaders: [7, 10, 5, 16], Sharpes: ['5.019', '4.769', '4.600', '4.408']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 11 (Daily): Strat Vol: 17.03%, Mkt Vol: 9.95%, Sharpe: 1.9709, Adj Sharpe: 1.3050\n",
      "--- Starting Fold 12/20 (Days 2857 to 3093) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [3, 8, 2, 12], Sharpes: ['5.286', '5.161', '4.677', '4.601']\n",
      "    Day 60: Top 4 leaders: [11, 18, 0, 15], Sharpes: ['4.950', '3.988', '3.808', '3.670']\n",
      "    Day 120: Top 4 leaders: [15, 8, 10, 1], Sharpes: ['3.519', '1.823', '1.265', '1.162']\n",
      "    Day 180: Top 4 leaders: [17, 15, 13, 11], Sharpes: ['4.690', '4.688', '4.269', '4.213']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 12 (Daily): Strat Vol: 15.09%, Mkt Vol: 10.09%, Sharpe: 1.7989, Adj Sharpe: 1.3881\n",
      "--- Starting Fold 13/20 (Days 3094 to 3330) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [14, 7, 10, 6], Sharpes: ['2.929', '2.035', '1.992', '0.973']\n",
      "    Day 60: Top 4 leaders: [4, 1, 8, 17], Sharpes: ['2.718', '2.669', '2.073', '2.051']\n",
      "    Day 120: Top 4 leaders: [6, 15, 11, 14], Sharpes: ['3.964', '3.638', '3.571', '2.904']\n",
      "    Day 180: Top 4 leaders: [4, 0, 1, 12], Sharpes: ['-0.755', '-1.668', '-1.672', '-1.732']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 13 (Daily): Strat Vol: 27.54%, Mkt Vol: 16.21%, Sharpe: -0.4609, Adj Sharpe: -0.1410\n",
      "--- Starting Fold 14/20 (Days 3331 to 3567) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [14, 7, 5, 16], Sharpes: ['2.432', '2.099', '1.680', '1.466']\n",
      "    Day 60: Top 4 leaders: [10, 17, 18, 7], Sharpes: ['5.344', '4.874', '4.700', '4.600']\n",
      "    Day 120: Top 4 leaders: [5, 16, 19, 15], Sharpes: ['7.388', '6.504', '6.451', '6.433']\n",
      "    Day 180: Top 4 leaders: [15, 17, 1, 11], Sharpes: ['3.181', '2.619', '2.506', '2.410']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 14 (Daily): Strat Vol: 19.62%, Mkt Vol: 11.79%, Sharpe: 1.7012, Adj Sharpe: 1.1620\n",
      "--- Starting Fold 15/20 (Days 3568 to 3804) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [4, 16, 9, 13], Sharpes: ['6.472', '5.944', '5.344', '5.019']\n",
      "    Day 60: Top 4 leaders: [4, 7, 11, 3], Sharpes: ['-1.313', '-1.370', '-1.588', '-1.643']\n",
      "    Day 120: Top 4 leaders: [16, 12, 5, 4], Sharpes: ['3.607', '2.846', '2.680', '2.631']\n",
      "    Day 180: Top 4 leaders: [12, 7, 1, 8], Sharpes: ['3.558', '1.011', '0.764', '0.650']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 15 (Daily): Strat Vol: 42.51%, Mkt Vol: 26.01%, Sharpe: 0.4652, Adj Sharpe: 0.3162\n",
      "--- Starting Fold 16/20 (Days 3805 to 4041) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [2, 6, 7, 10], Sharpes: ['3.294', '3.207', '3.174', '3.085']\n",
      "    Day 60: Top 4 leaders: [5, 16, 18, 0], Sharpes: ['2.364', '2.043', '1.950', '1.130']\n",
      "    Day 120: Top 4 leaders: [18, 10, 11, 16], Sharpes: ['1.646', '1.620', '1.385', '1.367']\n",
      "    Day 180: Top 4 leaders: [16, 5, 18, 2], Sharpes: ['2.496', '2.032', '1.677', '1.611']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 16 (Daily): Strat Vol: 20.91%, Mkt Vol: 12.50%, Sharpe: 1.3969, Adj Sharpe: 0.9483\n",
      "--- Starting Fold 17/20 (Days 4042 to 4278) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [16, 18, 0, 14], Sharpes: ['4.181', '2.352', '2.310', '2.297']\n",
      "    Day 60: Top 4 leaders: [1, 8, 10, 18], Sharpes: ['-1.420', '-2.065', '-2.119', '-2.475']\n",
      "    Day 120: Top 4 leaders: [11, 1, 14, 9], Sharpes: ['-3.025', '-3.353', '-3.490', '-3.499']\n",
      "    Day 180: Top 4 leaders: [4, 14, 19, 18], Sharpes: ['4.872', '3.585', '3.578', '3.563']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 17 (Daily): Strat Vol: 38.55%, Mkt Vol: 23.74%, Sharpe: -1.1671, Adj Sharpe: -0.1066\n",
      "--- Starting Fold 18/20 (Days 4279 to 4515) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [8, 16, 2, 4], Sharpes: ['-0.029', '-0.478', '-0.906', '-1.057']\n",
      "    Day 60: Top 4 leaders: [6, 18, 3, 16], Sharpes: ['2.553', '1.591', '1.564', '1.526']\n",
      "    Day 120: Top 4 leaders: [5, 16, 4, 11], Sharpes: ['0.976', '0.634', '0.583', '0.536']\n",
      "    Day 180: Top 4 leaders: [5, 11, 4, 2], Sharpes: ['4.753', '4.713', '4.610', '4.592']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 18 (Daily): Strat Vol: 26.97%, Mkt Vol: 14.79%, Sharpe: 0.5552, Adj Sharpe: 0.3421\n",
      "--- Starting Fold 19/20 (Days 4516 to 4752) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [13, 17, 7, 11], Sharpes: ['1.498', '-1.710', '-1.722', '-1.928']\n",
      "    Day 60: Top 4 leaders: [15, 4, 5, 8], Sharpes: ['4.431', '4.025', '3.991', '3.585']\n",
      "    Day 120: Top 4 leaders: [12, 9, 2, 16], Sharpes: ['0.766', '0.726', '0.568', '0.468']\n",
      "    Day 180: Top 4 leaders: [6, 10, 0, 19], Sharpes: ['5.386', '5.259', '5.056', '4.923']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 19 (Daily): Strat Vol: 21.93%, Mkt Vol: 12.39%, Sharpe: 2.3821, Adj Sharpe: 1.5172\n",
      "--- Starting Fold 20/20 (Days 4753 to 4989) ---\n",
      "  Iterating daily and re-ranking leaders...\n",
      "    Day 0: Top 4 leaders: [4, 18, 17, 14], Sharpes: ['4.380', '4.252', '4.150', '4.139']\n",
      "    Day 60: Top 4 leaders: [7, 0, 18, 19], Sharpes: ['1.838', '1.632', '1.537', '1.452']\n",
      "    Day 120: Top 4 leaders: [15, 1, 11, 13], Sharpes: ['-1.997', '-2.322', '-2.339', '-2.369']\n",
      "    Day 180: Top 4 leaders: [2, 10, 0, 7], Sharpes: ['4.273', '4.174', '4.069', '4.056']\n",
      "  Fold iteration complete. Scoring fold...\n",
      "  Fold 20 (Daily): Strat Vol: 28.09%, Mkt Vol: 16.28%, Sharpe: 0.2989, Adj Sharpe: 0.1937\n",
      "\n",
      "--- CV Finished ---\n",
      "Mean Ensembled CV Score (Daily Re-ranking): 0.6384, std: 0.5832\n",
      "\n",
      "--- Overall OOS Performance (Daily Re-ranking) ---\n",
      "  Fold 20 (Daily): Strat Vol: 27.21%, Mkt Vol: 17.49%, Sharpe: 0.4344, Adj Sharpe: 0.3205\n",
      " Overall Ensembled Score: 0.3205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3204867667262339"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def evaluate(excessarg: int) -> float:\n",
    "    \"\"\"\n",
    "    Main evaluation function for FunSearch. It loads the data\n",
    "    and runs the solver which performs cross-validation.\n",
    "    \"\"\"\n",
    "    full_train_df = pl.read_csv(TRAIN_DATA_PATH)\n",
    "    # Use a slice of data for faster evaluation runs during development\n",
    "    df_raw = full_train_df.slice(4000)\n",
    "    print(df_raw.shape)\n",
    "\n",
    "    #fill nulls in df with mean\n",
    "    df = df_raw.with_columns(\n",
    "        # Select all numeric columns for the operation\n",
    "        pl.selectors.numeric()\n",
    "          # Step 1: Attempt to fill with the rolling mean of each respective column\n",
    "          .fill_null(\n",
    "              pl.selectors.numeric().rolling_mean(window_size=5, min_periods=1)\n",
    "          )\n",
    "          # Step 2: Fall back to the global column mean for any remaining nulls\n",
    "          #.fill_null(strategy='mean')\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "    pl.col(\"date_id\").cast(pl.Int64)\n",
    "    )\n",
    "    \n",
    "    weekday_df = add_weekday_column(SPY_DATA_PATH)\n",
    "    print(\"\\n--- Joining weekday feature onto sliced data ---\")\n",
    "    # Join the weekday information onto the sliced training data.\n",
    "    # A 'left' join ensures we keep all rows from the original `df`.\n",
    "    df_with_features = df.join(weekday_df, on=\"date_id\", how=\"left\")\n",
    "    # print(\"DataFrame after join:\")\n",
    "    # print(df_with_features.shape)\n",
    "    return solve(df_with_features)\n",
    "  \n",
    "def add_weekday_column(input_csv_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file, adds a 'weekday' column based on the 'Date' column,\n",
    "    and saves the result to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): The path to the source CSV file.\n",
    "        output_csv_path (str): The path where the output CSV will be saved.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a Polars DataFrame\n",
    "    df = pl.read_csv(input_csv_path)\n",
    "\n",
    "    # Add a new column named 'weekday'\n",
    "    # 1. Select the 'Date' column.\n",
    "    # 2. Convert the string representation to a proper date type.\n",
    "    # 3. Use the .dt.weekday() function to get the day of the week (Monday=1, Sunday=7).\n",
    "    # 4. Alias the new expression to 'weekday'.\n",
    "    df_with_weekday = df.with_columns(\n",
    "        pl.col(\"Date\").str.to_date().dt.weekday().alias(\"weekday\")\n",
    "    )\n",
    "\n",
    "    # Print the transformed DataFrame to the console to show the result\n",
    "    returned_df = df_with_weekday.select([\"date_id\", \"weekday\"])\n",
    "    return returned_df\n",
    "\n",
    "evaluate(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
