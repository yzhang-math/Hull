{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8352e561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647429097/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Copying your exact evaluation function ---\n",
    "# We will use this to score our agent at the end of each training epoch.\n",
    "def calculate_competition_score(y_true_df: pd.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "    ''' Calculates the competition score based on true values and predicted signals. '''\n",
    "    solution = y_true_df.copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "    solution['position'] = y_pred_signals\n",
    "    solution['strategy_returns'] = (\n",
    "        solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "        solution['position'] * solution['forward_returns']\n",
    "    )\n",
    "    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "    strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "    strategy_std = solution['strategy_returns'].std()\n",
    "    \n",
    "    # Handle zero std dev (e.g., if agent learns to always output 0)\n",
    "    if strategy_std < 1e-8: return 0.0 \n",
    "    \n",
    "    trading_days_per_yr = 252\n",
    "    sharpe = strategy_geo_mean / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "    market_std = solution['forward_returns'].std()\n",
    "    market_volatility = market_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    strategy_volatility = strategy_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    \n",
    "    # Handle zero market vol\n",
    "    if market_volatility < 1e-8: \n",
    "        excess_vol = 0.0\n",
    "    else:\n",
    "        excess_vol = max(0, strategy_volatility / market_volatility - 1.2)\n",
    "        \n",
    "    vol_penalty = 1 + excess_vol\n",
    "    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "    market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "    return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * trading_days_per_yr)\n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    \n",
    "    # print(f\"Strategy Vol: {strategy_volatility:.2f}%, Market Vol: {market_volatility:.2f}%, Sharpe: {sharpe:.4f}, Adj Sharpe: {adjusted_sharpe:.4f}\")\n",
    "    return adjusted_sharpe\n",
    "\n",
    "# class RlTradingEnv:\n",
    "#     \"\"\"\n",
    "#     An RL environment built from your data pipeline.\n",
    "    \n",
    "#     It provides a per-step reward based on the 'target' column.\n",
    "#     It uses your 'calculate_competition_score' for final evaluation.\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     An RL environment built from your data pipeline.\n",
    "#     (With nan_fallback_counter)\n",
    "#     \"\"\"\n",
    "#     def __init__(self, features, targets, scorer_info, transaction_cost=0.0001):\n",
    "#         self.features = features.to_numpy()\n",
    "#         self.targets = targets.to_numpy().flatten()\n",
    "#         self.scorer_info_df = scorer_info.to_pandas() \n",
    "        \n",
    "#         self.transaction_cost = transaction_cost\n",
    "#         self.reward_scale = 100.0  # Let's keep the scaling for when it works\n",
    "        \n",
    "#         self.n_steps = len(self.features)\n",
    "#         self.n_features = self.features.shape[1]\n",
    "#         self.action_space_dim = 1\n",
    "#         self.max_action = 2.0\n",
    "        \n",
    "#         self.current_step = 0\n",
    "#         self.last_leverage = 0.0\n",
    "        \n",
    "#         # --- ADD COUNTER ---\n",
    "#         self.nan_fallback_counter = 0\n",
    "\n",
    "#     def reset(self):\n",
    "#         \"\"\"Resets the environment and returns the first state.\"\"\"\n",
    "#         self.current_step = 0\n",
    "#         self.last_leverage = 0.0\n",
    "        \n",
    "#         # --- RESET COUNTER ---\n",
    "#         self.nan_fallback_counter = 0\n",
    "#         return self.features[self.current_step]\n",
    "\n",
    "#     def step(self, action_leverage):\n",
    "#         \"\"\"\n",
    "#         Takes an action, calculates reward, and returns the next state.\n",
    "#         (With NaN/inf failsafe and counter)\n",
    "#         \"\"\"\n",
    "#         if self.current_step >= self.n_steps - 2:\n",
    "#             return self.features[self.current_step], 0.0, True\n",
    "\n",
    "#         leverage = np.clip(action_leverage, 0.0, self.max_action)[0]\n",
    "#         target_excess_return = self.targets[self.current_step]\n",
    "        \n",
    "#         cost = self.transaction_cost * abs(leverage - self.last_leverage)\n",
    "#         reward = (leverage * target_excess_return) - cost\n",
    "        \n",
    "#         if np.isnan(reward) or np.isinf(reward):\n",
    "#             reward = 0.0 \n",
    "#             # --- INCREMENT COUNTER ---\n",
    "#             self.nan_fallback_counter += 1\n",
    "\n",
    "#         self.last_leverage = leverage\n",
    "#         self.current_step += 1\n",
    "#         next_state = self.features[self.current_step]\n",
    "#         done = (self.current_step == self.n_steps - 2)\n",
    "        \n",
    "#         return next_state, (reward * self.reward_scale), done\n",
    "\n",
    "\n",
    "#     def run_evaluation(self, policy_actor):\n",
    "#         \"\"\"\n",
    "#         Runs a full backtest with a given policy (actor)\n",
    "#         and returns the final adjusted Sharpe score.\n",
    "#         \"\"\"\n",
    "#         state = self.reset()\n",
    "#         signals = []\n",
    "#         # --- FIX IS HERE ---\n",
    "#         # If we have a torch actor, get its device (e.g., 'cuda:0')\n",
    "#         actor_device = None\n",
    "#         if isinstance(policy_actor, Actor):\n",
    "#             actor_device = next(policy_actor.parameters()).device\n",
    "#         # --- END FIX ---\n",
    "        \n",
    "#         for t in range(self.n_steps - 1):\n",
    "#             if isinstance(policy_actor, Actor): # It's our MLP/TD3 actor\n",
    "#                 # --- FIX IS HERE ---\n",
    "#                 # Create tensor and move it to the actor's device\n",
    "#                 state_tensor = torch.FloatTensor(state).unsqueeze(0).to(actor_device)\n",
    "#                 # --- END FIX ---\n",
    "#                 #state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "#                 action = policy_actor(state_tensor).cpu().data.numpy().flatten()\n",
    "#             else: # It's our XGBoost agent\n",
    "#                 action = [policy_actor.select_action(state, exploration_rate=0.0)]\n",
    "                \n",
    "#             signals.append(np.clip(action, 0.0, self.max_action)[0])\n",
    "            \n",
    "#             # We don't need reward here, just the next state\n",
    "#             if t < self.n_steps - 2:\n",
    "#                 state = self.features[t + 1]\n",
    "        \n",
    "#         # Score the *entire* run\n",
    "#         # We need to trim the scorer_info_df to match the length of signals\n",
    "#         scorer_df_trimmed = self.scorer_info_df.iloc[:len(signals)]\n",
    "#         return calculate_competition_score(scorer_df_trimmed, np.array(signals))\n",
    "\n",
    "class RlTradingEnv:\n",
    "    \"\"\"\n",
    "    An RL environment with a HYBRID reward:\n",
    "    (1 - lambda) * DSR_reward + (lambda) * Return_reward\n",
    "    \"\"\"\n",
    "    def __init__(self, features, targets, scorer_info, transaction_cost=0.0001):\n",
    "        self.features = features.to_numpy()\n",
    "        self.targets = targets.to_numpy().flatten()\n",
    "        self.scorer_info_df = scorer_info.to_pandas() \n",
    "        \n",
    "        self.transaction_cost = transaction_cost\n",
    "        \n",
    "        self.n_steps = len(self.features)\n",
    "        self.n_features = self.features.shape[1]\n",
    "        self.action_space_dim = 1\n",
    "        self.max_action = 2.0\n",
    "        \n",
    "        # --- REWARD HYPERPARAMETERS ---\n",
    "        # 0.0 = all DSR, 1.0 = all Return\n",
    "        self.reward_lambda = 1\n",
    "        self.return_reward_scale = 100.0 # Scale for the \"fast\" return reward\n",
    "        \n",
    "        # --- DSR STATE VARIABLES ---\n",
    "        self.dsr_eta = 0.01  # Learning rate for the EMAs\n",
    "        self.dsr_a = 0.0     # EMA of returns\n",
    "        self.dsr_b = 0.0     # EMA of squared returns\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.last_leverage = 0.0\n",
    "        self.nan_fallback_counter = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment and returns the first state.\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.last_leverage = 0.0\n",
    "        self.nan_fallback_counter = 0\n",
    "        \n",
    "        self.reward_lambda = self.reward_lambda/1.02\n",
    "        \n",
    "        # --- RESET DSR STATE ---\n",
    "        self.dsr_a = 0.0\n",
    "        self.dsr_b = 1e-4 # Small value to avoid div by zero\n",
    "        \n",
    "        return self.features[self.current_step]\n",
    "\n",
    "    def _calculate_dsr(self, R_p):\n",
    "        \"\"\"\n",
    "        Calculates the Differential Sharpe Ratio for a given portfolio return.\n",
    "        This is the \"slow\" risk-adjusted reward.\n",
    "        \"\"\"\n",
    "        # 1. Update the EMAs\n",
    "        self.dsr_a = self.dsr_a + self.dsr_eta * (R_p - self.dsr_a)\n",
    "        self.dsr_b = self.dsr_b + self.dsr_eta * (R_p**2 - self.dsr_b)\n",
    "        \n",
    "        std_dev = np.sqrt(self.dsr_b - self.dsr_a**2)\n",
    "        \n",
    "        if std_dev < 1e-8:\n",
    "            return 0.0\n",
    "\n",
    "        # 3. Calculate the DSR reward\n",
    "        dsr_reward = ( (R_p - self.dsr_a) * self.dsr_b - 0.5 * (R_p**2 - self.dsr_b) * self.dsr_a ) \\\n",
    "                     / ( (self.dsr_b - self.dsr_a**2)**1.5 + 1e-6)\n",
    "        \n",
    "        return dsr_reward\n",
    "\n",
    "    def step(self, action_leverage):\n",
    "        \"\"\"\n",
    "        Takes an action, calculates reward, and returns the next state.\n",
    "        (With HYBRID reward)\n",
    "        \"\"\"\n",
    "        if self.current_step >= self.n_steps - 2:\n",
    "            return self.features[self.current_step], 0.0, True\n",
    "\n",
    "        leverage = np.clip(action_leverage, 0.0, self.max_action)[0]\n",
    "        target_excess_return = self.targets[self.current_step]\n",
    "        \n",
    "        # --- 1. Calculate Portfolio Return (R_p) ---\n",
    "        cost = self.transaction_cost * abs(leverage - self.last_leverage)\n",
    "        portfolio_return = (leverage * target_excess_return) - cost\n",
    "        \n",
    "        # --- 2. Handle NaNs (Failsafe) ---\n",
    "        if np.isnan(portfolio_return) or np.isinf(portfolio_return):\n",
    "            portfolio_return = 0.0 \n",
    "            self.nan_fallback_counter += 1\n",
    "\n",
    "        # --- 3. Calculate the two reward components ---\n",
    "        \n",
    "        # \"Fast\" Reward: Simple scaled return\n",
    "        # This encourages the agent to make *any* profitable trade\n",
    "        fast_reward = portfolio_return * self.return_reward_scale\n",
    "        \n",
    "        # \"Slow\" Reward: DSR\n",
    "        # This encourages the agent to make *risk-adjusted* trades\n",
    "        slow_reward = self._calculate_dsr(portfolio_return)\n",
    "        \n",
    "        # Handle NaN in DSR calculation (though unlikely if R_p is clean)\n",
    "        if np.isnan(slow_reward) or np.isinf(slow_reward):\n",
    "            slow_reward = 0.0\n",
    "\n",
    "        # --- 4. Combine rewards ---\n",
    "        final_reward = (1.0 - self.reward_lambda) * slow_reward + self.reward_lambda * fast_reward\n",
    "\n",
    "        if leverage < 1e-3:\n",
    "            final_reward -= 0.1  # Penalize doing nothing a bit\n",
    "        \n",
    "        # 5. Update state\n",
    "        self.last_leverage = leverage\n",
    "        self.current_step += 1\n",
    "        next_state = self.features[self.current_step]\n",
    "        done = (self.current_step == self.n_steps - 2)\n",
    "        \n",
    "        return next_state, final_reward, done\n",
    "\n",
    "    # ... (run_evaluation is unchanged) ...\n",
    "    def run_evaluation(self, policy_actor):\n",
    "        \"\"\"\n",
    "        Runs a full backtest with a given policy (actor)\n",
    "        and returns the final adjusted Sharpe score.\n",
    "        \"\"\"\n",
    "        state = self.reset()\n",
    "        signals = []\n",
    "        # --- FIX IS HERE ---\n",
    "        # If we have a torch actor, get its device (e.g., 'cuda:0')\n",
    "        actor_device = None\n",
    "        if isinstance(policy_actor, Actor):\n",
    "            actor_device = next(policy_actor.parameters()).device\n",
    "        # --- END FIX ---\n",
    "        \n",
    "        for t in range(self.n_steps - 1):\n",
    "            if isinstance(policy_actor, Actor): # It's our MLP/TD3 actor\n",
    "                # --- FIX IS HERE ---\n",
    "                # Create tensor and move it to the actor's device\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(actor_device)\n",
    "                # --- END FIX ---\n",
    "                #state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                action = policy_actor(state_tensor).cpu().data.numpy().flatten()\n",
    "            else: # It's our XGBoost agent\n",
    "                action = [policy_actor.select_action(state, exploration_rate=0.0)]\n",
    "                \n",
    "            signals.append(np.clip(action, 0.0, self.max_action)[0])\n",
    "            \n",
    "            # We don't need reward here, just the next state\n",
    "            if t < self.n_steps - 2:\n",
    "                state = self.features[t + 1]\n",
    "        \n",
    "        # Score the *entire* run\n",
    "        # We need to trim the scorer_info_df to match the length of signals\n",
    "        scorer_df_trimmed = self.scorer_info_df.iloc[:len(signals)]\n",
    "        return calculate_competition_score(scorer_df_trimmed, np.array(signals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3074703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Replay Buffer (for off-policy learning) ---\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A simple replay buffer, as requested for 'random subset' training.\"\"\"\n",
    "    def __init__(self, max_size=1_000_000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (np.array(states), np.array(actions), \n",
    "                np.array(rewards).reshape(-1, 1), \n",
    "                np.array(next_states), \n",
    "                np.array(dones).reshape(-1, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- Data Loading (Adapted from your notebook) ---\n",
    "# We need to copy the feature generation functions from your notebook\n",
    "# I'll just copy generate_features_7 as it's the last one you used.\n",
    "\n",
    "def generate_features_7 (df: pl.DataFrame) -> pl.DataFrame:\n",
    "  \"\"\"Generates new features from the base data.\n",
    "    This function is the target of the evolutionary algorithm.\n",
    "  \n",
    "    Available Feature Categories:\n",
    "    - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "    - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "    - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "    - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "    - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "    - S* (Sentiment features): 12 columns (S1-S12)\n",
    "    - V* (Volatility features): 13 columns (V1-V13)\n",
    "  \"\"\"\n",
    "  new_features = pl.DataFrame({\n",
    "      # --- 20 Pairwise Interactions ---\n",
    "      'feat_M1_x_V1': df['M1'] * df['V1'],\n",
    "      'feat_P1_add_E1': df['P1'] + df['E1'],\n",
    "      'feat_S1_sub_I1': df['S1'] - df['I1'],\n",
    "      'feat_M10_div_V10': df['M10'] / (df['V10'] + 1e-6),\n",
    "      'feat_P10_x_E10': df['P10'] * df['E10'],\n",
    "      'feat_M2_x_S3': df['M2'] * df['S3'],\n",
    "      'feat_V2_div_P2': df['V2'] / (df['P2'] + 1e-6),\n",
    "      'feat_E4_sub_I3': df['E4'] - df['I3'],\n",
    "      'feat_S7_add_M12': df['S7'] + df['M12'],\n",
    "      'feat_I5_x_V11': df['I5'] * df['V11'],\n",
    "      'feat_P5_div_S8': df['P5'] / (df['S8'] + 1e-6),\n",
    "      'feat_E12_x_I9': df['E12'] * df['I9'],\n",
    "      'feat_M1_div_S1': df['M1'] / (df['S1'] + 1e-6),\n",
    "      'feat_V1_add_P1': df['V1'] + df['P1'],\n",
    "      'feat_E1_sub_I1': df['E1'] - df['I1'],\n",
    "      'feat_M2_div_V2': df['M2'] / (df['V2'] + 1e-6),\n",
    "      'feat_P2_x_S3': df['P2'] * df['S3'],\n",
    "      'feat_E4_add_M10': df['E4'] + df['M10'],\n",
    "      'feat_I3_sub_V10': df['I3'] - df['V10'],\n",
    "      'feat_S7_x_P10': df['S7'] * df['P10'],\n",
    "      # --- 10 Rolling Window Features ---\n",
    "      'feat_V2_roll_mean_5': df['V2'].rolling_mean(window_size=5),\n",
    "      'feat_V1_roll_std_5': df['V1'].rolling_std(window_size=5),\n",
    "      'feat_M1_roll_mean_20': df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_M3_roll_std_20': df['M3'].rolling_std(window_size=20),\n",
    "      'feat_P1_roll_max_10': df['P1'].rolling_max(window_size=10),\n",
    "      'feat_P1_roll_min_10': df['P1'].rolling_min(window_size=10),\n",
    "      'feat_E5_roll_mean_50': df['E5'].rolling_mean(window_size=50),\n",
    "      'feat_S1_roll_std_50': df['S1'].rolling_std(window_size=50),\n",
    "      'feat_I1_roll_mean_10': df['I1'].rolling_mean(window_size=10),\n",
    "      'feat_V10_roll_std_10': df['V10'].rolling_std(window_size=10),\n",
    "      # --- 10 Complex Interactions (3+ elements) ---\n",
    "      'feat_M1_V1_div_P1': (df['M1'] * df['V1']) / (df['P1'] + 1e-6),\n",
    "      'feat_E1_S1_add_I1': df['E1'] + df['S1'] - df['I1'],\n",
    "      'feat_M2_P2_sub_V2': df['M2'] + df['P2'] - df['V2'],\n",
    "      'feat_S7_div_E4_I3': df['S7'] / (df['E4'] + df['I3'] + 1e-6),\n",
    "      'feat_P5_x_M10_x_V10': df['P5'] * df['M10'] * df['V10'],\n",
    "      'feat_roll_diff_M1_5_20': df['M1'].rolling_mean(window_size=5) - df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_roll_diff_V1_5_20': df['V1'].rolling_mean(window_size=5) - df['V1'].rolling_mean(window_size=20),\n",
    "      'feat_M_S_P_combo': (df['M12'] - df['M1']) / (df['S1'] + df['P1'] + 1e-6),\n",
    "      'feat_V_E_I_combo': (df['V11'] + df['V2']) * (df['E1'] - df['I1']),\n",
    "      'feat_ratio_of_ratios': (df['M1']/(df['V1']+1e-6)) / (df['P1']/(df['S1']+1e-6)),\n",
    "      # --- 10 New Features ---\n",
    "      'feat_M1_x_V1_x_P1': df['M1'] * df['V1'] * df['P1'],\n",
    "      'feat_E1_div_S1': df['E1'] / (df['S1'] + 1e-6),\n",
    "      'feat_I1_sub_V1': df['I1'] - df['V1'],\n",
    "      'feat_M10_add_V10': df['M10'] + df['V10'],\n",
    "      'feat_P10_div_E10': df['P10'] / (df['E10'] + 1e-6),\n",
    "      'feat_M2_add_S3': df['M2'] + df['S3'],\n",
    "      'feat_V2_x_P2': df['V2'] * df['P2'],\n",
    "      'feat_E4_add_I3': df['E4'] + df['I3'],\n",
    "      'feat_S7_div_M12': df['S7'] / (df['M12'] + 1e-6),\n",
    "      'feat_I5_div_V11': df['I5'] / (df['V11'] + 1e-6),\n",
    "      #'feat_M1_log_P1': np.log(df['M1'] + 1e-6) / np.log(df['P1'] + 1e-6),\n",
    "      # --- SAFER LOGIC HERE ---\n",
    "      #'feat_M1_log_P1': pl.when( (df['M1'] > 0) & (df['P1'] > 0) & (df['P1'] != 1) ).then( df['M1'].log() / df['P1'].log() ).otherwise(0),\n",
    "      # --- END SAFER LOGIC ---\n",
    "  })\n",
    "  # Fill any nulls created by rolling windows\n",
    "  return new_features.with_columns(pl.all().forward_fill())\n",
    "\n",
    "\n",
    "\n",
    "def load_and_prep_data(train_path, spy_path, slice_start=2000, test_days=252):\n",
    "    \"\"\"\n",
    "    Loads and processes data just like your notebook.\n",
    "    (Robust version to handle NaNs and infs)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load base data and slice\n",
    "    full_train_df = pl.read_csv(train_path)\n",
    "    df_raw = full_train_df.slice(slice_start)\n",
    "    \n",
    "    # 2. Basic cleaning from notebook\n",
    "    # Use fill_null(0.0) instead of 'mean' to avoid propagating bad data\n",
    "    df = df_raw.with_columns(pl.selectors.numeric().fill_null(0.0))\n",
    "    df = df.with_columns(pl.col(\"date_id\").cast(pl.Int64))\n",
    "\n",
    "    # 3. Add weekday feature\n",
    "    spy_df = pl.read_csv(spy_path)\n",
    "    weekday_df = spy_df.with_columns(\n",
    "        pl.col(\"Date\").str.to_date().dt.weekday().alias(\"weekday\")\n",
    "    ).select([\"date_id\", \"weekday\"])\n",
    "    \n",
    "    df_with_weekday = df.join(weekday_df, on=\"date_id\", how=\"left\")\n",
    "    \n",
    "    # 4. Prep for feature generation\n",
    "    base_df = df_with_weekday.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    \n",
    "    if 'E7' in base_df.columns:\n",
    "        base_df = base_df.drop('E7')\n",
    "        \n",
    "    base_df = base_df.with_columns(pl.all().forward_fill().backward_fill())\n",
    "\n",
    "    # 5. Generate and combine features (using corrected function)\n",
    "    new_features_df = generate_features_7(base_df)\n",
    "    processed_df = pl.concat([base_df, new_features_df], how=\"horizontal\")\n",
    "    \n",
    "    # 6. Finalize X, y, and scorer_info\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_feature_names = new_features_df.columns\n",
    "    ALL_FEATURES = base_features + new_feature_names\n",
    "    \n",
    "    X = processed_df.select(ALL_FEATURES)\n",
    "    y = processed_df.select(\"target\")\n",
    "    scorer_info_df = processed_df.select([\"forward_returns\", \"risk_free_rate\"])\n",
    "    \n",
    "    # --- FINAL ROBUST CLEANING ---\n",
    "    Xy = pl.concat([X, y, scorer_info_df], how=\"horizontal\")\n",
    "    Xy = Xy.with_columns(pl.all().replace([np.inf, -np.inf], None))\n",
    "    original_rows = Xy.height\n",
    "    Xy = Xy.drop_nulls()\n",
    "    cleaned_rows = Xy.height\n",
    "    # --- END FINAL CLEANING ---\n",
    "    \n",
    "    print(f\"Data ready. Original rows: {original_rows}, Cleaned rows: {cleaned_rows}\")\n",
    "    \n",
    "    # --- NEW: SPLIT DATA ---\n",
    "    if cleaned_rows <= test_days:\n",
    "        raise ValueError(f\"Not enough data. Cleaned rows ({cleaned_rows}) must be greater than test_days ({test_days}).\")\n",
    "        \n",
    "    Xy_train = Xy.slice(0, -test_days)\n",
    "    Xy_test = Xy.slice(-test_days)\n",
    "    \n",
    "    print(f\"Splitting data: {Xy_train.height} train rows, {Xy_test.height} test rows.\")\n",
    "    \n",
    "    # Deconstruct Train Set\n",
    "    X_train = Xy_train.select(ALL_FEATURES)\n",
    "    y_train = Xy_train.select(\"target\")\n",
    "    scorer_info_train = Xy_train.select([\"forward_returns\", \"risk_free_rate\"])\n",
    "    \n",
    "    # Deconstruct Test Set\n",
    "    X_test = Xy_test.select(ALL_FEATURES)\n",
    "    y_test = Xy_test.select(\"target\")\n",
    "    scorer_info_test = Xy_test.select([\"forward_returns\", \"risk_free_rate\"])\n",
    "    \n",
    "    return X_train, y_train, scorer_info_train, X_test, y_test, scorer_info_test\n",
    "    \n",
    "    return X, y, scorer_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0d4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Actor and Critic MLP Networks ---\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor Network (Policy) - Maps State -> Action\n",
    "    (With proper final layer initialization)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim, 256)\n",
    "        self.layer_2 = nn.Linear(256, 256)\n",
    "        self.layer_3 = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # --- ADD INITIALIZATION ---\n",
    "        # Initialize final layer weights to be small\n",
    "        # This makes the tanh output close to 0, which scales to leverage 1.0\n",
    "        # This prevents the policy from starting at 0.0 leverage.\n",
    "        init_w = 3e-3\n",
    "        torch.nn.init.uniform_(self.layer_3.weight, -init_w, init_w)\n",
    "        torch.nn.init.uniform_(self.layer_3.bias, -init_w, init_w)\n",
    "        # --- END INITIALIZATION ---\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x_tanh = torch.tanh(self.layer_3(x))\n",
    "        x_scaled = (x_tanh + 1) * 0.5 \n",
    "        return x_scaled * self.max_action\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic Network (Q-Value) - Maps (State, Action) -> Q-Value\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        # Q1 network\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "        \n",
    "        # Q2 network\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "        self.l6 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        \n",
    "        q1 = F.relu(self.l1(xu))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        \n",
    "        q2 = F.relu(self.l4(xu))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        return q1, q2\n",
    "    \n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        q1 = F.relu(self.l1(xu))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1\n",
    "\n",
    "# --- TD3 Agent ---\n",
    "\n",
    "class TD3Agent:\n",
    "    def __init__(self, state_dim, action_dim, max_action, device, gamma=0.99, \n",
    "                 tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "        \n",
    "        # --- MODIFY THIS BLOCK ---\n",
    "        self.device = device\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(self.device)\n",
    "        self.actor_target = deepcopy(self.actor).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.critic = Critic(state_dim, action_dim).to(self.device)\n",
    "        self.critic_target = deepcopy(self.critic).to(self.device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        # --- END MODIFY ---\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_freq = policy_freq\n",
    "        self.total_it = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # --- MODIFY THIS LINE ---\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        # --- END MODIFY ---\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=100):\n",
    "        self.total_it += 1\n",
    "        \n",
    "        # 1. Sample from buffer\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        # --- MODIFY THIS BLOCK ---\n",
    "        # Move all tensors to the GPU\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action = torch.FloatTensor(action).to(self.device)\n",
    "        reward = torch.FloatTensor(reward).to(self.device)\n",
    "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "        done = torch.FloatTensor(done).to(self.device)\n",
    "        # --- END MODIFY ---\n",
    "        \n",
    "        # state = torch.FloatTensor(state)\n",
    "        # action = torch.FloatTensor(action)\n",
    "        # reward = torch.FloatTensor(reward)\n",
    "        # next_state = torch.FloatTensor(next_state)\n",
    "        # done = torch.FloatTensor(done)\n",
    "\n",
    "        # 2. Get target action from target actor, add noise\n",
    "        with torch.no_grad():\n",
    "            noise = (torch.randn_like(action) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(0, self.max_action)\n",
    "\n",
    "            # 3. Compute target Q-value (Clipped Double-Q)\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + (1 - done) * self.gamma * target_Q\n",
    "\n",
    "        # 4. Update Critic networks\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "        \n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # 5. Delayed Policy (Actor) update\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "            \n",
    "            # Compute actor loss\n",
    "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "            \n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # 6. Soft update target networks\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "            \n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f52f73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost_FQI_Agent:\n",
    "    \"\"\"\n",
    "    Fitted Q-Iteration (FQI) agent using XGBoost.\n",
    "    This requires a DISCRETE action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, gamma=0.99):\n",
    "        # 1. DISCRETIZE the action space\n",
    "        self.action_space = np.linspace(0.0, 2.0, num=10)  # 21 discrete actions from 0.0 to 2.0\n",
    "        self.n_actions = len(self.action_space)\n",
    "        print(f\"XGBoost agent initialized with discrete actions: {self.action_space}\")\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # We use a MultiOutputRegressor to fit one XGB model per action\n",
    "        self.model = MultiOutputRegressor(\n",
    "            xgb.XGBRegressor(\n",
    "                objective='reg:squarederror', n_estimators=100,\n",
    "                learning_rate=0.05, max_depth=5, subsample=0.8, \n",
    "                colsample_bytree=0.8, n_jobs=-1, random_state=42,\n",
    "                # --- THIS IS THE ONLY CHANGE NEEDED ---\n",
    "                device='cuda' \n",
    "            )\n",
    "        )\n",
    "        # Fit with dummy data to initialize\n",
    "        self.model.fit(np.random.rand(1, state_dim), np.random.rand(1, self.n_actions))\n",
    "\n",
    "    def select_action(self, state, exploration_rate=0.1):\n",
    "        \"\"\"Selects the best action (epsilon-greedy).\"\"\"\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return np.random.choice(self.action_space)\n",
    "        \n",
    "        q_values = self.model.predict(state.reshape(1, -1))[0]\n",
    "        best_action_idx = np.argmax(q_values)\n",
    "        return self.action_space[best_action_idx]\n",
    "\n",
    "    def train_fqi(self, replay_buffer, iterations=3):\n",
    "        \"\"\"\n",
    "        Trains the XGBoost model using Fitted Q-Iteration.\n",
    "        This is an OFFLINE, BATCH process.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Starting XGBoost FQI Training ---\")\n",
    "        \n",
    "        if len(replay_buffer) < 1000:\n",
    "             print(\"Buffer not full enough. Need at least 1000 samples.\")\n",
    "             return\n",
    "        \n",
    "        # Use the *entire* buffer, as FQI is a batch algorithm\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(len(replay_buffer))\n",
    "        \n",
    "        for k in range(iterations):\n",
    "            print(f\"FQI Iteration {k+1}/{iterations}...\")\n",
    "            \n",
    "            # 1. Calculate the target Q-value (Bellman update)\n",
    "            # Q_target(s, a) = r + gamma * max_a'(Q_k(s', a'))\n",
    "            \n",
    "            # Predict Q(s', a') for all next_states\n",
    "            next_q_values = self.model.predict(next_states)\n",
    "            \n",
    "            # Find max_a'(Q_k(s', a'))\n",
    "            max_next_q = np.max(next_q_values, axis=1)\n",
    "            \n",
    "            # The target: r + gamma * max_Q (or just r if done)\n",
    "            target_q = rewards.flatten() + (1.0 - dones.flatten()) * self.gamma * max_next_q\n",
    "            \n",
    "            # 2. Create the training set for the *new* XGBoost model\n",
    "            # We want to train Q_k+1(s, a) -> target_q\n",
    "            X_train = states\n",
    "            \n",
    "            # Start with the model's current predictions as the base\n",
    "            y_train = self.model.predict(X_train)\n",
    "            \n",
    "            # Update y_train at the index of the action *actually taken*\n",
    "            for i in range(len(states)):\n",
    "                action_taken = actions[i][0]\n",
    "                # Find the index of the action in our discrete space\n",
    "                action_idx = np.where(self.action_space == action_taken)[0]\n",
    "                \n",
    "                if len(action_idx) > 0:\n",
    "                    action_idx = action_idx[0]\n",
    "                    y_train[i, action_idx] = target_q[i]\n",
    "\n",
    "            # 4. Train a new XGBoost model on these (s, a) -> target_q pairs\n",
    "            print(\"Fitting new XGBoost model...\")\n",
    "            self.model.fit(X_train, y_train)\n",
    "            \n",
    "        print(\"--- FQI Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "329957b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Data ready. Original rows: 6990, Cleaned rows: 6941\n",
      "Splitting data: 6689 train rows, 252 test rows.\n",
      "Using device: cpu\n",
      "\n",
      "==================================================\n",
      "Training MLP (TD3) Agent\n",
      "==================================================\n",
      "TD3 Training Complete. Best Test Sharpe: -inf\n"
     ]
    }
   ],
   "source": [
    "# --- Main execution script ---\n",
    "\n",
    "\n",
    "# --- 1. Load Data and Init Env ---\n",
    "print(\"Loading and preparing data...\")\n",
    "TRAIN_DATA_PATH = \"./kaggle/train.csv\"\n",
    "SPY_DATA_PATH = \"./kaggle/spy-historical.csv\"\n",
    "\n",
    "# --- MODIFIED: Load split data ---\n",
    "X_train, y_train, scorer_info_train, X_test, y_test, scorer_info_test = \\\n",
    "    load_and_prep_data(TRAIN_DATA_PATH, SPY_DATA_PATH, slice_start=2000, test_days=252)\n",
    "\n",
    "# --- MODIFIED: Create two environments ---\n",
    "train_env = RlTradingEnv(X_train, y_train, scorer_info_train, transaction_cost=0.000001)\n",
    "test_env = RlTradingEnv(X_test, y_test, scorer_info_test, transaction_cost=0.000001)\n",
    "\n",
    "# Use train_env dimensions for the agent\n",
    "state_dim = train_env.n_features\n",
    "action_dim = train_env.action_space_dim\n",
    "max_action = train_env.max_action\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- 2. Train MLP (TD3) Agent ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training MLP (TD3) Agent\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "agent_td3 = TD3Agent(state_dim, action_dim, max_action, device=device)\n",
    "buffer_td3 = ReplayBuffer(max_size=500_000)\n",
    "\n",
    "N_EPISODES = 0\n",
    "BATCH_SIZE = 64\n",
    "START_TRAINING = 1000 \n",
    "EVAL_EVERY = 1\n",
    "\n",
    "best_eval_score = -np.inf # Track the best test score\n",
    "\n",
    "for ep in range(N_EPISODES):\n",
    "    # --- MODIFIED: Train on train_env ---\n",
    "    state = train_env.reset()\n",
    "    done = False\n",
    "    ep_reward_sum = 0\n",
    "    \n",
    "    while not done:\n",
    "        if len(buffer_td3) < START_TRAINING:\n",
    "            action = np.random.uniform(0, max_action, size=(action_dim,))\n",
    "        else:\n",
    "            action = (\n",
    "                agent_td3.select_action(state)\n",
    "                + np.random.normal(0, max_action * 0.1, size=action_dim)\n",
    "            ).clip(0, max_action)\n",
    "        \n",
    "        # --- MODIFIED: Step the train_env ---\n",
    "        next_state, reward, done = train_env.step(action)\n",
    "        buffer_td3.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        ep_reward_sum += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if len(buffer_td3) > START_TRAINING:\n",
    "            agent_td3.train(buffer_td3, BATCH_SIZE)\n",
    "    \n",
    "    print(f\"Episode {ep+1}/{N_EPISODES} | Train Reward: {ep_reward_sum:.2f}\")\n",
    "    #print(f\"    Train Fallbacks: {train_env.nan_fallback_counter} times\")\n",
    "    \n",
    "    # --- MODIFIED: Evaluate on test_env ---\n",
    "    if (ep + 1) % EVAL_EVERY == 0:\n",
    "        eval_score = test_env.run_evaluation(agent_td3.actor)\n",
    "        print(f\"--- TEST EVAL | Episode {ep+1} | Adjusted Sharpe: {eval_score:.4f} ---\")\n",
    "        \n",
    "        if eval_score > best_eval_score:\n",
    "            best_eval_score = eval_score\n",
    "            # You would save your model here, e.g.:\n",
    "            # torch.save(agent_td3.actor.state_dict(), 'best_actor.pth')\n",
    "            print(f\"    New best test score! Model saved.\")\n",
    "\n",
    "print(f\"TD3 Training Complete. Best Test Sharpe: {best_eval_score:.4f}\")\n",
    "\n",
    "\n",
    "# # --- 3. Train XGBoost (FQI) Agent ---\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"Training XGBoost (FQI) Agent\")\n",
    "# print(\"=\"*50)\n",
    "\n",
    "# agent_fqi = XGBoost_FQI_Agent(state_dim, gamma=0.99)\n",
    "# buffer_fqi = ReplayBuffer(max_size=500_000) \n",
    "\n",
    "# print(\"Populating buffer for FQI using train_env...\")\n",
    "# # --- MODIFIED: Use train_env to populate buffer ---\n",
    "# state = train_env.reset()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action = np.random.choice(agent_fqi.action_space)\n",
    "#     next_state, reward, done = train_env.step(np.array([action]))\n",
    "#     buffer_fqi.add(state, np.array([action]), reward, next_state, done)\n",
    "#     state = next_state\n",
    "\n",
    "# # Run the offline FQI training (trains on train_env data)\n",
    "# agent_fqi.train_fqi(buffer_fqi, iterations=3)\n",
    "\n",
    "# # --- MODIFIED: Evaluate on test_env ---\n",
    "# fqi_eval_score = test_env.run_evaluation(agent_fqi)\n",
    "# print(f\"--- FQI FINAL TEST EVAL | Adjusted Sharpe: {fqi_eval_score:.4f} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa16b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import copy # Use copy.deepcopy for target networks\n",
    "\n",
    "# --- 1. Define the SAC Actor (Policy) Network ---\n",
    "# This is different from the TD3 actor. It outputs a distribution.\n",
    "\n",
    "class ActorSAC(nn.Module):\n",
    "    \"\"\"\n",
    "    SAC Actor Network (Policy) - Maps State -> Action Distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(ActorSAC, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim, 368)\n",
    "        self.layer_2 = nn.Linear(368, 368)\n",
    "        self.mean = nn.Linear(368, action_dim)      # Outputs mean of distribution\n",
    "        self.log_std = nn.Linear(368, action_dim) # Outputs log std dev\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # Initialize final layers for stable starting policy\n",
    "        # (similar to what we did for TD3)\n",
    "        init_w = 1e-2\n",
    "        torch.nn.init.uniform_(self.mean.weight, -init_w, init_w)\n",
    "        torch.nn.init.uniform_(self.mean.bias, -init_w, init_w)\n",
    "        torch.nn.init.uniform_(self.log_std.weight, -init_w, init_w)\n",
    "        torch.nn.init.uniform_(self.log_std.bias, -init_w, init_w)\n",
    "\n",
    "    def forward(self, state, deterministic=False, with_logprob=True):\n",
    "        x = F.relu(self.layer_1(state))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        \n",
    "        mean = self.mean(x)\n",
    "        \n",
    "        # Clamp log_std for stability\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, min=-20, max=2)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        # Create the distribution\n",
    "        dist = Normal(mean, std)\n",
    "        \n",
    "        if deterministic:\n",
    "            # Used for evaluation: take the mean\n",
    "            u = mean\n",
    "        else:\n",
    "            # Used for training: sample w/ reparameterization trick\n",
    "            u = dist.rsample()\n",
    "\n",
    "        # --- This is the key SAC action-squashing logic ---\n",
    "        # 1. Get action and log-probability\n",
    "        # Action is squashed to [-1, 1] by tanh\n",
    "        action_tanh = torch.tanh(u)\n",
    "        \n",
    "        if with_logprob:\n",
    "            # Calculate log-prob of the *squashed* action\n",
    "            # This is the complex part: log_prob = log_prob_normal - log(1 - tanh(u)^2)\n",
    "            log_prob = dist.log_prob(u)\n",
    "            log_prob -= torch.log(1 - action_tanh.pow(2) + 1e-6)\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "        else:\n",
    "            log_prob = None\n",
    "            \n",
    "        # 2. Rescale action from [-1, 1] to [0, max_action]\n",
    "        # (tanh(u) + 1) / 2 maps to [0, 1]\n",
    "        action_scaled = (action_tanh + 1) * 0.5 * self.max_action\n",
    "        \n",
    "        return action_scaled, log_prob\n",
    "\n",
    "    def select_action(self, state, device):\n",
    "        \"\"\"Helper function to select action for stepping the env\"\"\"\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        # Pass deterministic=False to explore during collection\n",
    "        action, _ = self.forward(state, deterministic=False, with_logprob=False)\n",
    "        return action.cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "# --- 2. Define the SAC Agent ---\n",
    "# This class contains the full SAC logic.\n",
    "# It re-uses the 'Critic' class defined for TD3.\n",
    "\n",
    "class SACAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action, device,\n",
    "                 gamma=0.99, tau=0.005, alpha=0.2, lr=3e-4, weight_decay=1e-8):\n",
    "        \n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        # --- ADD THIS LINE ---\n",
    "        self.max_action = max_action\n",
    "        # --- END OF FIX ---\n",
    "\n",
    "        # --- Actor ---\n",
    "        self.actor = ActorSAC(state_dim, action_dim, max_action).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        # --- Critic (re-uses TD3's Critic class) ---\n",
    "        self.critic = Critic(state_dim, action_dim).to(self.device)\n",
    "        self.critic_target = copy.deepcopy(self.critic).to(self.device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # --- Alpha (Entropy Coefficient) ---\n",
    "        # We make it a learnable parameter\n",
    "        self.target_entropy = -float(action_dim)*4\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n",
    "        self.alpha = self.log_alpha.exp().item() # for use in loss calcs\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        return self.actor.select_action(state, self.device)\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=100):\n",
    "        # 1. Sample from buffer and move to device\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action = torch.FloatTensor(action).to(self.device)\n",
    "        reward = torch.FloatTensor(reward).to(self.device)\n",
    "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "        done = torch.FloatTensor(done).to(self.device)\n",
    "\n",
    "        # --- 2. Update Critic (Q-Networks) ---\n",
    "        with torch.no_grad():\n",
    "            # Get next action and log-prob from *current* policy\n",
    "            next_action_scaled, next_log_prob = self.actor(next_state)\n",
    "            \n",
    "            # Get Q-values from *target* critic\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action_scaled)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            \n",
    "            # Bellman backup: r + gamma * (1-d) * (Q_target - alpha * log_prob)\n",
    "            target_Q = reward + (1 - done) * self.gamma * (target_Q - self.alpha * next_log_prob)\n",
    "\n",
    "        # Get current Q estimates\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "        \n",
    "        # MSE loss for both critics\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # --- 3. Update Actor (Policy) ---\n",
    "        # Sample new action and log-prob from current policy\n",
    "        action_scaled, log_prob = self.actor(state)\n",
    "        \n",
    "        # Get Q-values from *current* critic\n",
    "        Q1, Q2 = self.critic(state, action_scaled)\n",
    "        Q = torch.min(Q1, Q2)\n",
    "        \n",
    "        # Actor loss: (alpha * log_prob - Q)\n",
    "        actor_loss = (self.alpha * log_prob - Q).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # --- 4. Update Alpha (Entropy Coefficient) ---\n",
    "        alpha_loss = -(self.log_alpha.exp() * (log_prob + self.target_entropy).detach()).mean()\n",
    "        \n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        \n",
    "        # Update alpha value\n",
    "        self.alpha = self.log_alpha.exp().item()\n",
    "\n",
    "        # --- 5. Soft Update Target Critic ---\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def run_evaluation(self, env):\n",
    "        \"\"\"\n",
    "        Runs a full backtest with the SAC agent in deterministic mode.\n",
    "        (This is a helper, you can also use env.run_evaluation)\n",
    "        \"\"\"\n",
    "        state = env.reset()\n",
    "        signals = []\n",
    "        \n",
    "        for t in range(env.n_steps - 1):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            # Use deterministic=True for evaluation\n",
    "            action, _ = self.actor(state_tensor, deterministic=True, with_logprob=False)\n",
    "            action = action.cpu().data.numpy().flatten()\n",
    "                \n",
    "            signals.append(np.clip(action, 0.0, self.max_action)[0])\n",
    "            \n",
    "            if t < env.n_steps - 2:\n",
    "                state = env.features[t + 1]\n",
    "        \n",
    "        scorer_df_trimmed = env.scorer_info_df.iloc[:len(signals)]\n",
    "        return calculate_competition_score(scorer_df_trimmed, np.array(signals))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "257ed976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data for SAC test...\n",
      "Data ready. Original rows: 6990, Cleaned rows: 6941\n",
      "Splitting data: 6689 train rows, 252 test rows.\n",
      "Using device: cpu for SAC\n",
      "\n",
      "==================================================\n",
      "Training SAC (Soft Actor-Critic) Agent\n",
      "==================================================\n",
      "SAC Training Complete. Best Test Sharpe: -inf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Main SAC Training and Evaluation Loop ---\n",
    "    \n",
    "# --- 1. Load Data and Init Env (Re-using existing setup) ---\n",
    "print(\"Loading and preparing data for SAC test...\")\n",
    "# (Assuming these are defined in your notebook)\n",
    "# TRAIN_DATA_PATH = \"./kaggle/train.csv\"\n",
    "# SPY_DATA_PATH = \"./kaggle/spy-historical.csv\"\n",
    "\n",
    "X_train, y_train, scorer_info_train, X_test, y_test, scorer_info_test = \\\n",
    "    load_and_prep_data(TRAIN_DATA_PATH, SPY_DATA_PATH, slice_start=2000, test_days=252)\n",
    "\n",
    "train_env_sac = RlTradingEnv(X_train, y_train, scorer_info_train, transaction_cost=0.00001)\n",
    "test_env_sac = RlTradingEnv(X_test, y_test, scorer_info_test, transaction_cost=0.00001)\n",
    "\n",
    "state_dim = train_env_sac.n_features\n",
    "action_dim = train_env_sac.action_space_dim\n",
    "max_action = train_env_sac.max_action\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} for SAC\")\n",
    "\n",
    "\n",
    "# --- 2. Train SAC Agent ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training SAC (Soft Actor-Critic) Agent\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "agent_sac = SACAgent(state_dim, action_dim, max_action, device=device)\n",
    "buffer_sac = ReplayBuffer(max_size=500_000)\n",
    "\n",
    "N_EPISODES = 0\n",
    "BATCH_SIZE = 128\n",
    "START_TRAINING = 1000 \n",
    "EVAL_EVERY = 1\n",
    "\n",
    "best_eval_score_sac = -np.inf \n",
    "\n",
    "for ep in range(N_EPISODES):\n",
    "    state = train_env_sac.reset()\n",
    "    done = False\n",
    "    ep_reward_sum = 0\n",
    "    \n",
    "    while not done:\n",
    "        if len(buffer_sac) < START_TRAINING:\n",
    "            action = np.random.uniform(0, max_action, size=(action_dim,))\n",
    "        else:\n",
    "            action = agent_sac.select_action(state)\n",
    "        \n",
    "        next_state, reward, done = train_env_sac.step(action)\n",
    "        buffer_sac.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        ep_reward_sum += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if len(buffer_sac) > START_TRAINING:\n",
    "            agent_sac.train(buffer_sac, BATCH_SIZE)\n",
    "    \n",
    "    print(f\"SAC Episode {ep+1}/{N_EPISODES} | Train Reward: {ep_reward_sum:.2f}\")\n",
    "    #print(f\"    Train Fallbacks: {train_env_sac.nan_fallback_counter} times | Current Alpha: {agent_sac.alpha:.4f}\")\n",
    "    \n",
    "    # Evaluate on test_env\n",
    "    if (ep + 1) % EVAL_EVERY == 0:\n",
    "        # Use the agent's internal eval function which uses deterministic=True\n",
    "        eval_score = agent_sac.run_evaluation(test_env_sac)\n",
    "        print(f\"--- SAC TEST EVAL | Episode {ep+1} | Adjusted Sharpe: {eval_score:.4f} ---\")\n",
    "        \n",
    "        if eval_score > best_eval_score_sac:\n",
    "            best_eval_score_sac = eval_score\n",
    "            # torch.save(agent_sac.actor.state_dict(), 'best_actor_sac.pth')\n",
    "            print(f\"    New best SAC test score! Model saved.\")\n",
    "\n",
    "print(f\"SAC Training Complete. Best Test Sharpe: {best_eval_score_sac:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing full dataset for CV...\n",
      "Data ready. Total cleaned rows: 6941 (from 6990)\n",
      "Features shape: (6941, 144), Target shape: (6941, 1)\n",
      "Using device: cpu for SAC\n",
      "\n",
      "==================================================\n",
      "Starting SAC Cross-Validation with 20 Folds\n",
      "==================================================\n",
      "\n",
      "--- Starting Fold 1/20 ---\n",
      "Train indices: 341, Test indices: 330\n",
      "  Fold 1, Ep 1/30 | Train Reward: 21.41 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 2/30 | Train Reward: -10.40 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 3/30 | Train Reward: 36.48 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 4/30 | Train Reward: 9.89 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 5/30 | Train Reward: 36.25 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 6/30 | Train Reward: 26.38 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 7/30 | Train Reward: 0.17 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 8/30 | Train Reward: 38.77 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 9/30 | Train Reward: 6.58 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 10/30 | Train Reward: 23.95 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 11/30 | Train Reward: 35.77 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 12/30 | Train Reward: 6.37 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 13/30 | Train Reward: 31.63 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 14/30 | Train Reward: 9.73 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 15/30 | Train Reward: 19.51 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 16/30 | Train Reward: 0.44 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 17/30 | Train Reward: 34.74 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 18/30 | Train Reward: 9.59 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 19/30 | Train Reward: 27.10 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 20/30 | Train Reward: 0.61 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 21/30 | Train Reward: 43.82 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 22/30 | Train Reward: 19.81 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 23/30 | Train Reward: 30.94 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 24/30 | Train Reward: 0.61 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 25/30 | Train Reward: -0.72 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 26/30 | Train Reward: -7.94 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 27/30 | Train Reward: -1.34 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 28/30 | Train Reward: 14.16 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 29/30 | Train Reward: -4.50 | Test Sharpe: 0.0000\n",
      "  Fold 1, Ep 30/30 | Train Reward: 19.68 | Test Sharpe: 0.0000\n",
      "--- Fold 1 Complete. Final Test Sharpe: 0.0000 ---\n",
      "\n",
      "--- Starting Fold 2/20 ---\n",
      "Train indices: 671, Test indices: 330\n",
      "  Fold 2, Ep 1/30 | Train Reward: 44.16 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 2/30 | Train Reward: 3.69 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 3/30 | Train Reward: 15.46 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 4/30 | Train Reward: 30.51 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 5/30 | Train Reward: 30.10 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 6/30 | Train Reward: 15.10 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 7/30 | Train Reward: -13.93 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 8/30 | Train Reward: 28.72 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 9/30 | Train Reward: 11.07 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 10/30 | Train Reward: -3.59 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 11/30 | Train Reward: 0.59 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 12/30 | Train Reward: 22.10 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 13/30 | Train Reward: 5.25 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 14/30 | Train Reward: 23.60 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 15/30 | Train Reward: 5.99 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 16/30 | Train Reward: -19.81 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 17/30 | Train Reward: 9.08 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 18/30 | Train Reward: -4.95 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 19/30 | Train Reward: -20.70 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 20/30 | Train Reward: 21.68 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 21/30 | Train Reward: 55.59 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 22/30 | Train Reward: -6.73 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 23/30 | Train Reward: 18.63 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 24/30 | Train Reward: 56.19 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 25/30 | Train Reward: -20.34 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 26/30 | Train Reward: -8.49 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 27/30 | Train Reward: 15.29 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 28/30 | Train Reward: 4.25 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 29/30 | Train Reward: 0.42 | Test Sharpe: 0.0000\n",
      "  Fold 2, Ep 30/30 | Train Reward: -99.79 | Test Sharpe: 0.0000\n",
      "--- Fold 2 Complete. Final Test Sharpe: 0.0000 ---\n",
      "\n",
      "--- Starting Fold 3/20 ---\n",
      "Train indices: 1001, Test indices: 330\n",
      "  Fold 3, Ep 1/30 | Train Reward: -22.21 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 2/30 | Train Reward: 2.61 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 3/30 | Train Reward: -83.47 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 4/30 | Train Reward: -49.40 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 5/30 | Train Reward: -27.04 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 6/30 | Train Reward: -25.18 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 7/30 | Train Reward: -55.32 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 8/30 | Train Reward: -22.69 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 9/30 | Train Reward: -55.91 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 10/30 | Train Reward: -21.65 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 11/30 | Train Reward: -70.90 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 12/30 | Train Reward: -46.61 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 13/30 | Train Reward: -52.62 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 14/30 | Train Reward: -22.16 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 15/30 | Train Reward: -40.29 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 16/30 | Train Reward: -30.62 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 17/30 | Train Reward: 4.81 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 18/30 | Train Reward: -26.97 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 19/30 | Train Reward: 12.30 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 20/30 | Train Reward: -31.47 | Test Sharpe: -0.0793\n",
      "  Fold 3, Ep 21/30 | Train Reward: -53.37 | Test Sharpe: -0.0748\n",
      "  Fold 3, Ep 22/30 | Train Reward: -48.07 | Test Sharpe: -0.0748\n",
      "  Fold 3, Ep 23/30 | Train Reward: -48.07 | Test Sharpe: -0.0748\n",
      "  Fold 3, Ep 24/30 | Train Reward: -48.07 | Test Sharpe: -0.0748\n",
      "  Fold 3, Ep 25/30 | Train Reward: -48.07 | Test Sharpe: -0.0748\n",
      "  Fold 3, Ep 26/30 | Train Reward: -48.07 | Test Sharpe: -0.0748\n",
      "  Fold 3, Ep 27/30 | Train Reward: -48.07 | Test Sharpe: -0.0748\n",
      "  Fold 3, Ep 28/30 | Train Reward: -48.07 | Test Sharpe: -0.0748\n",
      "  Fold 3, Ep 29/30 | Train Reward: -48.07 | Test Sharpe: -0.0748\n",
      "  Fold 3, Ep 30/30 | Train Reward: -48.07 | Test Sharpe: -0.0748\n",
      "--- Fold 3 Complete. Final Test Sharpe: -0.0748 ---\n",
      "\n",
      "--- Starting Fold 4/20 ---\n",
      "Train indices: 1331, Test indices: 330\n",
      "  Fold 4, Ep 1/30 | Train Reward: -73.92 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 2/30 | Train Reward: -54.61 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 3/30 | Train Reward: -85.74 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 4/30 | Train Reward: -46.97 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 5/30 | Train Reward: -70.01 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 6/30 | Train Reward: -23.26 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 7/30 | Train Reward: -84.46 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 8/30 | Train Reward: -58.38 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 9/30 | Train Reward: -40.16 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 10/30 | Train Reward: -76.00 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 11/30 | Train Reward: -55.02 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 12/30 | Train Reward: -105.47 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 13/30 | Train Reward: -106.43 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 14/30 | Train Reward: -88.57 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 15/30 | Train Reward: -85.11 | Test Sharpe: 0.5765\n",
      "  Fold 4, Ep 16/30 | Train Reward: -1653.44 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 17/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 18/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 19/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 20/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 21/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 22/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 23/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 24/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 25/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 26/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 27/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 28/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 29/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "  Fold 4, Ep 30/30 | Train Reward: -1836.08 | Test Sharpe: 0.0000\n",
      "--- Fold 4 Complete. Final Test Sharpe: 0.0000 ---\n",
      "\n",
      "--- Starting Fold 5/20 ---\n",
      "Train indices: 1661, Test indices: 330\n",
      "  Fold 5, Ep 1/30 | Train Reward: -75.12 | Test Sharpe: 0.0000\n",
      "  Fold 5, Ep 2/30 | Train Reward: -74.09 | Test Sharpe: 0.0000\n",
      "  Fold 5, Ep 3/30 | Train Reward: -54.14 | Test Sharpe: 0.0000\n",
      "  Fold 5, Ep 4/30 | Train Reward: -57.02 | Test Sharpe: 0.0000\n",
      "  Fold 5, Ep 5/30 | Train Reward: -63.31 | Test Sharpe: 0.0000\n",
      "  Fold 5, Ep 6/30 | Train Reward: -9.16 | Test Sharpe: 0.0000\n",
      "  Fold 5, Ep 7/30 | Train Reward: -66.01 | Test Sharpe: 0.0000\n",
      "  Fold 5, Ep 8/30 | Train Reward: -70.71 | Test Sharpe: 0.0000\n",
      "  Fold 5, Ep 9/30 | Train Reward: -27.68 | Test Sharpe: 0.0000\n",
      "  Fold 5, Ep 10/30 | Train Reward: -47.62 | Test Sharpe: 0.0000\n",
      "  Fold 5, Ep 11/30 | Train Reward: -10.68 | Test Sharpe: 0.0000\n",
      "  Fold 5, Ep 12/30 | Train Reward: -51.86 | Test Sharpe: 0.0000\n",
      "  Fold 5, Ep 13/30 | Train Reward: -120.87 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 14/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 15/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 16/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 17/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 18/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 19/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 20/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 21/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 22/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 23/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 24/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 25/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 26/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 27/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 28/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 29/30 | Train Reward: -90.65 | Test Sharpe: 0.4423\n",
      "  Fold 5, Ep 30/30 | Train Reward: -1362.31 | Test Sharpe: 0.1555\n",
      "--- Fold 5 Complete. Final Test Sharpe: 0.1555 ---\n",
      "\n",
      "--- Starting Fold 6/20 ---\n",
      "Train indices: 1991, Test indices: 330\n",
      "  Fold 6, Ep 1/30 | Train Reward: -10.13 | Test Sharpe: 0.4372\n",
      "  Fold 6, Ep 2/30 | Train Reward: -35.75 | Test Sharpe: 0.4372\n",
      "  Fold 6, Ep 3/30 | Train Reward: -21.60 | Test Sharpe: 0.4372\n",
      "  Fold 6, Ep 4/30 | Train Reward: -16.39 | Test Sharpe: 0.4372\n",
      "  Fold 6, Ep 5/30 | Train Reward: -35.01 | Test Sharpe: 0.4372\n",
      "  Fold 6, Ep 6/30 | Train Reward: 3.65 | Test Sharpe: 0.4372\n",
      "  Fold 6, Ep 7/30 | Train Reward: -90.54 | Test Sharpe: 0.4372\n",
      "  Fold 6, Ep 8/30 | Train Reward: -0.68 | Test Sharpe: 0.4372\n",
      "  Fold 6, Ep 9/30 | Train Reward: -94.75 | Test Sharpe: 0.4372\n",
      "  Fold 6, Ep 10/30 | Train Reward: -14.68 | Test Sharpe: 0.4372\n",
      "  Fold 6, Ep 11/30 | Train Reward: -1160.11 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 12/30 | Train Reward: -1313.11 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 13/30 | Train Reward: -1313.11 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 14/30 | Train Reward: -1313.11 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 15/30 | Train Reward: -1313.11 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 16/30 | Train Reward: -1313.11 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 17/30 | Train Reward: -1313.11 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 18/30 | Train Reward: -1313.11 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 19/30 | Train Reward: -1313.11 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 20/30 | Train Reward: -1313.11 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 21/30 | Train Reward: -1293.30 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 22/30 | Train Reward: -1293.30 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 23/30 | Train Reward: -1293.30 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 24/30 | Train Reward: -1293.30 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 25/30 | Train Reward: -1412.13 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 26/30 | Train Reward: -1412.13 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 27/30 | Train Reward: -1412.13 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 28/30 | Train Reward: -1412.13 | Test Sharpe: 0.4079\n",
      "  Fold 6, Ep 29/30 | Train Reward: -1527.40 | Test Sharpe: 0.0000\n",
      "  Fold 6, Ep 30/30 | Train Reward: -2747.91 | Test Sharpe: 0.0000\n",
      "--- Fold 6 Complete. Final Test Sharpe: 0.0000 ---\n",
      "\n",
      "--- Starting Fold 7/20 ---\n",
      "Train indices: 2000, Test indices: 330\n",
      "  Fold 7, Ep 1/30 | Train Reward: -60.43 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 2/30 | Train Reward: -47.03 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 3/30 | Train Reward: -46.80 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 4/30 | Train Reward: -71.63 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 5/30 | Train Reward: -61.97 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 6/30 | Train Reward: -24.67 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 7/30 | Train Reward: -80.29 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 8/30 | Train Reward: -89.04 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 9/30 | Train Reward: -42.21 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 10/30 | Train Reward: -81.35 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 11/30 | Train Reward: -2706.14 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 12/30 | Train Reward: -2752.18 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 13/30 | Train Reward: -2736.71 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 14/30 | Train Reward: -2728.33 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 15/30 | Train Reward: -2738.23 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 16/30 | Train Reward: -2745.67 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 17/30 | Train Reward: -2739.97 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 18/30 | Train Reward: -2729.24 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 19/30 | Train Reward: -2736.77 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 20/30 | Train Reward: -2741.32 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 21/30 | Train Reward: -2738.37 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 22/30 | Train Reward: -2734.10 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 23/30 | Train Reward: -2737.00 | Test Sharpe: 0.0000\n",
      "  Fold 7, Ep 24/30 | Train Reward: -1718.97 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 25/30 | Train Reward: -111.98 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 26/30 | Train Reward: -107.54 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 27/30 | Train Reward: -114.22 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 28/30 | Train Reward: -116.10 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 29/30 | Train Reward: -113.44 | Test Sharpe: -0.1140\n",
      "  Fold 7, Ep 30/30 | Train Reward: -111.00 | Test Sharpe: -0.1140\n",
      "--- Fold 7 Complete. Final Test Sharpe: -0.1140 ---\n",
      "\n",
      "--- Starting Fold 8/20 ---\n",
      "Train indices: 2000, Test indices: 330\n",
      "  Fold 8, Ep 1/30 | Train Reward: -57.17 | Test Sharpe: -0.6020\n",
      "  Fold 8, Ep 2/30 | Train Reward: -88.15 | Test Sharpe: -0.6020\n",
      "  Fold 8, Ep 3/30 | Train Reward: -119.36 | Test Sharpe: -0.6020\n",
      "  Fold 8, Ep 4/30 | Train Reward: -70.01 | Test Sharpe: -0.6020\n",
      "  Fold 8, Ep 5/30 | Train Reward: -59.90 | Test Sharpe: -0.6020\n",
      "  Fold 8, Ep 6/30 | Train Reward: -74.80 | Test Sharpe: -0.6020\n",
      "  Fold 8, Ep 7/30 | Train Reward: -93.45 | Test Sharpe: -0.6020\n",
      "  Fold 8, Ep 8/30 | Train Reward: -90.66 | Test Sharpe: -0.6020\n",
      "  Fold 8, Ep 9/30 | Train Reward: -33.44 | Test Sharpe: -0.6020\n",
      "  Fold 8, Ep 10/30 | Train Reward: -71.83 | Test Sharpe: -0.6020\n",
      "  Fold 8, Ep 11/30 | Train Reward: -225.20 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 12/30 | Train Reward: -215.66 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 13/30 | Train Reward: -212.42 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 14/30 | Train Reward: -209.65 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 15/30 | Train Reward: -213.15 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 16/30 | Train Reward: -205.70 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 17/30 | Train Reward: -212.04 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 18/30 | Train Reward: -213.18 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 19/30 | Train Reward: -212.94 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 20/30 | Train Reward: -216.82 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 21/30 | Train Reward: -211.25 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 22/30 | Train Reward: -207.00 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 23/30 | Train Reward: -205.30 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 24/30 | Train Reward: -213.61 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 25/30 | Train Reward: -405.20 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 26/30 | Train Reward: -1307.19 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 27/30 | Train Reward: -1312.33 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 28/30 | Train Reward: -1298.38 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 29/30 | Train Reward: -1303.22 | Test Sharpe: -0.0533\n",
      "  Fold 8, Ep 30/30 | Train Reward: -1309.76 | Test Sharpe: -0.0533\n",
      "--- Fold 8 Complete. Final Test Sharpe: -0.0533 ---\n",
      "\n",
      "--- Starting Fold 9/20 ---\n",
      "Train indices: 2000, Test indices: 330\n",
      "  Fold 9, Ep 1/30 | Train Reward: -1.69 | Test Sharpe: 0.4431\n",
      "  Fold 9, Ep 2/30 | Train Reward: -95.39 | Test Sharpe: 0.4431\n",
      "  Fold 9, Ep 3/30 | Train Reward: -67.76 | Test Sharpe: 0.4431\n",
      "  Fold 9, Ep 4/30 | Train Reward: -17.86 | Test Sharpe: 0.4431\n",
      "  Fold 9, Ep 5/30 | Train Reward: -40.41 | Test Sharpe: 0.4431\n",
      "  Fold 9, Ep 6/30 | Train Reward: -92.05 | Test Sharpe: 0.4431\n",
      "  Fold 9, Ep 7/30 | Train Reward: -53.28 | Test Sharpe: 0.4431\n",
      "  Fold 9, Ep 8/30 | Train Reward: -45.64 | Test Sharpe: 0.4431\n",
      "  Fold 9, Ep 9/30 | Train Reward: -40.90 | Test Sharpe: 0.4431\n",
      "  Fold 9, Ep 10/30 | Train Reward: -51.70 | Test Sharpe: 0.4431\n",
      "  Fold 9, Ep 11/30 | Train Reward: -2731.37 | Test Sharpe: 0.0000\n",
      "  Fold 9, Ep 12/30 | Train Reward: -2760.34 | Test Sharpe: 0.0000\n",
      "  Fold 9, Ep 13/30 | Train Reward: -2760.34 | Test Sharpe: 0.0000\n",
      "  Fold 9, Ep 14/30 | Train Reward: -2760.34 | Test Sharpe: 0.0000\n",
      "  Fold 9, Ep 15/30 | Train Reward: -2760.34 | Test Sharpe: 0.0000\n",
      "  Fold 9, Ep 16/30 | Train Reward: -2760.34 | Test Sharpe: 0.0000\n",
      "  Fold 9, Ep 17/30 | Train Reward: -2760.34 | Test Sharpe: 0.0000\n",
      "  Fold 9, Ep 18/30 | Train Reward: -2760.34 | Test Sharpe: 0.0000\n",
      "  Fold 9, Ep 19/30 | Train Reward: -2760.34 | Test Sharpe: 0.0000\n",
      "  Fold 9, Ep 20/30 | Train Reward: -2760.34 | Test Sharpe: 0.0000\n",
      "  Fold 9, Ep 21/30 | Train Reward: -2760.34 | Test Sharpe: 0.0000\n",
      "  Fold 9, Ep 22/30 | Train Reward: -2760.34 | Test Sharpe: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit # Add this to your imports\n",
    "\n",
    "def load_and_prep_data(train_path, spy_path, slice_start=2000):\n",
    "    \"\"\"\n",
    "    Loads and processes the *entire* dataset for use in cross-validation.\n",
    "    (Corrected for InvalidOperationError)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load base data and slice\n",
    "    full_train_df = pl.read_csv(train_path)\n",
    "    df_raw = full_train_df.slice(slice_start)\n",
    "    \n",
    "    # 2. Basic cleaning\n",
    "    df = df_raw.with_columns(\n",
    "        # --- FIX 1 ---\n",
    "        pl.selectors.float().replace([np.inf, -np.inf], None) # Was pl.all()\n",
    "    ).with_columns(\n",
    "        pl.selectors.numeric().fill_null(0.0) \n",
    "    )\n",
    "    df = df.with_columns(pl.col(\"date_id\").cast(pl.Int64))\n",
    "\n",
    "    # 3. Add weekday feature (as before)\n",
    "    spy_df = pl.read_csv(spy_path)\n",
    "    weekday_df = spy_df.with_columns(\n",
    "        pl.col(\"Date\").str.to_date().dt.weekday().alias(\"weekday\")\n",
    "    ).select([\"date_id\", \"weekday\"])\n",
    "    df_with_weekday = df.join(weekday_df, on=\"date_id\", how=\"left\").fill_null(0.0)\n",
    "    \n",
    "    # 4. Prep for feature generation (as before)\n",
    "    base_df = df_with_weekday.rename({'market_forward_excess_returns': 'target'})\n",
    "    feature_cols = [col for col in base_df.columns if col != 'date_id']\n",
    "    base_df = base_df.with_columns(pl.col(feature_cols).cast(pl.Float64, strict=False))\n",
    "    if 'E7' in base_df.columns:\n",
    "        base_df = base_df.drop('E7')\n",
    "        \n",
    "    # --- PRE-CLEAN BASE_DF ---\n",
    "    base_df = base_df.with_columns(\n",
    "        # --- FIX 2 ---\n",
    "        pl.selectors.float().replace([np.inf, -np.inf], None) # Was pl.all()\n",
    "    ).with_columns(\n",
    "        pl.all().fill_null(0.0).forward_fill().backward_fill()\n",
    "    )\n",
    "\n",
    "    # 5. Generate and combine features (as before)\n",
    "    new_features_df = generate_features_7(base_df) \n",
    "    processed_df = pl.concat([base_df, new_features_df], how=\"horizontal\")\n",
    "    \n",
    "    # 6. Finalize X, y, and scorer_info\n",
    "    base_features = [col for col in base_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    new_feature_names = new_features_df.columns\n",
    "    ALL_FEATURES = base_features + new_feature_names\n",
    "    \n",
    "    Xy = pl.concat([processed_df.select(ALL_FEATURES), \n",
    "                    processed_df.select(\"target\"), \n",
    "                    processed_df.select([\"forward_returns\", \"risk_free_rate\"])], \n",
    "                   how=\"horizontal\")\n",
    "    \n",
    "    # --- FINAL ROBUST CLEANING ---\n",
    "    Xy = Xy.with_columns(\n",
    "        # --- FIX 3 ---\n",
    "        pl.selectors.float().replace([np.inf, -np.inf], None) # Was pl.all()\n",
    "    )\n",
    "    original_rows = Xy.height\n",
    "    Xy = Xy.drop_nulls()\n",
    "    cleaned_rows = Xy.height\n",
    "    \n",
    "    X = Xy.select(ALL_FEATURES)\n",
    "    y = Xy.select(\"target\")\n",
    "    scorer_info_df = Xy.select([\"forward_returns\", \"risk_free_rate\"])\n",
    "    \n",
    "    print(f\"Data ready. Total cleaned rows: {cleaned_rows} (from {original_rows})\")\n",
    "    print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "    \n",
    "    return X, y, scorer_info_df\n",
    "\n",
    "# --- 3. Main SAC Cross-Validation Training and Evaluation Loop ---\n",
    "\n",
    "\n",
    "# --- 1. Load FULL Data (using new function) ---\n",
    "print(\"Loading and preparing full dataset for CV...\")\n",
    "TRAIN_DATA_PATH = \"./kaggle/train.csv\"\n",
    "SPY_DATA_PATH = \"./kaggle/spy-historical.csv\" # Make sure this path is correct\n",
    "\n",
    "X_full, y_full, scorer_info_full = \\\n",
    "    load_and_prep_data(TRAIN_DATA_PATH, SPY_DATA_PATH, slice_start=2000)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} for SAC\")\n",
    "\n",
    "# --- 2. Setup CV Parameters ---\n",
    "N_SPLITS = 20  # Number of folds for cross-validation\n",
    "N_EPISODES_PER_FOLD = 30 # As requested\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "START_TRAINING = 20000 # Steps before training starts (per fold)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=N_SPLITS, max_train_size=2000)\n",
    "\n",
    "# This will store the scores for each episode from each fold\n",
    "# Shape will be (N_SPLITS, N_EPISODES_PER_FOLD)\n",
    "all_fold_scores = [] \n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Starting SAC Cross-Validation with {N_SPLITS} Folds\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fold_num = 0\n",
    "for train_index, test_index in tscv.split(X_full):\n",
    "    fold_num += 1\n",
    "    print(f\"\\n--- Starting Fold {fold_num}/{N_SPLITS} ---\")\n",
    "    print(f\"Train indices: {len(train_index)}, Test indices: {len(test_index)}\")\n",
    "\n",
    "    # --- 3. Create data and environments for THIS fold ---\n",
    "    X_train, X_test = X_full[train_index], X_full[test_index]\n",
    "    y_train, y_test = y_full[train_index], y_full[test_index]\n",
    "    scorer_train, scorer_test = scorer_info_full[train_index], scorer_info_full[test_index]\n",
    "    \n",
    "    train_env_fold = RlTradingEnv(X_train, y_train, scorer_train, transaction_cost=0)\n",
    "    test_env_fold = RlTradingEnv(X_test, y_test, scorer_test, transaction_cost=0)\n",
    "\n",
    "    # --- 4. CRITICAL: Re-initialize Agent and Buffer for each fold ---\n",
    "    # This prevents data leakage from previous folds\n",
    "    state_dim = train_env_fold.n_features\n",
    "    action_dim = train_env_fold.action_space_dim\n",
    "    max_action = train_env_fold.max_action\n",
    "    \n",
    "    agent_sac_fold = SACAgent(state_dim, action_dim, max_action, device=device, gamma = 0.95)\n",
    "    buffer_sac_fold = ReplayBuffer(max_size=500_000)\n",
    "    \n",
    "    fold_episode_scores = [] # Stores scores for this fold's 15 episodes\n",
    "    \n",
    "    # --- 5. Inner Loop: Train for 15 episodes ---\n",
    "    for ep in range(N_EPISODES_PER_FOLD):\n",
    "        state = train_env_fold.reset()\n",
    "        done = False\n",
    "        ep_reward_sum = 0\n",
    "        \n",
    "        while not done:\n",
    "            if len(buffer_sac_fold) < START_TRAINING:\n",
    "                action = np.random.uniform(0, max_action, size=(action_dim,))\n",
    "            else:\n",
    "                action = agent_sac_fold.select_action(state)\n",
    "            \n",
    "            next_state, reward, done = train_env_fold.step(action)\n",
    "            buffer_sac_fold.add(state, action, reward, next_state, done)\n",
    "            \n",
    "            ep_reward_sum += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if len(buffer_sac_fold) > START_TRAINING:\n",
    "                agent_sac_fold.train(buffer_sac_fold, BATCH_SIZE)\n",
    "        \n",
    "        # --- 6. Evaluate and Record Score AFTER EACH EPISODE ---\n",
    "        eval_score = agent_sac_fold.run_evaluation(test_env_fold)\n",
    "        fold_episode_scores.append(eval_score)\n",
    "        \n",
    "        print(f\"  Fold {fold_num}, Ep {ep+1}/{N_EPISODES_PER_FOLD} | Train Reward: {ep_reward_sum:.2f} | Test Sharpe: {eval_score:.4f}\")\n",
    "    \n",
    "    # This fold is done, store its 15 episode scores\n",
    "    all_fold_scores.append(fold_episode_scores)\n",
    "    print(f\"--- Fold {fold_num} Complete. Final Test Sharpe: {eval_score:.4f} ---\")\n",
    "\n",
    "\n",
    "# --- 7. Final Results ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Cross-Validation Complete - Aggregated Results\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "scores_array = np.array(all_fold_scores)\n",
    "\n",
    "# Calculate mean and std dev across folds for each episode\n",
    "mean_scores_per_episode = np.mean(scores_array, axis=0)\n",
    "std_scores_per_episode = np.std(scores_array, axis=0)\n",
    "\n",
    "print(\"Mean Test Sharpe Score (Adjusted) per Episode (averaged across all folds):\")\n",
    "for i in range(N_EPISODES_PER_FOLD):\n",
    "    print(f\"  Episode {i+1}: {mean_scores_per_episode[i]:.4f} +/- {std_scores_per_episode[i]:.4f}\")\n",
    "\n",
    "print(\"\\nFinal Model Performance (Episode 15):\")\n",
    "print(f\"  Mean: {mean_scores_per_episode[-1]:.4f}\")\n",
    "print(f\"  Std Dev: {std_scores_per_episode[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3f67d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "# ... other imports\n",
    "\n",
    "class RlTradingEnv:\n",
    "    \"\"\"\n",
    "    An RL environment with a MEMORYLESS Quadratic Utility reward:\n",
    "    reward = (lambda_R * R_p) - (lambda_V * R_p^2)\n",
    "    \n",
    "    This implements the \"Contextual Bandit\" approach.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, targets, scorer_info, transaction_cost=0):\n",
    "        self.features = features.to_numpy()\n",
    "        self.targets = targets.to_numpy().flatten()\n",
    "        self.scorer_info_df = scorer_info.to_pandas() \n",
    "        \n",
    "        self.transaction_cost = transaction_cost\n",
    "        \n",
    "        self.n_steps = len(self.features)\n",
    "        self.n_features = self.features.shape[1]\n",
    "        self.action_space_dim = 1\n",
    "        self.max_action = 2.0\n",
    "        \n",
    "        # --- NEW UTILITY REWARD HYPERPARAMETERS ---\n",
    "        # Tune these to find the desired risk/return trade-off\n",
    "        # This is the \"Return\" component (your old reward_scale)\n",
    "        self.lambda_return = 100.0  \n",
    "        # This is the \"Risk Aversion\" component\n",
    "        # It's the penalty for volatility. Start high and tune down.\n",
    "        self.lambda_volatility = 0.0 \n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.last_leverage = 0.0\n",
    "        self.nan_fallback_counter = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment and returns the first state.\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.last_leverage = 0.0\n",
    "        self.nan_fallback_counter = 0\n",
    "        return self.features[self.current_step]\n",
    "\n",
    "    # No _calculate_dsr method is needed.\n",
    "    \n",
    "    def step(self, action_leverage):\n",
    "        \"\"\"\n",
    "        Takes an action, calculates memoryless utility reward,\n",
    "        and returns the next state.\n",
    "        \"\"\"\n",
    "        if self.current_step >= self.n_steps - 2:\n",
    "            return self.features[self.current_step], 0.0, True\n",
    "\n",
    "        leverage = np.clip(action_leverage, 0.0, self.max_action)[0]\n",
    "        target_excess_return = self.targets[self.current_step]\n",
    "        \n",
    "        # --- 1. Calculate Portfolio Return (R_p) ---\n",
    "        cost = self.transaction_cost * abs(leverage - self.last_leverage)\n",
    "        portfolio_return = (leverage * target_excess_return) - cost\n",
    "        \n",
    "        # --- 2. Handle NaNs (Failsafe) ---\n",
    "        if np.isnan(portfolio_return) or np.isinf(portfolio_return):\n",
    "            portfolio_return = 0.0 \n",
    "            self.nan_fallback_counter += 1\n",
    "\n",
    "        # --- 3. Calculate Memoryless Utility Reward ---\n",
    "        \n",
    "        # Return-seeking component (scaled)\n",
    "        return_component = self.lambda_return * portfolio_return\n",
    "        \n",
    "        # Risk-aversion component (penalizes squared return)\n",
    "        # The (portfolio_return**2) term is always positive,\n",
    "        # so this is a pure volatility penalty.\n",
    "        risk_component = self.lambda_volatility * (portfolio_return**2)\n",
    "\n",
    "        zero_penalty = 0.0\n",
    "        if leverage < 1e-3:\n",
    "            zero_penalty = np.log(leverage + 1e-6) * 0.1  # Small penalty for zero leverage\n",
    "        return_component += zero_penalty\n",
    "        \n",
    "        # Final reward is the trade-off\n",
    "        final_reward = return_component - risk_component\n",
    "        \n",
    "        # 4. Update state\n",
    "        self.last_leverage = leverage\n",
    "        self.current_step += 1\n",
    "        next_state = self.features[self.current_step]\n",
    "        done = (self.current_step == self.n_steps - 2)\n",
    "        \n",
    "        return next_state, final_reward, done\n",
    "\n",
    "    # ... (run_evaluation is unchanged) ...\n",
    "    def run_evaluation(self, policy_actor):\n",
    "        \"\"\"\n",
    "        Runs a full backtest with a given policy (actor)\n",
    "        and returns the final adjusted Sharpe score.\n",
    "        \"\"\"\n",
    "        state = self.reset()\n",
    "        signals = []\n",
    "        # --- FIX IS HERE ---\n",
    "        # If we have a torch actor, get its device (e.g., 'cuda:0')\n",
    "        actor_device = None\n",
    "        if isinstance(policy_actor, Actor):\n",
    "            actor_device = next(policy_actor.parameters()).device\n",
    "        # --- END FIX ---\n",
    "        \n",
    "        for t in range(self.n_steps - 1):\n",
    "            if isinstance(policy_actor, Actor): # It's our MLP/TD3 actor\n",
    "                # --- FIX IS HERE ---\n",
    "                # Create tensor and move it to the actor's device\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(actor_device)\n",
    "                # --- END FIX ---\n",
    "                #state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                action = policy_actor(state_tensor).cpu().data.numpy().flatten()\n",
    "            else: # It's our XGBoost agent\n",
    "                action = [policy_actor.select_action(state, exploration_rate=0.0)]\n",
    "                \n",
    "            signals.append(np.clip(action, 0.0, self.max_action)[0])\n",
    "            \n",
    "            # We don't need reward here, just the next state\n",
    "            if t < self.n_steps - 2:\n",
    "                state = self.features[t + 1]\n",
    "        \n",
    "        # Score the *entire* run\n",
    "        # We need to trim the scorer_info_df to match the length of signals\n",
    "        scorer_df_trimmed = self.scorer_info_df.iloc[:len(signals)]\n",
    "        return calculate_competition_score(scorer_df_trimmed, np.array(signals))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
