{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b6dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import polars as pl \n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import kaggle_evaluation.default_inference_server\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# Set environment variable to make ONLY GPU 1 (index 1) visible\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
    "\n",
    "# ============ PATHS ============\n",
    "DATA_PATH: Path = Path('./kaggle')\n",
    "\n",
    "# ============ RETURNS TO SIGNAL CONFIGS ============\n",
    "MIN_SIGNAL: float = 0.0                         # Minimum value for the daily signal \n",
    "MAX_SIGNAL: float = 2.0                         # Maximum value for the daily signal \n",
    "SIGNAL_MULTIPLIER: float = 400.0                # Multiplier of the OLS market forward excess returns predictions to signal \n",
    "\n",
    "# ============ MODEL CONFIGS ============\n",
    "CV: int = 10                                    # Number of cross validation folds in the model fitting\n",
    "L1_RATIO: float = 0.5                           # ElasticNet mixing parameter\n",
    "ALPHAS: np.ndarray = np.logspace(-4, 2, 100)    # Constant that multiplies the penalty terms\n",
    "MAX_ITER: int = 1000000                         # The maximum number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b79a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ================================\n",
    "# Hull Tactical Market Prediction\n",
    "# Optimized Baseline (Paraphrased)\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from warnings import filterwarnings\n",
    "from scipy.optimize import minimize, Bounds\n",
    "from gc import collect\n",
    "\n",
    "import kaggle_evaluation.default_inference_server\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------------\n",
    "# Global constants\n",
    "# -------------------------------\n",
    "MIN_POSITION = 0\n",
    "MAX_POSITION = 2\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Custom evaluation metric\n",
    "# -------------------------------\n",
    "class UserVisibleError(Exception):\n",
    "    \"\"\"Custom error for invalid predictions.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def adjusted_sharpe(solution: pd.DataFrame, submission: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Compute a Sharpe-like score with penalties for excess volatility and poor returns.\n",
    "    \"\"\"\n",
    "\n",
    "    solution = solution.copy()\n",
    "    solution['position'] = submission['prediction']\n",
    "\n",
    "    if solution['position'].max() > MAX_POSITION:\n",
    "        raise UserVisibleError(f\"Prediction above max limit {MAX_POSITION}\")\n",
    "    if solution['position'].min() < MIN_POSITION:\n",
    "        raise UserVisibleError(f\"Prediction below min limit {MIN_POSITION}\")\n",
    "\n",
    "    # Strategy returns\n",
    "    solution['strategy_returns'] = (\n",
    "        solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "        solution['position'] * solution['forward_returns']\n",
    "    )\n",
    "\n",
    "    excess = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "    cum_excess = (1 + excess).prod()\n",
    "    mean_excess = cum_excess ** (1 / len(solution)) - 1\n",
    "    std_excess = solution['strategy_returns'].std()\n",
    "\n",
    "    annual_days = 252\n",
    "    if std_excess == 0:\n",
    "        raise ZeroDivisionError\n",
    "    sharpe = mean_excess / std_excess * np.sqrt(annual_days)\n",
    "    strat_vol = float(std_excess * np.sqrt(annual_days) * 100)\n",
    "\n",
    "    # Market comparison\n",
    "    market_excess = solution['forward_returns'] - solution['risk_free_rate']\n",
    "    market_cum = (1 + market_excess).prod()\n",
    "    market_mean = market_cum ** (1 / len(solution)) - 1\n",
    "    market_std = solution['forward_returns'].std()\n",
    "    market_vol = float(market_std * np.sqrt(annual_days) * 100)\n",
    "\n",
    "    # Penalties\n",
    "    excess_vol_penalty = (\n",
    "        1 + max(0, strat_vol / market_vol - 1.2) if market_vol > 0 else 1\n",
    "    )\n",
    "    return_gap = max(0, (market_mean - mean_excess) * 100 * annual_days)\n",
    "    return_penalty = 1 + (return_gap ** 2) / 100\n",
    "\n",
    "    score = sharpe / (excess_vol_penalty * return_penalty)\n",
    "    return float(min(score, 1_000_000))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load training data\n",
    "# -------------------------------\n",
    "train = pd.read_csv(\n",
    "    \"./kaggle/train.csv\",\n",
    "    index_col=\"date_id\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Optimization objective\n",
    "# -------------------------------\n",
    "def objective(x):\n",
    "    recent = train[-180:].copy()\n",
    "    submission = pd.DataFrame({'prediction': x.clip(0, 2)}, index=recent.index)\n",
    "    return -adjusted_sharpe(recent, submission)\n",
    "\n",
    "\n",
    "# Initial guess + optimization\n",
    "# x0 = np.full(180, 0.05)\n",
    "# res = minimize(objective, x0, method=\"Powell\", bounds=Bounds(0, 2), tol=1e-8)\n",
    "# print(res)\n",
    "\n",
    "# optimal_preds = res.x\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Prediction function for Kaggle server\n",
    "# -------------------------------\n",
    "counter = 0\n",
    "\n",
    "def predict(batch: pl.DataFrame) -> float:\n",
    "    global counter, optimal_preds\n",
    "    value = np.float64(optimal_preds[counter])\n",
    "    print(f\"[{counter}] Prediction: {value:.8f}\")\n",
    "    counter += 1\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Run Kaggle evaluation server\n",
    "# -------------------------------\n",
    "# server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "# if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "#     server.serve()\n",
    "# else:\n",
    "#     server.run_local_gateway((\"./kaggle/\",))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df74473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base DataFrame shape after cleaning: (7990, 98)\n",
      "Starting batched feature generation for 94 base features...\n",
      "  ... Saved batch 0 with 570 features to features/interactions_batch_0.parquet\n",
      "  ... Saved batch 1 with 1770 features to features/interactions_batch_1.parquet\n",
      "  ... Saved batch 2 with 2970 features to features/interactions_batch_2.parquet\n",
      "  ... Saved batch 3 with 4170 features to features/interactions_batch_3.parquet\n",
      "  ... Saved batch 4 with 3633 features to features/interactions_batch_4.parquet\n",
      "\n",
      "Loading all original and generated features for selection...\n",
      "\n",
      "Generated a total of 13207 features for selection.\n",
      "\n",
      "Starting feature selection...\n",
      "Selected the top 150 most important features.\n",
      "Successfully saved final training data with 153 columns to 'final_training_data_150_features.parquet'\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Metric Implementation (No changes here) ---\n",
    "def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "    solution = y_true_df.to_pandas()\n",
    "    solution['position'] = y_pred_signals\n",
    "    solution['strategy_returns'] = (\n",
    "        solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "        solution['position'] * solution['forward_returns']\n",
    "    )\n",
    "    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "    strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "    strategy_std = solution['strategy_returns'].std()\n",
    "    if strategy_std == 0: return 0.0\n",
    "    trading_days_per_yr = 252\n",
    "    sharpe = strategy_geo_mean / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "    market_std = solution['forward_returns'].std()\n",
    "    market_volatility = market_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    strategy_volatility = strategy_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "    vol_penalty = 1 + excess_vol\n",
    "    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "    market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "    return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * trading_days_per_yr)\n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    return adjusted_sharpe\n",
    "\n",
    "\n",
    "# --- 2. Feature Engineering (MODIFIED) ---\n",
    "def create_and_save_interaction_features(df: pl.DataFrame, batch_size: int = 20, output_dir=\"features\") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates pairwise interaction features in batches to conserve memory and saves them to disk.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    base_features = [col for col in df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "    print(f\"Starting batched feature generation for {len(base_features)} base features...\")\n",
    "    \n",
    "    file_paths = []\n",
    "    \n",
    "    # Iterate through the features in chunks\n",
    "    for i in range(0, len(base_features), batch_size):\n",
    "        batch_features = base_features[i:i + batch_size]\n",
    "        \n",
    "        # Create a temporary DataFrame for this batch's new features\n",
    "        batch_interaction_df = pl.DataFrame()\n",
    "        \n",
    "        # --- Interactions WITHIN the current batch ---\n",
    "        for f1, f2 in itertools.combinations(batch_features, 2):\n",
    "            batch_interaction_df = batch_interaction_df.with_columns(\n",
    "                (df[f1] + df[f2]).alias(f'{f1}_add_{f2}'),\n",
    "                (df[f1] - df[f2]).alias(f'{f1}_sub_{f2}'),\n",
    "                (df[f1] * df[f2]).alias(f'{f1}_mult_{f2}'),\n",
    "            )\n",
    "            \n",
    "        # --- Interactions BETWEEN the current batch and ALL PREVIOUS features ---\n",
    "        previous_features = base_features[:i]\n",
    "        for f1 in batch_features:\n",
    "            for f2 in previous_features:\n",
    "                 batch_interaction_df = batch_interaction_df.with_columns(\n",
    "                    (df[f1] + df[f2]).alias(f'{f1}_add_{f2}'),\n",
    "                    (df[f1] - df[f2]).alias(f'{f1}_sub_{f2}'),\n",
    "                    (df[f1] * df[f2]).alias(f'{f1}_mult_{f2}'),\n",
    "                )\n",
    "        \n",
    "        if batch_interaction_df.width > 0:\n",
    "            file_path = f\"{output_dir}/interactions_batch_{i//batch_size}.parquet\"\n",
    "            batch_interaction_df.write_parquet(file_path)\n",
    "            file_paths.append(file_path)\n",
    "            print(f\"  ... Saved batch {i//batch_size} with {batch_interaction_df.width} features to {file_path}\")\n",
    "            \n",
    "    return file_paths\n",
    "\n",
    "# --- Main Script ---\n",
    "# 1. Load and do initial prep\n",
    "full_train_df = pl.read_csv(\"./kaggle/train.csv\")\n",
    "full_train_df = full_train_df.slice(1000) # Optional: Skip initial rows if needed\n",
    "full_train_df = full_train_df.rename({'market_forward_excess_returns': 'target'})\n",
    "\n",
    "# Explicitly cast all columns except date_id to Float64 to ensure they are all numeric\n",
    "feature_cols = [col for col in full_train_df.columns if col != 'date_id']\n",
    "full_train_df = full_train_df.with_columns(\n",
    "    pl.col(feature_cols).cast(pl.Float64, strict=False)\n",
    ")\n",
    "\n",
    "# Handle nulls in the base data first\n",
    "base_df = full_train_df.with_columns(pl.all().forward_fill())#.drop_nulls()\n",
    "print(f\"Base DataFrame shape after cleaning: {base_df.shape}\")\n",
    "\n",
    "# 2. Generate and save interaction features in batches\n",
    "interaction_files = create_and_save_interaction_features(base_df, batch_size=20)\n",
    "\n",
    "# 3. Load all features for the selection process\n",
    "print(\"\\nLoading all original and generated features for selection...\")\n",
    "interaction_dfs = [pl.read_parquet(f) for f in interaction_files]\n",
    "# Combine original data with all generated feature batches horizontally\n",
    "processed_df = pl.concat([base_df] + interaction_dfs, how=\"horizontal\")\n",
    "\n",
    "# 4. Chronological split (same as before)\n",
    "VALIDATION_SIZE = 180\n",
    "train_df = processed_df.head(-VALIDATION_SIZE)\n",
    "# We don't need the validation set for feature selection, only training data\n",
    "ALL_FEATURES = [col for col in train_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
    "TARGET_COL = \"target\"\n",
    "X_train_all = train_df.select(ALL_FEATURES)\n",
    "y_train = train_df.select(TARGET_COL)\n",
    "\n",
    "print(f\"\\nGenerated a total of {len(ALL_FEATURES)} features for selection.\")\n",
    "\n",
    "# 5. Feature Selection using XGBoost Importance\n",
    "print(\"\\nStarting feature selection...\")\n",
    "N_FEATURES_TO_SELECT = 150\n",
    "selector_model = xgb.XGBRegressor(objective='reg:absoluteerror', n_estimators=500, random_state=42, n_jobs=-1, tree_method='hist', device='cuda')\n",
    "selector_model.fit(X_train_all, y_train, verbose=False)\n",
    "\n",
    "importances = selector_model.feature_importances_\n",
    "feature_importance_df = pl.DataFrame({'feature': ALL_FEATURES, 'importance': importances}).sort('importance', descending=True)\n",
    "selected_features = feature_importance_df.head(N_FEATURES_TO_SELECT).get_column('feature').to_list()\n",
    "\n",
    "print(f\"Selected the top {len(selected_features)} most important features.\")\n",
    "final_training_data = processed_df.select(\n",
    "    selected_features + [\"target\", \"forward_returns\", \"risk_free_rate\"]\n",
    ")\n",
    "# 6. Save the list of selected features for later use\n",
    "output_filename = \"final_training_data_150_features.parquet\"\n",
    "final_training_data.write_parquet(output_filename)\n",
    "\n",
    "print(f\"Successfully saved final training data with {final_training_data.width} columns to '{output_filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f012d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_7 (df: pl.DataFrame) -> pl.DataFrame:\n",
    "  \"\"\"Generates new features from the base data.\n",
    "    This function is the target of the evolutionary algorithm.\n",
    "  \n",
    "    Available Feature Categories:\n",
    "    - D* (Dummy/Binary features): 9 columns (D1-D9)\n",
    "    - E* (Macro Economic features): 20 columns (E1-E20)\n",
    "    - I* (Interest Rate features): 9 columns (I1-I9)\n",
    "    - M* (Market Dynamics/Technical features): 18 columns (M1-M18)\n",
    "    - P* (Price/Valuation features): 13 columns (P1-P13)\n",
    "    - S* (Sentiment features): 12 columns (S1-S12)\n",
    "    - V* (Volatility features): 13 columns (V1-V13)\n",
    "  \"\"\"\n",
    "  new_features = pl.DataFrame({\n",
    "      # --- 20 Pairwise Interactions ---\n",
    "      'feat_M1_x_V1': df['M1'] * df['V1'],\n",
    "      'feat_P1_add_E1': df['P1'] + df['E1'],\n",
    "      'feat_S1_sub_I1': df['S1'] - df['I1'],\n",
    "      'feat_M10_div_V10': df['M10'] / (df['V10'] + 1e-6),\n",
    "      'feat_P10_x_E10': df['P10'] * df['E10'],\n",
    "      'feat_M2_x_S3': df['M2'] * df['S3'],\n",
    "      'feat_V2_div_P2': df['V2'] / (df['P2'] + 1e-6),\n",
    "      'feat_E4_sub_I3': df['E4'] - df['I3'],\n",
    "      'feat_S7_add_M12': df['S7'] + df['M12'],\n",
    "      'feat_I5_x_V11': df['I5'] * df['V11'],\n",
    "      'feat_P5_div_S8': df['P5'] / (df['S8'] + 1e-6),\n",
    "      'feat_E12_x_I9': df['E12'] * df['I9'],\n",
    "      'feat_M1_div_S1': df['M1'] / (df['S1'] + 1e-6),\n",
    "      'feat_V1_add_P1': df['V1'] + df['P1'],\n",
    "      'feat_E1_sub_I1': df['E1'] - df['I1'],\n",
    "      'feat_M2_div_V2': df['M2'] / (df['V2'] + 1e-6),\n",
    "      'feat_P2_x_S3': df['P2'] * df['S3'],\n",
    "      'feat_E4_add_M10': df['E4'] + df['M10'],\n",
    "      'feat_I3_sub_V10': df['I3'] - df['V10'],\n",
    "      'feat_S7_x_P10': df['S7'] * df['P10'],\n",
    "      # --- 10 Rolling Window Features ---\n",
    "      'feat_V2_roll_mean_5': df['V2'].rolling_mean(window_size=5),\n",
    "      'feat_V1_roll_std_5': df['V1'].rolling_std(window_size=5),\n",
    "      'feat_M1_roll_mean_20': df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_M3_roll_std_20': df['M3'].rolling_std(window_size=20),\n",
    "      'feat_P1_roll_max_10': df['P1'].rolling_max(window_size=10),\n",
    "      'feat_P1_roll_min_10': df['P1'].rolling_min(window_size=10),\n",
    "      'feat_E5_roll_mean_50': df['E5'].rolling_mean(window_size=50),\n",
    "      'feat_S1_roll_std_50': df['S1'].rolling_std(window_size=50),\n",
    "      'feat_I1_roll_mean_10': df['I1'].rolling_mean(window_size=10),\n",
    "      'feat_V10_roll_std_10': df['V10'].rolling_std(window_size=10),\n",
    "      # --- 10 Complex Interactions (3+ elements) ---\n",
    "      'feat_M1_V1_div_P1': (df['M1'] * df['V1']) / (df['P1'] + 1e-6),\n",
    "      'feat_E1_S1_add_I1': df['E1'] + df['S1'] - df['I1'],\n",
    "      'feat_M2_P2_sub_V2': df['M2'] + df['P2'] - df['V2'],\n",
    "      'feat_S7_div_E4_I3': df['S7'] / (df['E4'] + df['I3'] + 1e-6),\n",
    "      'feat_P5_x_M10_x_V10': df['P5'] * df['M10'] * df['V10'],\n",
    "      'feat_roll_diff_M1_5_20': df['M1'].rolling_mean(window_size=5) - df['M1'].rolling_mean(window_size=20),\n",
    "      'feat_roll_diff_V1_5_20': df['V1'].rolling_mean(window_size=5) - df['V1'].rolling_mean(window_size=20),\n",
    "      'feat_M_S_P_combo': (df['M12'] - df['M1']) / (df['S1'] + df['P1'] + 1e-6),\n",
    "      'feat_V_E_I_combo': (df['V11'] + df['V2']) * (df['E1'] - df['I1']),\n",
    "      'feat_ratio_of_ratios': (df['M1']/(df['V1']+1e-6)) / (df['P1']/(df['S1']+1e-6)),\n",
    "      # --- 10 New Features ---\n",
    "      'feat_M1_x_V1_x_P1': df['M1'] * df['V1'] * df['P1'],\n",
    "      'feat_E1_div_S1': df['E1'] / (df['S1'] + 1e-6),\n",
    "      'feat_I1_sub_V1': df['I1'] - df['V1'],\n",
    "      'feat_M10_add_V10': df['M10'] + df['V10'],\n",
    "      'feat_P10_div_E10': df['P10'] / (df['E10'] + 1e-6),\n",
    "      'feat_M2_add_S3': df['M2'] + df['S3'],\n",
    "      'feat_V2_x_P2': df['V2'] * df['P2'],\n",
    "      'feat_E4_add_I3': df['E4'] + df['I3'],\n",
    "      'feat_S7_div_M12': df['S7'] / (df['M12'] + 1e-6),\n",
    "      'feat_I5_div_V11': df['I5'] / (df['V11'] + 1e-6),\n",
    "      'feat_M1_log_P1': np.log(df['M1'] + 1e-6) / np.log(df['P1'] + 1e-6),\n",
    "  })\n",
    "  # Fill any nulls created by rolling windows\n",
    "  return new_features.with_columns(pl.all().forward_fill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45020b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed training data for cross-validation...\n",
      "Final feature set shape: (7990, 150)\n",
      "\n",
      "==================================================\n",
      "Starting 40-Fold Time Series Cross-Validation xgboost\n",
      "  Fold 1/40 Score: 1.7969\n",
      "  Fold 2/40 Score: 2.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/ml_311/lib/python3.11/site-packages/polars/series/series.py:1571: RuntimeWarning: invalid value encountered in log\n",
      "  lambda out: ufunc(*args, out=out, dtype=dtype_char, **kwargs),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 3/40 Score: 0.7756\n",
      "  Fold 4/40 Score: 1.1288\n",
      "  Fold 5/40 Score: 0.7408\n",
      "  Fold 6/40 Score: 1.7309\n",
      "  Fold 7/40 Score: 0.6708\n",
      "  Fold 8/40 Score: 0.1927\n",
      "  Fold 9/40 Score: -0.1879\n",
      "  Fold 10/40 Score: -0.5499\n",
      "  Fold 11/40 Score: 0.0730\n",
      "  Fold 12/40 Score: 2.2683\n",
      "  Fold 13/40 Score: 0.7544\n",
      "  Fold 14/40 Score: 0.7824\n",
      "  Fold 15/40 Score: 0.9627\n",
      "  Fold 16/40 Score: 0.6598\n",
      "  Fold 17/40 Score: 0.2653\n",
      "  Fold 18/40 Score: 0.0446\n",
      "  Fold 19/40 Score: -0.4093\n",
      "  Fold 20/40 Score: 2.5169\n",
      "  Fold 21/40 Score: 0.0093\n",
      "  Fold 22/40 Score: 0.0431\n",
      "  Fold 23/40 Score: 1.0502\n",
      "  Fold 24/40 Score: 1.9792\n",
      "  Fold 25/40 Score: 1.3466\n",
      "  Fold 26/40 Score: 2.0052\n",
      "  Fold 27/40 Score: -0.1972\n",
      "  Fold 28/40 Score: 1.9224\n",
      "  Fold 29/40 Score: 1.9095\n",
      "  Fold 30/40 Score: 2.5819\n",
      "  Fold 31/40 Score: 0.5177\n",
      "  Fold 32/40 Score: 0.1207\n",
      "  Fold 33/40 Score: 0.4032\n",
      "  Fold 34/40 Score: 1.8381\n",
      "  Fold 35/40 Score: 1.5766\n",
      "  Fold 36/40 Score: -0.2658\n",
      "  Fold 37/40 Score: -0.2125\n",
      "  Fold 38/40 Score: 1.9496\n",
      "  Fold 39/40 Score: 1.9097\n",
      "  Fold 40/40 Score: 0.2973\n",
      "\n",
      "Mean CV Score: 0.9362 (+/- 0.9155)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# === Run this in a SEPARATE \"Experimentation\" Notebook ===\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# --- Copy the metric and signal functions from your other notebook ---\n",
    "\n",
    "def calculate_competition_score(y_true_df: pl.DataFrame, y_pred_signals: np.ndarray) -> float:\n",
    "    # (The full function code goes here)\n",
    "    solution = y_true_df.to_pandas()\n",
    "    solution['position'] = y_pred_signals\n",
    "    solution['strategy_returns'] = (\n",
    "        solution['risk_free_rate'] * (1 - solution['position']) +\n",
    "        solution['position'] * solution['forward_returns']\n",
    "    )\n",
    "    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n",
    "    strategy_geo_mean = (1 + strategy_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "    strategy_std = solution['strategy_returns'].std()\n",
    "    if strategy_std == 0: return 0.0\n",
    "    trading_days_per_yr = 252\n",
    "    sharpe = strategy_geo_mean / strategy_std * np.sqrt(trading_days_per_yr)\n",
    "    market_std = solution['forward_returns'].std()\n",
    "    market_volatility = market_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    strategy_volatility = strategy_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "    vol_penalty = 1 + excess_vol\n",
    "    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n",
    "    market_geo_mean = (1 + market_excess_returns).prod() ** (1 / len(solution)) - 1\n",
    "    return_gap = max(0, (market_geo_mean - strategy_geo_mean) * 100 * trading_days_per_yr)\n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    return adjusted_sharpe\n",
    "\n",
    "def convert_to_signal(predictions: np.ndarray, multiplier: float = 400.0) -> np.ndarray:\n",
    "    # (The full function code goes here)\n",
    "    signals = predictions * multiplier + 1\n",
    "    return np.clip(signals, 0.0, 2.0)\n",
    "\n",
    "# --- Main Cross-Validation Script ---\n",
    "\n",
    "print(\"Loading pre-processed training data for cross-validation...\")\n",
    "training_df = pl.read_parquet(\"./final_training_data_150_features.parquet\")\n",
    "new_features_df = generate_features_7(base_df)\n",
    "\n",
    "# 3. Combine base data with new features\n",
    "processed_df = pl.concat([training_df, new_features_df], how=\"horizontal\")\n",
    "train_df = processed_df\n",
    "\n",
    "FEATURES = [col for col in training_df.columns if col not in [\"target\", \"forward_returns\", \"risk_free_rate\"]]\n",
    "TARGET_COL = \"target\"\n",
    "\n",
    "X = training_df.select(FEATURES)\n",
    "y = training_df.select(TARGET_COL)\n",
    "scorer_info_df = training_df.select([\"forward_returns\", \"risk_free_rate\"])\n",
    "print(f\"Final feature set shape: {X.shape}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "nsplits = 40\n",
    "print(f\"Starting {nsplits}-Fold Time Series Cross-Validation xgboost\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=nsplits)\n",
    "cv_scores = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test_info = scorer_info_df[test_index]\n",
    "    \n",
    "    # Use your final model parameters for an accurate score estimate\n",
    "    model = xgb.XGBRegressor(\n",
    "        objective='reg:absoluteerror', n_estimators=30, device='cuda',\n",
    "        learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8,\n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    signals = convert_to_signal(predictions)\n",
    "    score = calculate_competition_score(y_test_info, signals)\n",
    "    cv_scores.append(score)\n",
    "    print(f\"  Fold {i+1}/{nsplits} Score: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nMean CV Score: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
